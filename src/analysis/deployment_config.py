"""
Adaptive NLP Configuration for Docker/Native Deployment

This module automatically configures NLP components based on the deployment environment:
- Native macOS: Uses larger models with GPU acceleration (MPS)
- Docker containers: Uses smaller CPU-optimized models
- External APIs: Offloads heavy processing to external services

Follows your existing pattern of external embeddings for GPU-intensive tasks.
"""

import os
import logging
from typing import Dict, Any, Optional
from dataclasses import dataclass
from enum import Enum

logger = logging.getLogger(__name__)


class DeploymentEnvironment(Enum):
    """Detected deployment environment."""

    NATIVE_MACOS = "native_macos"  # Native Python on macOS
    DOCKER_CONTAINER = "docker_container"  # Running inside Docker
    CLOUD_INSTANCE = "cloud_instance"  # Cloud deployment
    UNKNOWN = "unknown"


@dataclass
class NLPConfiguration:
    """NLP configuration optimized for deployment environment."""

    environment: DeploymentEnvironment
    spacy_model: str
    use_external_embeddings: bool
    use_gpu_acceleration: bool
    performance_mode: str  # 'fast', 'balanced', 'accurate'

    def __post_init__(self):
        """Validate configuration after initialization."""
        if self.environment == DeploymentEnvironment.DOCKER_CONTAINER and self.use_gpu_acceleration:
            logger.warning("GPU acceleration not available in Docker on macOS - disabling")
            self.use_gpu_acceleration = False


def detect_deployment_environment() -> DeploymentEnvironment:
    """Detect the current deployment environment."""

    # Check if running in Docker
    if os.path.exists("/.dockerenv") or os.getenv("DOCKER_CONTAINER") == "true":
        return DeploymentEnvironment.DOCKER_CONTAINER

    # Check if running on macOS
    import platform

    if platform.system() == "Darwin":
        return DeploymentEnvironment.NATIVE_MACOS

    # Check for cloud instance indicators
    cloud_indicators = [
        "AWS_REGION",
        "GOOGLE_CLOUD_PROJECT",
        "AZURE_SUBSCRIPTION_ID",
        "KUBERNETES_SERVICE_HOST",
        "RENDER",
        "HEROKU",
    ]
    if any(os.getenv(indicator) for indicator in cloud_indicators):
        return DeploymentEnvironment.CLOUD_INSTANCE

    return DeploymentEnvironment.UNKNOWN


def get_optimal_nlp_config() -> NLPConfiguration:
    """Get optimal NLP configuration for current environment."""

    environment = detect_deployment_environment()

    # Base configuration
    config = {
        "use_external_embeddings": True,  # Always use external embeddings (your proven approach)
        "performance_mode": "balanced",
    }

    if environment == DeploymentEnvironment.NATIVE_MACOS:
        # Native macOS: Use large models with GPU acceleration
        config.update(
            {
                "spacy_model": os.getenv("NLP_SPACY_MODEL", "en_core_web_lg"),
                "use_gpu_acceleration": True,
                "performance_mode": "accurate",
            }
        )
        logger.info("Configured for native macOS with GPU acceleration")

    elif environment == DeploymentEnvironment.DOCKER_CONTAINER:
        # Docker: Use smaller CPU-optimized models
        config.update(
            {
                "spacy_model": "en_core_web_sm",  # Smaller model for Docker
                "use_gpu_acceleration": False,
                "performance_mode": "fast",
            }
        )
        logger.info("Configured for Docker container with CPU optimization")

    elif environment == DeploymentEnvironment.CLOUD_INSTANCE:
        # Cloud: Balanced approach, check for GPU availability
        try:
            import torch

            has_gpu = torch.cuda.is_available()
        except ImportError:
            has_gpu = False

        config.update(
            {
                "spacy_model": "en_core_web_md" if has_gpu else "en_core_web_sm",
                "use_gpu_acceleration": has_gpu,
                "performance_mode": "balanced",
            }
        )
        logger.info(f"Configured for cloud deployment (GPU: {has_gpu})")

    else:
        # Unknown environment: Conservative settings
        config.update(
            {
                "spacy_model": "en_core_web_sm",
                "use_gpu_acceleration": False,
                "performance_mode": "fast",
            }
        )
        logger.warning("Unknown deployment environment - using conservative settings")

    return NLPConfiguration(environment=environment, **config)


def get_deployment_recommendations() -> Dict[str, Any]:
    """Get deployment recommendations based on current environment."""

    config = get_optimal_nlp_config()

    recommendations = {
        "current_config": {
            "environment": config.environment.value,
            "spacy_model": config.spacy_model,
            "external_embeddings": config.use_external_embeddings,
            "gpu_acceleration": config.use_gpu_acceleration,
            "performance_mode": config.performance_mode,
        },
        "recommendations": [],
    }

    if config.environment == DeploymentEnvironment.NATIVE_MACOS:
        recommendations["recommendations"].extend(
            [
                "âœ… Optimal setup detected - using GPU acceleration",
                "âœ… External embeddings prevent Docker GPU limitations",
                "ðŸš€ Can use large spaCy models for best accuracy",
                "ðŸ’¡ Consider native NLP service for multiple bot instances",
            ]
        )

    elif config.environment == DeploymentEnvironment.DOCKER_CONTAINER:
        recommendations["recommendations"].extend(
            [
                "âš ï¸  Docker on macOS cannot access GPU",
                "âœ… Using CPU-optimized models for Docker",
                "âœ… External embeddings maintain high quality",
                "ðŸ’¡ Consider hybrid deployment: Docker + native NLP service",
            ]
        )

    elif config.environment == DeploymentEnvironment.CLOUD_INSTANCE:
        recommendations["recommendations"].extend(
            [
                "â˜ï¸  Cloud deployment detected",
                "ðŸ” GPU availability checked automatically",
                "âœ… External embeddings provide consistency",
                "ðŸ’¡ Consider managed NLP services for scale",
            ]
        )

    return recommendations


def create_docker_optimized_config() -> Dict[str, str]:
    """Create environment variables for Docker deployment."""
    return {
        "NLP_SPACY_MODEL": "en_core_web_sm",
        "ENABLE_ADVANCED_TOPIC_EXTRACTION": "true",
        "USE_EXTERNAL_EMBEDDINGS": "true",
        "NLP_PERFORMANCE_MODE": "fast",
        "DOCKER_CONTAINER": "true",
    }


def create_native_optimized_config() -> Dict[str, str]:
    """Create environment variables for native deployment."""
    return {
        "NLP_SPACY_MODEL": "en_core_web_lg",
        "ENABLE_ADVANCED_TOPIC_EXTRACTION": "true",
        "USE_EXTERNAL_EMBEDDINGS": "true",
        "NLP_PERFORMANCE_MODE": "accurate",
        "DOCKER_CONTAINER": "false",
    }


# Quick status check
def print_deployment_status():
    """Print current deployment status and recommendations."""
    recommendations = get_deployment_recommendations()

    print("ðŸ”§ NLP Deployment Configuration")
    print("=" * 50)

    config = recommendations["current_config"]
    print(f"Environment: {config['environment']}")
    print(f"spaCy Model: {config['spacy_model']}")
    print(f"External Embeddings: {config['external_embeddings']}")
    print(f"GPU Acceleration: {config['gpu_acceleration']}")
    print(f"Performance Mode: {config['performance_mode']}")

    print(f"\nðŸ“‹ Recommendations:")
    for rec in recommendations["recommendations"]:
        print(f"  {rec}")

    print(f"\nðŸ’¡ Your Current Approach (External Embeddings) is PERFECT for:")
    print(f"  â€¢ Avoiding Docker GPU limitations on macOS")
    print(f"  â€¢ Consistent performance across environments")
    print(f"  â€¢ Leveraging powerful external GPU resources")
    print(f"  â€¢ Keeping containers lightweight and portable")


if __name__ == "__main__":
    print_deployment_status()
