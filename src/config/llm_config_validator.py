#!/usr/bin/env python3
"""
LLM Configuration Validator
Validates and reports the effective LLM configuration for WhisperEngine
"""

import os
import logging
from typing import Optional, List
from dataclasses import dataclass

logger = logging.getLogger(__name__)


@dataclass
class LLMConfigStatus:
    """Status of LLM configuration"""

    is_valid: bool
    source: str  # 'user_override', 'auto_detected', 'invalid'
    api_url: Optional[str]
    model: Optional[str]
    has_api_key: bool
    backend_name: str
    issues: List[str]
    recommendations: List[str]


class LLMConfigValidator:
    """Validates LLM configuration and provides recommendations"""

    def __init__(self):
        self.logger = logging.getLogger(__name__)

    def validate_configuration(self) -> LLMConfigStatus:
        """Validate the current LLM configuration"""
        issues = []
        recommendations = []

        # Check for user overrides
        user_url = os.getenv("LLM_CHAT_API_URL")
        user_base_url = os.getenv("LLM_BASE_URL")
        user_key = os.getenv("LLM_CHAT_API_KEY")
        user_model = os.getenv("LLM_CHAT_MODEL")

        # Determine effective URL
        effective_url = user_url or user_base_url

        if effective_url:
            # User has configured URL
            source = "user_override"
            backend_name = self._detect_backend_from_url(effective_url)

            # Validate user configuration
            if not self._is_valid_url(effective_url):
                issues.append(f"Invalid API URL format: {effective_url}")

            if "openrouter" in effective_url.lower() and not user_key:
                issues.append("OpenRouter API requires an API key")
                recommendations.append("Set LLM_CHAT_API_KEY environment variable")

            if not user_model:
                issues.append("No model specified")
                recommendations.append("Set LLM_CHAT_MODEL environment variable")

        else:
            # Try auto-detection
            try:
                from src.llm.smart_backend_selector import get_smart_backend_selector

                selector = get_smart_backend_selector()
                optimal_backend = selector.get_optimal_backend()

                if optimal_backend:
                    source = "auto_detected"
                    backend_name = optimal_backend.name
                    config = selector.get_backend_config(optimal_backend)
                    effective_url = config.get("LLM_CHAT_API_URL")

                    if not effective_url:
                        issues.append("Auto-detection failed to provide valid URL")

                else:
                    source = "invalid"
                    backend_name = "None Available"
                    effective_url = None
                    issues.append("No suitable LLM backend found")
                    recommendations.extend(selector.get_setup_recommendations())

            except ImportError as e:
                source = "invalid"
                backend_name = "Detection Failed"
                effective_url = None
                issues.append(f"Backend auto-detection not available: {str(e)}")
                recommendations.append("Install smart backend selector dependencies")
            except (AttributeError, RuntimeError, OSError) as e:
                source = "invalid"
                backend_name = "Detection Failed"
                effective_url = None
                issues.append(f"Backend auto-detection failed: {str(e)}")
                recommendations.append("Manual configuration required")

        # Final validation
        is_valid = len(issues) == 0 and effective_url is not None

        return LLMConfigStatus(
            is_valid=is_valid,
            source=source,
            api_url=effective_url,
            model=user_model,
            has_api_key=bool(user_key),
            backend_name=backend_name,
            issues=issues,
            recommendations=recommendations,
        )

    def _detect_backend_from_url(self, url: str) -> str:
        """Detect backend type from URL"""
        url_lower = url.lower()

        if "openrouter" in url_lower:
            return "OpenRouter"
        elif "localhost:1234" in url_lower or "127.0.0.1:1234" in url_lower:
            return "LM Studio"
        elif "localhost:11434" in url_lower or "127.0.0.1:11434" in url_lower:
            return "Ollama"
        elif "openai.com" in url_lower:
            return "OpenAI"
        elif "anthropic.com" in url_lower:
            return "Anthropic"
        elif "mlx://" in url_lower:
            return "MLX"
        elif "llamacpp://" in url_lower:
            return "llama-cpp-python"
        elif "local://" in url_lower:
            return "Transformers"
        else:
            return "Custom API"

    def _is_valid_url(self, url: str) -> bool:
        """Basic URL validation"""
        if not url:
            return False

        # Special schemes
        if url.startswith(("mlx://", "llamacpp://", "local://")):
            return True

        # HTTP/HTTPS URLs
        if url.startswith(("http://", "https://")):
            return True

        return False

    def print_configuration_report(self):
        """Print a detailed configuration report"""
        config = self.validate_configuration()

        print("ðŸ”§ WhisperEngine LLM Configuration Report")
        print("=" * 50)

        # Status
        status_icon = "âœ…" if config.is_valid else "âŒ"
        print(f"{status_icon} Status: {'Valid' if config.is_valid else 'Invalid'}")
        print(f"ðŸ“ Source: {config.source}")
        print(f"ðŸ”— Backend: {config.backend_name}")

        # Configuration details
        if config.api_url:
            print(f"ðŸŒ API URL: {config.api_url}")
        else:
            print("ðŸŒ API URL: Not configured")

        if config.model:
            print(f"ðŸ¤– Model: {config.model}")
        else:
            print("ðŸ¤– Model: Not specified")

        print(f"ðŸ”‘ API Key: {'Configured' if config.has_api_key else 'Not set'}")

        # Issues
        if config.issues:
            print("\nâš ï¸ Issues:")
            for issue in config.issues:
                print(f"   - {issue}")

        # Recommendations
        if config.recommendations:
            print("\nðŸ’¡ Recommendations:")
            for rec in config.recommendations:
                print(f"   - {rec}")

        # Override instructions
        print("\nðŸ”§ User Override Variables:")
        print("   LLM_CHAT_API_URL - Primary API endpoint URL")
        print("   LLM_BASE_URL - Alternative base URL setting")
        print("   LLM_CHAT_API_KEY - API authentication key")
        print("   LLM_CHAT_MODEL - Specific model name")
        print("\nSet any of these environment variables to override auto-detection.")


def validate_llm_config() -> LLMConfigStatus:
    """Convenience function to validate LLM configuration"""
    validator = LLMConfigValidator()
    return validator.validate_configuration()


def print_llm_config_report():
    """Convenience function to print configuration report"""
    validator = LLMConfigValidator()
    validator.print_configuration_report()


if __name__ == "__main__":
    # Command-line usage
    print_llm_config_report()
