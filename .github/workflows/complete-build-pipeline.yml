name: ðŸš€ Complete Build and Release Pipeline

on:
  push:
    branches: [ main, develop ]
    tags: [ 'v*' ]
  pull_request:
    branches: [ main ]
  workflow_dispatch:
    inputs:
      build_type:
        description: 'Build type'
        required: true
        default: 'test'
        type: choice
        options:
          - test
          - release
          - docker-only

# Default least-privilege permissions for all jobs.
# Individual jobs (release, docker-build) will elevate where required.
permissions:
  contents: read        # Source checkout
  packages: read        # Pull public packages if needed
  actions: read         # Access to action metadata

# Prevent multiple concurrent builds
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

env:
  REGISTRY: docker.io
  IMAGE_NAME: whisperengine/whisperengine
  DOCKER_BUILDKIT: 1
  BUILDX_NO_DEFAULT_ATTESTATIONS: 1

jobs:
  # ===================================================================
  # Comprehensive Testing Suite
  # ===================================================================
  comprehensive_tests:
    name: ðŸ§ª Comprehensive Test Suite
    runs-on: ubuntu-latest
    needs: validation
    if: needs.validation.outputs.should_build == 'true'
    strategy:
      matrix:
        test_category: [unit, integration, performance, security]
      fail-fast: false
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          pip install -r requirements-core.txt
          pip install -r requirements-dev.txt

      - name: Configure test environment
        run: |
          # Set up test environment variables
          echo "ENV_MODE=testing" >> $GITHUB_ENV
          echo "LLM_CHAT_API_URL=http://mock-llm-server" >> $GITHUB_ENV
          echo "DISCORD_BOT_TOKEN=test_token_for_ci" >> $GITHUB_ENV

      - name: Run ${{ matrix.test_category }} tests
        run: |
          python tests/ci_test_runner.py \
            --category ${{ matrix.test_category }} \
            --report test_report_${{ matrix.test_category }}.json \
            --workspace .

      - name: Upload test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: test-results-${{ matrix.test_category }}
          path: |
            test_report_${{ matrix.test_category }}.json
            coverage.json
            benchmark.json
          retention-days: 30

      - name: Upload coverage to Codecov
        if: matrix.test_category == 'unit' || matrix.test_category == 'integration'
        uses: codecov/codecov-action@v3
        with:
          file: ./coverage.json
          flags: ${{ matrix.test_category }}
          name: codecov-${{ matrix.test_category }}
          fail_ci_if_error: false

  # ===================================================================
  # Test Results Aggregation
  # ===================================================================
  test_aggregation:
    name: ðŸ“Š Test Results Summary
    runs-on: ubuntu-latest
    needs: [comprehensive_tests, onboarding_validation]
    if: always()
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download all test results
        uses: actions/download-artifact@v4
        with:
          path: test-results/

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Aggregate test results
        run: |
          python -c "
          import json
          import os
          from pathlib import Path
          
          # Collect all test reports
          test_results = []
          for result_dir in Path('test-results').iterdir():
              if result_dir.is_dir():
                  for report_file in result_dir.glob('test_report_*.json'):
                      try:
                          with open(report_file) as f:
                              data = json.load(f)
                              test_results.append(data)
                      except Exception as e:
                          print(f'Error reading {report_file}: {e}')
          
          # Calculate totals
          total_tests = sum(r['overall_statistics']['total_tests'] for r in test_results)
          total_passed = sum(r['overall_statistics']['passed'] for r in test_results)
          total_failed = sum(r['overall_statistics']['failed'] for r in test_results)
          
          # Check onboarding validation status
          onboarding_status = '${{ needs.onboarding_validation.result }}'
          onboarding_passed = onboarding_status == 'success'
          
          success_rate = (total_passed / total_tests * 100) if total_tests > 0 else 0
          
          print(f'ðŸ“Š COMPREHENSIVE TEST SUMMARY')
          print(f'Total Tests: {total_tests}')
          print(f'âœ… Passed: {total_passed}')
          print(f'âŒ Failed: {total_failed}')
          print(f'ðŸ“ˆ Success Rate: {success_rate:.1f}%')
          print(f'ðŸŽ¯ Onboarding Validation: {"âœ… Passed" if onboarding_passed else "âŒ Failed"}')
          
          # Set GitHub Actions output
          with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
              f.write(f'total_tests={total_tests}\n')
              f.write(f'success_rate={success_rate}\n')
              f.write(f'all_passed={total_failed == 0 and onboarding_passed}\n')
              f.write(f'onboarding_passed={onboarding_passed}\n')
          "
        id: aggregate

      - name: Comment on PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const testSummary = `
            ## ðŸ§ª Test Results Summary
            
            | Metric | Value |
            |--------|-------|
            | Total Tests | ${{ steps.aggregate.outputs.total_tests }} |
            | Success Rate | ${{ steps.aggregate.outputs.success_rate }}% |
            | Test Status | ${{ steps.aggregate.outputs.all_passed == 'true' && 'âœ… All Passed' || 'âŒ Some Failed' }} |
            | Onboarding Validation | ${{ steps.aggregate.outputs.onboarding_passed == 'true' && 'âœ… Passed' || 'âŒ Failed' }} |
            
            ### ðŸŽ¯ Test Categories
            - **Unit Tests**: Core functionality validation
            - **Integration Tests**: End-to-end workflow testing  
            - **Performance Tests**: Scalability and resource usage
            - **Security Tests**: Vulnerability and protection validation
            - **Onboarding Tests**: User experience and setup validation
            
            **Detailed results are available in the workflow artifacts.**
            `;
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: testSummary
            });

      - name: Fail if tests failed
        if: steps.aggregate.outputs.all_passed != 'true'
        run: |
          echo "âŒ Some tests failed. Check the detailed results above."
          exit 1

  # ===================================================================
  # Pre-flight Checks (keep existing validation section)
  # ===================================================================
  validation:
    name: ðŸ” Validation & Linting
    runs-on: ubuntu-latest
    outputs:
      should_build: ${{ steps.changes.outputs.should_build }}
      version: ${{ steps.version.outputs.version }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install validation dependencies
        run: |
          pip install -r requirements-core.txt
          pip install ruff black pytest

      - name: Validate environment configuration
        run: |
          python -c "from env_manager import load_environment; assert load_environment()"

      - name: Test onboarding functionality
        run: |
          # Test onboarding manager functionality
          python -c "
          import asyncio
          from src.utils.onboarding_manager import ensure_onboarding_complete, OnboardingManager
          
          async def test_onboarding():
              # Test onboarding manager creation
              manager = OnboardingManager()
              print('âœ… OnboardingManager created successfully')
              
              # Test configuration detection
              config_status = manager.detect_configuration_completeness()
              print(f'âœ… Configuration detection: {config_status}')
              
              # Test that onboarding doesn't block in CI
              # Set CI environment to skip interactive prompts
              import os
              os.environ['CI'] = 'true'
              os.environ['SKIP_ONBOARDING'] = 'true'
              
              result = await ensure_onboarding_complete()
              assert result == True, 'Onboarding should not block in CI'
              print('âœ… Onboarding bypassed correctly in CI environment')
          
          asyncio.run(test_onboarding())
          "
          
      - name: Check code formatting
        run: |
          black --check --diff .
          ruff check .

      - name: Validate requirements consistency
        run: |
          python -m pip check

      - name: Determine version
        id: version
        run: |
          if [[ "${{ github.ref }}" =~ ^refs/tags/v[0-9]+\.[0-9]+\.[0-9]+$ ]]; then
            VERSION="${{ github.ref_name }}"
          else
            VERSION="dev-$(git rev-parse --short HEAD)"
          fi
          echo "version=$VERSION" >> $GITHUB_OUTPUT
          echo "Version: $VERSION"

      - name: Check if build needed
        id: changes
        run: |
          # Always build on tags, main, or workflow_dispatch
          if [[ "${{ github.ref }}" =~ ^refs/tags/ ]] || [[ "${{ github.ref }}" == "refs/heads/main" ]] || [[ "${{ github.event_name }}" == "workflow_dispatch" ]]; then
            echo "should_build=true" >> $GITHUB_OUTPUT
          else
            # For PRs, check if relevant files changed
            git diff --name-only ${{ github.event.before }}..${{ github.sha }} > changed_files.txt
            if grep -E "\.(py|yml|yaml|txt|toml|spec)$|Dockerfile|requirements" changed_files.txt; then
              echo "should_build=true" >> $GITHUB_OUTPUT
            else
              echo "should_build=false" >> $GITHUB_OUTPUT
            fi
          fi

  # ===================================================================
  # Onboarding Experience Validation
  # ===================================================================
  onboarding_validation:
    name: ðŸŽ¯ Onboarding Experience Test
    runs-on: ubuntu-latest
    needs: validation
    if: needs.validation.outputs.should_build == 'true'
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          pip install -r requirements-core.txt
          pip install -r requirements-dev.txt

      - name: Test onboarding in clean environment
        run: |
          # Test onboarding with minimal configuration
          export ENV_MODE=testing
          export CI=true
          export SKIP_ONBOARDING=false  # Actually test onboarding logic
          
          # Remove any existing config to simulate fresh install
          rm -f .env .env.* .setup_complete || true
          
          # Test onboarding detection and flow
          python -c "
          import asyncio
          import os
          from src.utils.onboarding_manager import FirstRunDetector, OnboardingManager
          
          async def test_complete_onboarding():
              print('ðŸ§ª Testing complete onboarding experience...')
              
              # Test 1: Fresh installation detection
              detector = FirstRunDetector()
              is_first_run = detector.is_first_run()
              assert is_first_run, 'Fresh install should be detected as first run'
              print('âœ… Fresh installation correctly detected')
              
              # Test 2: Missing requirements detection
              missing = detector.get_missing_requirements()
              print(f'ðŸ“Š Missing requirements: {missing}')
              
              # Test 3: Configuration validation with sample config
              import tempfile
              from pathlib import Path
              
              # Create a sample valid config
              with tempfile.NamedTemporaryFile(mode='w', suffix='.env', delete=False) as f:
                  f.write('LLM_CHAT_API_URL=http://localhost:1234/v1\\n')
                  f.write('LLM_CHAT_API_KEY=not-needed\\n')
                  f.write('ENV_MODE=testing\\n')
                  sample_config = Path(f.name)
              
              is_valid = detector._has_valid_configuration(sample_config)
              assert is_valid, 'Sample config should be valid'
              print('âœ… Configuration validation working')
              
              # Cleanup
              os.unlink(sample_config)
              
              # Test 4: OnboardingManager creation and flow
              manager = OnboardingManager()
              assert hasattr(manager, 'detector'), 'Manager should have detector'
              assert hasattr(manager, 'user_preferences'), 'Manager should have preferences'
              print('âœ… OnboardingManager correctly initialized')
              
              # Test 5: Startup help generation
              help_text = detector.get_startup_help()
              assert isinstance(help_text, str), 'Help should be string'
              assert len(help_text) > 0, 'Help should not be empty'
              assert 'WhisperEngine' in help_text, 'Help should mention WhisperEngine'
              print('âœ… Startup help generation working')
              
              print('ðŸŽ‰ All onboarding tests passed!')
              
          asyncio.run(test_complete_onboarding())
          "

      - name: Test run.py with onboarding integration
        run: |
          # Test the actual run.py with onboarding integration
          export ENV_MODE=testing
          export CI=true
          export DEBUG_MODE=true
          export DISCORD_BOT_TOKEN=test_token_12345
          export LLM_CHAT_API_URL=http://localhost:1234/v1
          export LLM_MODEL_NAME=test-model
          
          # Create setup completion marker to skip interactive onboarding
          touch .setup_complete
          
          # Test that run.py can import and setup without errors
          python -c "
          import sys
          import os
          sys.path.insert(0, '.')
          
          # Test imports and basic setup
          from env_manager import load_environment
          from src.utils.onboarding_manager import ensure_onboarding_complete
          from src.utils.logging_config import setup_logging
          
          print('âœ… All key imports successful')
          
          # Test environment loading
          result = load_environment()
          assert result is True, 'Environment should load successfully'
          print('âœ… Environment loading successful')
          
          # Test logging setup
          setup_logging(debug=True, environment='testing')
          print('âœ… Logging setup successful')
          
          print('âœ… run.py integration test passed')
          "

      - name: Test onboarding skip in CI
        run: |
          # Test that onboarding is properly skipped in CI environments
          export CI=true
          export ENV_MODE=testing
          
          # Create setup completion marker to test non-first-run behavior
          touch .setup_complete
          
          python -c "
          import asyncio
          from src.utils.onboarding_manager import ensure_onboarding_complete
          
          async def test_non_first_run():
              # Should return True for non-first-run
              result = await ensure_onboarding_complete()
              assert result == True, 'Should complete successfully when setup exists'
              print('âœ… Onboarding correctly handled with existing setup')
          
          asyncio.run(test_non_first_run())
          "

      - name: Upload onboarding test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: onboarding-test-results
          path: |
            .setup_complete
            .env*
          retention-days: 7

  # ===================================================================
  # Docker Builds (Multi-architecture)
  # ===================================================================
  docker-build:
    name: ðŸ³ Docker Build
    runs-on: ubuntu-latest
    needs: validation
    if: needs.validation.outputs.should_build == 'true'
    permissions:
      contents: read
      packages: write   # Needed if publishing to GHCR (safe even if using Docker Hub)
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Compute derived tags
        id: vars
        run: |
          echo "short_sha=${GITHUB_SHA::12}" >> $GITHUB_OUTPUT
          echo "ref_slug=${GITHUB_REF_NAME//\//-}" >> $GITHUB_OUTPUT

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Determine push settings
        id: pushmeta
        run: |
          echo "event_name=${{ github.event_name }}" >> $GITHUB_OUTPUT
          if [ "${{ github.event_name }}" = "pull_request" ]; then
            echo "will_push=false" >> $GITHUB_OUTPUT
            echo "ðŸ”’ PR build: images will not be pushed.";
          else
            # Prefer DOCKERHUB_USERNAME/DOCKERHUB_TOKEN; fallback to legacy DOCKER_USERNAME/DOCKER_PASSWORD
            if { [ -n "${{ secrets.DOCKERHUB_USERNAME }}" ] && [ -n "${{ secrets.DOCKERHUB_TOKEN }}" ]; } || { [ -n "${{ secrets.DOCKER_USERNAME }}" ] && [ -n "${{ secrets.DOCKER_PASSWORD }}" ]; }; then
              echo "will_push=true" >> $GITHUB_OUTPUT
              if [ -n "${{ secrets.DOCKERHUB_USERNAME }}" ]; then
                echo "using_secret_set=DOCKERHUB" >> $GITHUB_OUTPUT
                echo "âœ… DockerHub (token) credentials detected; images will be pushed.";
              else
                echo "using_secret_set=LEGACY" >> $GITHUB_OUTPUT
                echo "âœ… Legacy Docker credentials detected; images will be pushed.";
              fi
            else
              echo "will_push=false" >> $GITHUB_OUTPUT
              echo "âš ï¸  No Docker credentials found; build only (no push).";
            fi
          fi

      - name: Log in to Docker Hub
        if: steps.pushmeta.outputs.will_push == 'true'
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ secrets.DOCKERHUB_USERNAME || secrets.DOCKER_USERNAME }}
          password: ${{ secrets.DOCKERHUB_TOKEN || secrets.DOCKER_PASSWORD }}

      - name: Extract metadata
        id: meta
        uses: docker/metadata-action@v5
        with:
          # Publish to both Docker Hub and GHCR (mirror)
          images: |
            ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}
            ghcr.io/${{ github.repository }}
          tags: |
            type=ref,event=branch
            type=ref,event=pr
            type=semver,pattern={{version}}
            type=semver,pattern={{major}}.{{minor}}
            type=raw,value=latest,enable={{is_default_branch}}
            type=raw,value=${{ needs.validation.outputs.version }}
            type=raw,value=sha-${{ github.sha }}
            type=raw,value=short-${{ steps.vars.outputs.short_sha }}
            type=raw,value=${{ steps.vars.outputs.ref_slug }}

      - name: Build (and optionally push) Docker image
        id: buildimage
        uses: docker/build-push-action@v6
        with:
          context: .
          file: ./docker/Dockerfile.multi-stage
          platforms: linux/amd64,linux/arm64
          target: production
          push: ${{ steps.pushmeta.outputs.will_push == 'true' }}
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha,mode=max
          sbom: true            # Generate SBOM (Software Bill of Materials)
          provenance: true      # Supply chain provenance (SLSA-style attestations)
          build-args: |
            VERSION=${{ needs.validation.outputs.version }}
            BUILD_DATE=${{ fromJSON(steps.meta.outputs.json).labels['org.opencontainers.image.created'] }}

      - name: Generate SBOM (image reference)
        if: steps.pushmeta.outputs.will_push == 'true'
        uses: anchore/sbom-action@v0
        with:
          image: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}@${{ steps.buildimage.outputs.digest }}
          format: spdx-json
          output-file: sbom-image-${{ steps.vars.outputs.short_sha }}.spdx.json

      - name: Generate SBOM (source fallback)
        if: steps.pushmeta.outputs.will_push != 'true'
        uses: anchore/sbom-action@v0
        with:
          path: .
          format: spdx-json
          output-file: sbom-source-${{ steps.vars.outputs.short_sha }}.spdx.json

      - name: Upload SBOM artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: sbom-${{ steps.vars.outputs.short_sha }}
          path: |
            sbom-*.spdx.json
          retention-days: 30

      - name: Log in to GHCR (mirror)
        if: steps.pushmeta.outputs.will_push == 'true'
        uses: docker/login-action@v3
        with:
            registry: ghcr.io
            username: ${{ github.actor }}
            password: ${{ secrets.GITHUB_TOKEN }}

      - name: Warn if push skipped due to missing credentials
        if: steps.pushmeta.outputs.will_push != 'true' && github.event_name != 'pull_request'
        run: |
          echo "âš ï¸  Docker credentials not provided; image not pushed. Set DOCKER_USERNAME & DOCKER_PASSWORD secrets to enable publishing." && exit 0

  # ===================================================================
  # Cross-Platform Native Builds
  # ===================================================================
  build-matrix:
    name: ðŸ“¦ Build ${{ matrix.os }}
    runs-on: ${{ matrix.runner }}
    needs: validation
    if: needs.validation.outputs.should_build == 'true' && (github.ref_type == 'tag' || github.event.inputs.build_type == 'release')
    
    strategy:
      fail-fast: false
      matrix:
        include:
          - os: macos
            runner: macos-latest
            arch: universal
            extension: .app
          - os: windows
            runner: windows-latest
            arch: x64
            extension: .exe
          - os: linux
            runner: ubuntu-latest
            arch: x64
            extension: ""

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install system dependencies (Ubuntu)
        if: matrix.os == 'linux'
        run: |
          sudo apt-get update
          sudo apt-get install -y build-essential

      - name: Set up virtual environment
        run: |
          python -m venv .venv
          
      - name: Activate virtual environment and install dependencies
        shell: bash
        run: |
          source .venv/bin/activate || .venv/Scripts/activate
          pip install --upgrade pip
          pip install -r requirements-core.txt
          pip install -r requirements-desktop.txt
          pip install pyinstaller

      - name: Build executable
        shell: bash
        run: |
          source .venv/bin/activate || .venv/Scripts/activate
          python build_cross_platform.py build --platform ${{ matrix.os }}

      - name: Package build artifacts
        shell: bash
        run: |
          cd dist
          if [ "${{ matrix.os }}" = "macos" ]; then
            tar -czf WhisperEngine-${{ needs.validation.outputs.version }}-macos.tar.gz WhisperEngine.app
          elif [ "${{ matrix.os }}" = "windows" ]; then
            7z a WhisperEngine-${{ needs.validation.outputs.version }}-windows.zip WhisperEngine.exe
          else
            tar -czf WhisperEngine-${{ needs.validation.outputs.version }}-linux.tar.gz WhisperEngine
          fi

      - name: Upload build artifacts
        uses: actions/upload-artifact@v4
        with:
          name: WhisperEngine-${{ matrix.os }}-${{ needs.validation.outputs.version }}
          path: dist/WhisperEngine-${{ needs.validation.outputs.version }}-*
          retention-days: 30

  # ===================================================================
  # Release Creation
  # ===================================================================
  release:
    name: ðŸš€ Create Release
    runs-on: ubuntu-latest
    needs: [validation, docker-build, build-matrix]
    if: github.ref_type == 'tag' && startsWith(github.ref, 'refs/tags/v')
    permissions:
      contents: write   # Required to create release & upload assets
      packages: write   # Allow publishing package metadata / GHCR tags
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download all artifacts
        uses: actions/download-artifact@v4
        with:
          path: ./artifacts

      - name: Create release
        uses: softprops/action-gh-release@v2
        with:
          tag_name: ${{ github.ref_name }}
          name: WhisperEngine ${{ github.ref_name }}
          draft: false
          prerelease: ${{ contains(github.ref_name, '-') }}
          generate_release_notes: true
          files: |
            ./artifacts/*/WhisperEngine-*
          body: |
            ## ðŸŽ‰ WhisperEngine ${{ github.ref_name }}
            
            ### ðŸ“¦ Downloads
            - **Desktop App (macOS)**: WhisperEngine-${{ github.ref_name }}-macos.tar.gz
            - **Desktop App (Windows)**: WhisperEngine-${{ github.ref_name }}-windows.zip  
            - **Desktop App (Linux)**: WhisperEngine-${{ github.ref_name }}-linux.tar.gz
            - **Docker Image**: `docker pull whisperengine/whisperengine:${{ github.ref_name }}`
            
            ### ðŸš€ Quick Start
            ```bash
            # Docker (Recommended)
            docker run -d --name whisperengine whisperengine/whisperengine:${{ github.ref_name }}
            
            # Native Installation
            ./setup.sh
            ```
            
            ### ðŸ“‹ What's Changed
            See the release notes below for detailed changes.

  # ===================================================================
  # Post-Build Validation  
  # ===================================================================
  validate-builds:
    name: âœ… Validate Builds
    runs-on: ubuntu-latest
    needs: [docker-build, onboarding_validation]
    if: always() && needs.validation.outputs.should_build == 'true'
    permissions:
      contents: read
    
    steps:
      - name: Test Docker image
        if: needs.docker-build.result == 'success'
        run: |
          docker run --rm \
            -e DISCORD_BOT_TOKEN=test_token \
            -e DEBUG_MODE=true \
            --entrypoint python \
            ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ needs.validation.outputs.version }} \
            -c "import src.main; print('âœ… Import test passed')"

      - name: Test Docker onboarding experience
        if: needs.docker-build.result == 'success'
        run: |
          # Test that onboarding works correctly in Docker environment
          docker run --rm \
            -e CI=true \
            -e SKIP_ONBOARDING=true \
            -e DEBUG_MODE=true \
            -e ENV_MODE=testing \
            --entrypoint python \
            ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ needs.validation.outputs.version }} \
            -c "
          import asyncio
          from src.utils.onboarding_manager import ensure_onboarding_complete
          
          async def test():
              result = await ensure_onboarding_complete()
              assert result == True, 'Onboarding should work in Docker'
              print('âœ… Docker onboarding test passed')
          
          asyncio.run(test())
          "

      - name: Report status
        run: |
          echo "## ðŸ“Š Build Results" >> $GITHUB_STEP_SUMMARY
          echo "| Component | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|-----------|--------|" >> $GITHUB_STEP_SUMMARY
          echo "| Docker Build | ${{ needs.docker-build.result == 'success' && 'âœ… Success' || 'âŒ Failed' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Onboarding Validation | ${{ needs.onboarding_validation.result == 'success' && 'âœ… Success' || 'âŒ Failed' }} |" >> $GITHUB_STEP_SUMMARY
          if [ "${{ github.ref_type }}" = "tag" ]; then
            echo "| Native Builds | ${{ needs.build-matrix.result == 'success' && 'âœ… Success' || 'âŒ Failed' }} |" >> $GITHUB_STEP_SUMMARY
          fi