#!/usr/bin/env python3
"""
GPU Detection Test for WhisperEngine Desktop App
Checks if the current model configuration is using GPU acceleration.
"""

import os
import sys
from pathlib import Path

# Add the project root to Python path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# Load environment
from env_manager import load_environment

if not load_environment():
    print("‚ùå Failed to load environment")
    sys.exit(1)


def check_gpu_availability():
    """Check GPU availability and configuration"""
    print("üîç Checking GPU Availability...")

    try:
        import torch

        print(f"‚úÖ PyTorch version: {torch.__version__}")

        # Check CUDA availability
        cuda_available = torch.cuda.is_available()
        print(f"üî• CUDA available: {cuda_available}")

        if cuda_available:
            print(f"   - CUDA devices: {torch.cuda.device_count()}")
            for i in range(torch.cuda.device_count()):
                device_name = torch.cuda.get_device_name(i)
                print(f"   - Device {i}: {device_name}")
                memory_total = torch.cuda.get_device_properties(i).total_memory / 1024**3
                print(f"   - Memory: {memory_total:.1f} GB")

        # Check MPS (Apple Silicon) availability
        mps_available = torch.backends.mps.is_available()
        print(f"üçé MPS (Apple Silicon) available: {mps_available}")

        return cuda_available or mps_available

    except ImportError:
        print("‚ùå PyTorch not available")
        return False
    except Exception as e:
        print(f"‚ùå Error checking GPU: {e}")
        return False


def check_model_configuration():
    """Check current model configuration"""
    print("\nüìã Current Model Configuration:")

    # Check environment variables
    llm_url = os.getenv("LLM_CHAT_API_URL", "not_set")
    model_name = os.getenv("LLM_MODEL_NAME", "not_set")
    local_model = os.getenv("LOCAL_LLM_MODEL", "not_set")
    use_local = os.getenv("USE_LOCAL_LLM", "false")

    print(f"   - LLM_CHAT_API_URL: {llm_url}")
    print(f"   - LLM_MODEL_NAME: {model_name}")
    print(f"   - LOCAL_LLM_MODEL: {local_model}")
    print(f"   - USE_LOCAL_LLM: {use_local}")

    # Determine the execution path
    is_local_llm = llm_url == "local://"
    print(
        f"\nüí° Model Execution Path: {'Local LLM (Transformers)' if is_local_llm else 'External API'}"
    )

    return is_local_llm, model_name, local_model


def test_model_device_usage():
    """Test actual model device usage"""
    print("\nüß™ Testing Model Device Usage...")

    try:
        from src.llm.llm_client import LLMClient
        import torch

        # Initialize LLM client
        client = LLMClient()

        if hasattr(client, "local_model") and client.local_model is not None:
            print("‚úÖ Local model is loaded")

            # Check which device the model is on
            device = next(client.local_model.parameters()).device
            print(f"üîß Model device: {device}")

            # Check model dtype
            dtype = next(client.local_model.parameters()).dtype
            print(f"üìä Model dtype: {dtype}")

            # Test a small inference to see actual device usage
            if torch.cuda.is_available():
                print("üî• CUDA detected - checking GPU memory usage...")
                torch.cuda.empty_cache()
                memory_before = torch.cuda.memory_allocated() if torch.cuda.is_available() else 0

                # Try a small inference
                test_input = client.local_tokenizer("Hello", return_tensors="pt")
                with torch.no_grad():
                    _ = client.local_model(**test_input)

                memory_after = torch.cuda.memory_allocated() if torch.cuda.is_available() else 0
                memory_used = (memory_after - memory_before) / 1024**2  # MB
                print(f"   GPU memory used for inference: {memory_used:.1f} MB")

            elif torch.backends.mps.is_available():
                print("üçé MPS detected - model should be using Apple Silicon GPU")

            return True

        else:
            print("‚ùå Local model not loaded - likely using external API")
            return False

    except Exception as e:
        print(f"‚ùå Error testing model device: {e}")
        import traceback

        traceback.print_exc()
        return False


def main():
    """Main test function"""
    print("üöÄ WhisperEngine GPU Detection Test")
    print("=" * 50)

    # Check GPU availability
    gpu_available = check_gpu_availability()

    # Check model configuration
    is_local, model_name, local_model = check_model_configuration()

    # Test actual model usage if local
    if is_local:
        model_on_gpu = test_model_device_usage()
    else:
        model_on_gpu = False
        print("\nüí° External API mode - GPU usage depends on the API service")

    # Summary
    print("\n" + "=" * 50)
    print("üìà SUMMARY:")
    print(f"   üî• GPU Available: {gpu_available}")
    print(f"   üè† Using Local Model: {is_local}")
    print(f"   üéØ Model on GPU: {model_on_gpu if is_local else 'N/A (External API)'}")

    if is_local and gpu_available and not model_on_gpu:
        print("\n‚ö†Ô∏è  POTENTIAL ISSUE:")
        print("   GPU is available but model may not be using it")
        print("   Consider checking model loading configuration")
    elif is_local and not gpu_available:
        print("\nüìù INFO:")
        print("   Model running on CPU (no GPU detected)")
    elif model_on_gpu:
        print("\n‚úÖ OPTIMAL:")
        print("   Model is using GPU acceleration!")


if __name__ == "__main__":
    main()
