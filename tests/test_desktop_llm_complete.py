#!/usr/bin/env python3
"""
Test Complete Desktop LLM Integration
Tests the full desktop app LLM initialization and auto-configuration flow.
"""

import asyncio
import logging
import os

# Set up logging
logging.basicConfig(level=logging.INFO, format="%(levelname)s - %(message)s")
logger = logging.getLogger(__name__)


async def test_desktop_llm_initialization():
    """Test desktop LLM initialization flow"""
    logger.info("üñ•Ô∏è Testing Desktop LLM Initialization...")

    try:
        from src.llm.desktop_llm_manager import DesktopLLMManager

        manager = DesktopLLMManager()

        # Test initialization
        result = await manager.initialize_llm_for_desktop()

        logger.info("üìä Initialization Result:")
        logger.info(f"   Status: {result.get('status', 'unknown')}")
        logger.info(f"   Next Steps: {', '.join(result.get('next_steps', []))}")

        if result.get("server_info"):
            server = result["server_info"]
            logger.info(f"   ‚úÖ Server: {server.name} at {server.url}")
            logger.info(
                f"   üì¶ Models: {', '.join(server.models[:2])}{'...' if len(server.models) > 2 else ''}"
            )

        if result.get("configuration"):
            config = result["configuration"]
            logger.info("   üîß Configuration Applied:")
            for key, value in config.items():
                if "API_KEY" in key:
                    logger.info(f"      {key}: {'[SET]' if value else '[NOT SET]'}")
                else:
                    logger.info(f"      {key}: {value}")

        if result.get("setup_guidance"):
            guidance = result["setup_guidance"]
            logger.info(f"   üí° Setup Guidance: {guidance.preferred_server}")
            logger.info(f"   üìù Memory Note: {guidance.memory_note}")

        return True, result

    except Exception as e:
        logger.error(f"‚ùå Desktop LLM initialization failed: {e}")
        return False, {}


async def test_llm_validation():
    """Test LLM configuration validation"""
    logger.info("\nüîç Testing LLM Configuration Validation...")

    try:
        from src.llm.desktop_llm_manager import validate_desktop_llm

        validation_result = await validate_desktop_llm()

        logger.info("üìä Validation Result:")
        logger.info(f"   Valid: {validation_result.get('is_valid', False)}")
        logger.info(f"   Service: {validation_result.get('service_name', 'Unknown')}")
        logger.info(f"   URL: {validation_result.get('api_url', 'Not set')}")
        logger.info(f"   Model: {validation_result.get('model_name', 'Not set')}")

        if validation_result.get("test_response"):
            response = validation_result["test_response"]
            logger.info(
                f"   üß™ Test Response: {response[:100]}{'...' if len(response) > 100 else ''}"
            )
            logger.info(
                f"   ‚úÖ Response Test: {'Passed' if validation_result.get('response_test_passed') else 'Failed'}"
            )

        if validation_result.get("error"):
            logger.info(f"   ‚ùå Error: {validation_result['error']}")

        return True, validation_result

    except Exception as e:
        logger.error(f"‚ùå LLM validation failed: {e}")
        return False, {}


async def test_ui_guidance_formatting():
    """Test setup guidance formatting for UI display"""
    logger.info("\nüé® Testing UI Guidance Formatting...")

    try:
        from src.llm.desktop_llm_manager import DesktopLLMManager
        from src.llm.local_server_detector import LocalLLMDetector

        # Get a setup recommendation
        detector = LocalLLMDetector()
        resources = detector.get_system_resources()
        recommendation = detector.get_setup_recommendation(resources)

        # Format for UI
        manager = DesktopLLMManager()
        ui_guidance = manager.get_setup_guidance_for_ui(recommendation)

        logger.info("üì± UI Guidance Format:")
        logger.info(f"   Title: {ui_guidance.get('title', 'No title')}")
        logger.info(f"   Description: {ui_guidance.get('description', 'No description')}")
        logger.info(f"   Setup URL: {ui_guidance.get('setup_url', 'No URL')}")

        if ui_guidance.get("benefits"):
            logger.info("   üíé Benefits:")
            for benefit in ui_guidance["benefits"]:
                logger.info(f"      {benefit}")

        if ui_guidance.get("steps"):
            logger.info("   üìã Steps:")
            for step in ui_guidance["steps"][:3]:  # Show first 3 steps
                logger.info(f"      {step}")
            if len(ui_guidance["steps"]) > 3:
                logger.info(f"      ... ({len(ui_guidance['steps']) - 3} more steps)")

        return True, ui_guidance

    except Exception as e:
        logger.error(f"‚ùå UI guidance formatting failed: {e}")
        return False, {}


async def test_complete_workflow():
    """Test the complete desktop LLM workflow"""
    logger.info("\nüîÑ Testing Complete Desktop LLM Workflow...")

    try:
        # Save current environment
        original_env = {
            "LLM_CHAT_API_URL": os.getenv("LLM_CHAT_API_URL"),
            "LLM_CHAT_API_KEY": os.getenv("LLM_CHAT_API_KEY"),
            "LLM_MODEL_NAME": os.getenv("LLM_MODEL_NAME"),
        }

        # Clear environment to simulate fresh install
        for key in original_env:
            if key in os.environ:
                del os.environ[key]

        logger.info("üßπ Cleared environment to simulate fresh install")

        # Test 1: Fresh initialization
        from src.llm.desktop_llm_manager import initialize_desktop_llm

        init_result = await initialize_desktop_llm()
        logger.info(f"‚úÖ Fresh initialization: {init_result.get('status')}")

        # Test 2: Validation after initialization
        from src.llm.desktop_llm_manager import validate_desktop_llm

        validation_result = await validate_desktop_llm()
        logger.info(
            f"‚úÖ Post-init validation: {'Valid' if validation_result.get('is_valid') else 'Invalid'}"
        )

        # Test 3: Check if we can make an actual request (if server available)
        if validation_result.get("is_valid"):
            try:
                from src.llm.llm_client import LLMClient

                client = LLMClient()

                test_message = "Hello! Just testing local LLM integration. Please respond with 'Local LLM working!'"
                response = client.get_chat_response([{"role": "user", "content": test_message}])

                if response and len(response) > 0:
                    logger.info(f"‚úÖ Live test successful: {response[:50]}...")
                    working = True
                else:
                    logger.warning("‚ö†Ô∏è Live test returned empty response")
                    working = False
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Live test failed: {str(e)[:50]}...")
                working = False
        else:
            working = False

        # Restore original environment
        for key, value in original_env.items():
            if value is not None:
                os.environ[key] = value
            elif key in os.environ:
                del os.environ[key]

        logger.info("üîÑ Restored original environment")

        return True, {
            "initialization": init_result,
            "validation": validation_result,
            "live_test_working": working,
        }

    except Exception as e:
        logger.error(f"‚ùå Complete workflow test failed: {e}")
        return False, {}


async def main():
    """Run all desktop LLM integration tests"""
    logger.info("üß™ Desktop LLM Integration Test Suite")
    logger.info("=" * 60)

    all_tests_passed = True

    # Test 1: Desktop LLM initialization
    logger.info("\n1Ô∏è‚É£ Testing Desktop LLM Initialization...")
    init_success, init_result = await test_desktop_llm_initialization()
    if not init_success:
        all_tests_passed = False

    # Test 2: LLM validation
    logger.info("\n2Ô∏è‚É£ Testing LLM Configuration Validation...")
    validation_success, validation_result = await test_llm_validation()
    if not validation_success:
        all_tests_passed = False

    # Test 3: UI guidance formatting
    logger.info("\n3Ô∏è‚É£ Testing UI Guidance Formatting...")
    ui_success, ui_guidance = await test_ui_guidance_formatting()
    if not ui_success:
        all_tests_passed = False

    # Test 4: Complete workflow
    logger.info("\n4Ô∏è‚É£ Testing Complete Desktop LLM Workflow...")
    workflow_success, workflow_result = await test_complete_workflow()
    if not workflow_success:
        all_tests_passed = False

    # Final results
    logger.info("\n" + "=" * 60)
    if all_tests_passed:
        logger.info("üéâ ALL TESTS PASSED - Desktop LLM integration complete!")

        # Summarize what's working
        if validation_result.get("is_valid"):
            logger.info(f"‚úÖ LLM Ready: {validation_result.get('service_name')}")
            if workflow_result.get("live_test_working"):
                logger.info("üöÄ Live conversations working!")
            else:
                logger.info("‚ö†Ô∏è Configuration valid but live test failed")
        else:
            logger.info("‚ö†Ô∏è LLM setup required - guidance provided")

        logger.info("\nüèóÔ∏è Desktop LLM Integration Summary:")
        logger.info("‚Ä¢ Auto-detection: ‚úÖ Finds local servers automatically")
        logger.info("‚Ä¢ Configuration: ‚úÖ Applies settings automatically")
        logger.info("‚Ä¢ Validation: ‚úÖ Tests connections before use")
        logger.info("‚Ä¢ Setup guidance: ‚úÖ Helps users get started")
        logger.info("‚Ä¢ UI formatting: ‚úÖ Ready for desktop app display")
        logger.info("‚Ä¢ Complete workflow: ‚úÖ End-to-end functionality")

        logger.info("\nüí° Recommendation: Enhanced HTTP API approach is optimal!")
        logger.info("‚úÖ Keeps existing proven architecture")
        logger.info("‚úÖ Adds intelligent auto-detection")
        logger.info("‚úÖ Provides seamless desktop experience")
        logger.info("‚úÖ Maintains cloud API fallback options")

    else:
        logger.error("‚ùå SOME TESTS FAILED - Check implementation")

    return all_tests_passed


if __name__ == "__main__":
    success = asyncio.run(main())
    exit(0 if success else 1)
