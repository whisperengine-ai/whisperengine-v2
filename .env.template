# =======================================================
# WhisperEngine Multi-Bot Environment Template
# =======================================================
# Copy this file for each bot: .env.botname
# Customize the bot-specific sections below
# =======================================================

# =======================================================
# üö® REQUIRED: Bot Identity (MUST BE UNIQUE)
# =======================================================
DISCORD_BOT_TOKEN=YOUR_BOT_TOKEN_HERE
DISCORD_BOT_NAME=YourBotName  # CRITICAL: Must match database character name
HEALTH_CHECK_PORT=9091  # Increment: 9091, 9092, 9093...
CONTAINER_NAME=your-bot  # For logging identification

# =======================================================
# üë• OPTIONAL: Bot-Specific Administration
# =======================================================
ADMIN_USER_IDS=  # Could be different per bot
LOG_LEVEL=INFO  # Could use DEBUG for specific bots

# =======================================================
# üß† OPTIONAL: Bot-Specific AI Configuration
# =======================================================
# Different characters could use different models
LLM_CHAT_MODEL=x-ai/grok-4-fast:free  # Elena: scientific model, Marcus: research model

# ==============================================================================
# FACT & PREFERENCE EXTRACTION CONFIGURATION
# ==============================================================================

# Runtime Fact Extraction (optional - default: true for backward compatibility)
# Controls whether facts are extracted during message processing (adds 200-500ms latency)
# Default 'true' maintains compatibility with main branch during gradual migration
# FUTURE: Set to 'false' after migration complete (enrichment worker handles asynchronously with better quality)
# Set to 'false' to disable runtime extraction and rely solely on enrichment worker
ENABLE_RUNTIME_FACT_EXTRACTION=true

# Runtime Preference Extraction (optional - default: true for backward compatibility)
# Controls whether user preferences are extracted during message processing (regex-based, limited)
# Default 'true' maintains compatibility with main branch during gradual migration
# FUTURE: Set to 'false' after migration complete (enrichment worker handles with LLM analysis + conversation context)
# Set to 'false' to disable runtime extraction and rely solely on enrichment worker
ENABLE_RUNTIME_PREFERENCE_EXTRACTION=true

# Fact Extraction Model Configuration (optional - lighter/cheaper model)
# If not set, uses LLM_CHAT_MODEL for fact extraction
# Recommended: Use faster/cheaper model for background fact extraction
# Examples: gpt-3.5-turbo, mistral/mistral-small, anthropic/claude-3-haiku
# Leave empty to use LLM_CHAT_MODEL
LLM_FACT_EXTRACTION_MODEL=  

# Fact Extraction Temperature (optional - default: 0.2)
# Lower temperature = more consistent, deterministic fact extraction
# DO NOT use same temperature as chat (LLM_TEMPERATURE) - fact extraction needs consistency
# Recommended: 0.1-0.3 for factual accuracy
# Leave empty to use default 0.2
LLM_FACT_EXTRACTION_TEMPERATURE=0.2


# RoBERTa Emotion Model Configuration
ROBERTA_EMOTION_MODEL_NAME=cardiffnlp/twitter-roberta-base-emotion-multilabel-latest

# Different engagement styles per character
ENGAGEMENT_ENGINE_TYPE=full  # Could be: full, basic, disabled per bot

# Proactive Engagement Configuration
ENGAGEMENT_STAGNATION_THRESHOLD_MINUTES=5  # Minutes before conversation considered stagnant
ENGAGEMENT_CHECK_INTERVAL_MINUTES=3        # How often to check engagement levels
ENGAGEMENT_MAX_SUGGESTIONS_PER_HOUR=8      # Limit proactive interventions per hour

# =======================================================
# ‚ö° CONCURRENT CONVERSATION MANAGEMENT (Always Enabled)
# =======================================================
# ConcurrentConversationManager is now required for optimal performance
MAX_CONCURRENT_SESSIONS=1000                   # Maximum concurrent conversation sessions
MAX_WORKER_THREADS=0                           # Thread pool size (0 = auto-detect)
MAX_WORKER_PROCESSES=0                         # Process pool size (0 = auto-detect)  
SESSION_TIMEOUT_MINUTES=30                     # Session timeout in minutes

# =======================================================
# üöÄ STARTUP RELIABILITY (Prevent Race Conditions)
# =======================================================
# Startup delays to prevent race conditions and ensure dependencies are ready
STARTUP_DEPENDENCY_WAIT_SECONDS=15             # Wait for PostgreSQL/Qdrant health checks
STARTUP_COMPONENT_INIT_DELAY_SECONDS=5         # Delay between component initializations
STARTUP_MAX_RETRY_ATTEMPTS=5                   # Max retries for component initialization
STARTUP_RETRY_DELAY_SECONDS=3                  # Delay between retry attempts

# =======================================================
# üé§ OPTIONAL: Bot-Specific Voice Configuration  
# =======================================================
VOICE_SERVICE_TYPE=discord_elevenlabs  # Could vary per character
# Voice text channel matching: bot responds ONLY in text channels that correspond to the voice channel
# Set VOICE_USE_PATTERN_FALLBACK=true to enable broader pattern matching as fallback
VOICE_USE_PATTERN_FALLBACK=false  # Precise matching by default
VOICE_TEXT_CHANNELS=voice,chat,general,bot  # Only used if pattern fallback enabled
ELEVENLABS_API_KEY=your_key_here  # Different voices/accounts per bot

# =======================================================
# ü§ù SHARED: Infrastructure (SAME FOR ALL BOTS)
# =======================================================
# LLM API (usually shared) - Default to local LM Studio
LLM_CLIENT_TYPE=lmstudio
LLM_CHAT_API_URL=http://host.docker.internal:1234/v1
LLM_CHAT_API_KEY=

# Support for vision/image analysis
LLM_SUPPORTS_VISION=true
LLM_VISION_MAX_IMAGES=5

# Database connections (shared infrastructure)
POSTGRES_HOST=postgres
POSTGRES_PORT=5432
POSTGRES_DB=whisperengine
POSTGRES_USER=whisperengine
POSTGRES_PASSWORD=whisperengine_password
POSTGRES_MIN_CONNECTIONS=5
POSTGRES_MAX_CONNECTIONS=25

REDIS_HOST=redis
REDIS_PORT=6379
REDIS_DB=0

QDRANT_HOST=qdrant
QDRANT_PORT=6333
QDRANT_GRPC_PORT=6334

# InfluxDB (Time-series data for temporal intelligence)
INFLUXDB_URL=http://influxdb:8086
INFLUXDB_TOKEN=whisperengine-fidelity-first-metrics-token
INFLUXDB_ORG=whisperengine
INFLUXDB_BUCKET=performance_metrics

# Memory system (shared)
MEMORY_SYSTEM_TYPE=vector
VECTOR_DIMENSION=384
# Use sentence-transformers/all-MiniLM-L6-v2 - 384 dims, best quality, excellent speed
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2

# Model Cache Configuration (Pre-downloaded models)
MODEL_CACHE_DIR=/app/models
DISABLE_MODEL_DOWNLOAD=true

# Emoji Intelligence Configuration  
EMOJI_ENABLED=true
EMOJI_BASE_THRESHOLD=0.4
EMOJI_NEW_USER_THRESHOLD=0.3
VISUAL_REACTION_ENABLED=true

# Global feature flags (usually same)
DEBUG_MODE=false
PYTHONUNBUFFERED=1
DISABLE_MEMORY_IMPORTANCE=false

# Prompt Logging Configuration
# WARNING: Prompt logging writes EVERY conversation to logs/prompts/ directory
# This includes full conversation context and LLM responses for debugging
# DISABLE in production to reduce disk I/O and protect user privacy
ENABLE_PROMPT_LOGGING=false

# =======================================================
# ÔøΩ SECURITY: Mystical Symbol Detection
# =======================================================
# Silently ignores messages with high density of mystical/esoteric symbols
# or ritualistic math patterns (spam/unwanted content)
# Default: false (disabled) - Set to 'true' to enable filtering
# Thresholds: 20% symbol density + 3 minimum symbols, or ritualistic math patterns
ENABLE_MYSTICAL_SYMBOL_FILTER=false

# =======================================================
# ÔøΩüí° ADVANCED: Character-Specific Customizations
# =======================================================
# You could even have character-specific settings like:

# Elena (Marine Biologist) - More scientific/cautious
# LLM_CHAT_MODEL=microsoft/wizardlm-2-8x22b  # More analytical model
# ENABLE_PROACTIVE_ENGAGEMENT=false  # More reserved personality

# Marcus (AI Researcher) - More experimental  
# LLM_CHAT_MODEL=anthropic/claude-3.5-sonnet  # Research-focused model
# ENABLE_PROACTIVE_ENGAGEMENT=true  # More engaging personality

# Dream (Mystical) - More creative
# LLM_CHAT_MODEL=openai/gpt-4-turbo  # More creative model
# ENGAGEMENT_ENGINE_TYPE=full  # Highly engaging