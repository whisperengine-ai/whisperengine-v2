# =======================================================
# WhisperEngine Configuration Template - OPTIMIZED 
# Copy this to .env and configure for your setup
# 
# üöÄ QUICK START:
#   1. Copy this file to .env
#   2. Set DISCORD_BOT_TOKEN with your bot's token
#   3. Choose OpenRouter for cloud LLM (single API endpoint)
#   4. Run: python run.py
#
# üßô INTERACTIVE SETUP: python setup_wizard.py
# üîç VALIDATE CONFIG: python env_manager.py --validate
# =======================================================

# =======================================================
# LOGGING CONFIGURATION GUIDE
# =======================================================
# NEW FEATURE: Separate Console and File Logging Levels
#
# LOG_LEVEL controls what gets written to log files
# CONSOLE_LOG_LEVEL controls what appears on your terminal/console
#
# Example configurations:
# ‚Ä¢ Development (verbose files, clean console):
#   LOG_LEVEL=DEBUG + CONSOLE_LOG_LEVEL=ERROR
#   ‚Üí All debug info in files, only errors on console
#
# ‚Ä¢ Production (balanced):
#   LOG_LEVEL=INFO + CONSOLE_LOG_LEVEL=WARNING
#   ‚Üí Standard info in files, warnings+ on console
#
# ‚Ä¢ Debugging (everything everywhere):
#   LOG_LEVEL=DEBUG + CONSOLE_LOG_LEVEL=DEBUG
#   ‚Üí Full verbosity in both files and console
#
# Available levels (most to least verbose):
# DEBUG ‚Üí INFO ‚Üí WARNING ‚Üí ERROR ‚Üí CRITICAL
# =======================================================

# =======================================================
# SYSTEM CONFIGURATION
# =======================================================
# Disable HuggingFace tokenizers parallelism warnings
TOKENIZERS_PARALLELISM=false

# Memory Optimization Features
ENABLE_MEMORY_SUMMARIZATION=true            # Intelligent conversation summarization
ENABLE_MEMORY_DEDUPLICATION=true            # Remove redundant memories
ENABLE_MEMORY_CLUSTERING=true               # Organize memories by topics
ENABLE_MEMORY_PRIORITIZATION=true           # Smart context ranking
MEMORY_OPTIMIZATION_INTERVAL=24             # Hours between optimization cycles

ENABLE_PRODUCTION_OPTIMIZATION=true

# =======================================================
# DEPLOYMENT MODE (auto-detected if not set)
# =======================================================
# ENV_MODE=development
# Options: development, desktop, discord, production

# =======================================================
# DISCORD CONFIGURATION
# =======================================================
# Discord Bot Token (required for Discord bot mode)
# Get from: https://discord.com/developers/applications
DISCORD_BOT_TOKEN=your_discord_bot_token_here

# Bot Configuration
DISCORD_BOT_NAME=whisperengine
ADMIN_USER_IDS=your_discord_user_id_here
DISCORD_RESPOND_MODE=mention
DISCORD_MESSAGE_TRACE=false

# Demo Bot Flag (shows data handling warnings)
DEMO_BOT=false

# =======================================================
# LLM CONFIGURATION - SIMPLIFIED (ONE API ENDPOINT)
# OpenRouter provides access to multiple models via single API
# =======================================================

# üè† OPTION 1: LOCAL LM STUDIO (Free, Private, Recommended for beginners)
# Download from: https://lmstudio.ai
# 1. Download LM Studio
# 2. Download a model (e.g., Llama 2 7B Chat)  
# 3. Start local server (default port 1234)
# 4. Use these settings:
LLM_CHAT_API_URL=http://localhost:1234/v1
LLM_CHAT_API_KEY=not-needed
LLM_CHAT_MODEL=local-model
LLM_MAX_TOKENS_CHAT=4096

# ü¶ô OPTION 2: LOCAL OLLAMA (Free, Private, Command-line)
# Install from: https://ollama.ai
# 1. Install Ollama
# 2. Run: ollama pull llama2
# 3. Run: ollama serve
# 4. Use these settings:
# LLM_CHAT_API_URL=http://localhost:11434/v1
# LLM_CHAT_API_KEY=not-needed
# LLM_CHAT_MODEL=llama2
# LLM_MAX_TOKENS_CHAT=4096

# üåê OPTION 3: OPENAI (Paid, High Quality)
# Get API key from: https://platform.openai.com/api-keys
# LLM_CHAT_API_URL=https://api.openai.com/v1
# LLM_CHAT_API_KEY=your_openai_api_key_here
# LLM_CHAT_MODEL=gpt-4
# LLM_MAX_TOKENS_CHAT=4096

# =======================================================
# Get API key from: https://openrouter.ai/keys  
LLM_CHAT_API_URL=https://openrouter.ai/api/v1
LLM_CHAT_API_KEY=your_openrouter_api_key_here
LLM_MODEL_NAME=openai/gpt-4o
LLM_CHAT_MODEL=openai/gpt-4o

# LLM Generation Controls
LLM_TEMPERATURE=0.55
LLM_MAX_TOKENS_CHAT=3072
LLM_MAX_TOKENS_COMPLETION=1024
LLM_MAX_TOKENS_PERSONAL_INFO=400
LLM_MAX_TOKENS_TRUST_DETECTION=300
LLM_MAX_TOKENS_USER_FACTS=400
STRICT_IMMERSIVE_MODE=true

# Vision Support
LLM_SUPPORTS_VISION=true
LLM_VISION_MAX_IMAGES=3

# =======================================================
# LOCAL AI SYSTEMS (OPTIMIZED) - No additional APIs needed
# =======================================================
LLM_CONNECTION_TIMEOUT=10

# === LLM Behavior Settings ===
STRICT_IMMERSIVE_MODE=true                  # Enable strict immersive mode for better roleplay

# === Vision Support ===
LLM_SUPPORTS_VISION=false
LLM_VISION_MAX_IMAGES=5

# Hybrid Vision Summarizer (secondary model) configuration
VISION_SUMMARIZER_ENABLED=true
VISION_SUMMARIZER_API_URL=http://127.0.0.1:1235/v1
VISION_SUMMARIZER_MODEL=llava-phi-3-mini
VISION_SUMMARIZER_MAX_IMAGES=3
VISION_SUMMARIZER_TIMEOUT=25
VISION_SUMMARIZER_MAX_TOKENS=180
VISION_SUMMARIZER_TEMPERATURE=0.2
# When primary model lacks native vision, image attachments are summarized via this service
# and inserted as a concise system or user proxy message.

# === Local Model Configuration ===
LOCAL_LLM_MODEL=microsoft_Phi-3-mini-4k-instruct

# =======================================================
# DATABASE CONFIGURATION
# =======================================================

# === SQLite (Default - Local Storage) ===
USE_SQLITE=true

# === PostgreSQL (Production) ===
USE_POSTGRESQL=false
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=whisper_engine
POSTGRES_USER=bot_user
POSTGRES_PASSWORD=bot_password_change_me
POSTGRES_MIN_CONNECTIONS=5
POSTGRES_MAX_CONNECTIONS=20

# === Redis (High-Performance Caching) ===
# üöÄ PERFORMANCE: Redis provides significant speed improvements for memory retrieval
# Memory cache hits reduce response times from 100ms+ to <5ms
USE_REDIS_CACHE=false                      # Enable Redis caching for memory/profile retrieval
REDIS_HOST=localhost                       # Redis server hostname (use "redis" for Docker)
REDIS_PORT=6379                           # Redis server port
REDIS_DB=0                                # Redis database number (0-15)
REDIS_PASSWORD=                           # Redis password (leave empty if no auth)
REDIS_CACHE_TTL_MINUTES=20                # Time-to-live for cached data (default: 20 minutes)

# === ChromaDB (Vector Database) ===
# ChromaDB always runs in HTTP client mode for containerized deployments
CHROMADB_HOST=localhost                   # Use "chromadb" for Docker mode
CHROMADB_PORT=8000
CHROMADB_COLLECTION_NAME=user_memories
CHROMADB_GLOBAL_COLLECTION_NAME=global_facts
ANONYMIZED_TELEMETRY=false

# === Neo4j Graph Database (Optional) ===
ENABLE_GRAPH_DATABASE=false

# =======================================================
# MEMORY & AI INTELLIGENCE CONFIGURATION
# =======================================================

# === Memory Optimization ===
ENABLE_MEMORY_OPTIMIZATION=auto

# =======================================================
# HIERARCHICAL MEMORY ARCHITECTURE CONFIGURATION
# üöÄ NEW: 4-Tier High-Performance Memory System
# =======================================================
# Enable the new hierarchical memory system for 50-200x performance improvement
# Provides intelligent, multi-tier memory storage and retrieval

# === Master Control ===
ENABLE_HIERARCHICAL_MEMORY=false           # Enable 4-tier hierarchical memory system

# === Tier 1: Redis Cache (< 1ms response time) ===
HIERARCHICAL_REDIS_ENABLED=true            # Enable Redis tier for recent conversations
HIERARCHICAL_REDIS_HOST=redis               # Redis hostname (use "redis" for Docker)
HIERARCHICAL_REDIS_PORT=6379                # Redis port
HIERARCHICAL_REDIS_TTL=1800                 # Time-to-live for cached data (30 minutes)

# === Tier 2: PostgreSQL Archive (< 50ms response time) ===
HIERARCHICAL_POSTGRESQL_ENABLED=true       # Enable PostgreSQL tier for structured storage
HIERARCHICAL_POSTGRESQL_HOST=postgres       # PostgreSQL hostname (use "postgres" for Docker)
HIERARCHICAL_POSTGRESQL_PORT=5432           # PostgreSQL port
HIERARCHICAL_POSTGRESQL_DATABASE=whisper_engine    # Database name
HIERARCHICAL_POSTGRESQL_USERNAME=bot_user   # Database username
HIERARCHICAL_POSTGRESQL_PASSWORD=securepassword123  # Database password

# === Tier 3: ChromaDB Semantic (< 30ms response time) ===
HIERARCHICAL_CHROMADB_ENABLED=true         # Enable ChromaDB tier for semantic similarity
HIERARCHICAL_CHROMADB_HOST=chromadb        # ChromaDB hostname (use "chromadb" for Docker)
HIERARCHICAL_CHROMADB_PORT=8000            # ChromaDB port

# === Tier 4: Neo4j Graph (< 20ms response time) ===
HIERARCHICAL_NEO4J_ENABLED=true            # Enable Neo4j tier for relationship mapping
HIERARCHICAL_NEO4J_HOST=neo4j               # Neo4j hostname (use "neo4j" for Docker)
HIERARCHICAL_NEO4J_PORT=7687                # Neo4j bolt port
HIERARCHICAL_NEO4J_USERNAME=neo4j           # Neo4j username
HIERARCHICAL_NEO4J_PASSWORD=neo4j_password_change_me  # Neo4j password
HIERARCHICAL_NEO4J_DATABASE=neo4j           # Neo4j database name

# === Performance Tuning ===
HIERARCHICAL_CONTEXT_ASSEMBLY_TIMEOUT=100  # Maximum time for context assembly (ms)
HIERARCHICAL_MIGRATION_BATCH_SIZE=50       # Batch size for data migration
HIERARCHICAL_MAX_CONCURRENT_BATCHES=3      # Maximum concurrent migration batches

# === Quick Setup Guide ===
# 1. Set ENABLE_HIERARCHICAL_MEMORY=true
# 2. Start infrastructure: ./bot.sh start infrastructure  
# 3. Initialize schemas: Automatic on first run
# 4. Start bot: ./bot.sh start dev
# 5. Enjoy 50-200x faster memory operations!
#
# üìä Performance Benchmarks:
#   ‚Ä¢ Context Assembly: < 100ms (vs 5000ms+ standard)
#   ‚Ä¢ Recent Messages: < 1ms from Redis cache
#   ‚Ä¢ Semantic Search: < 30ms from ChromaDB
#   ‚Ä¢ Relationship Queries: < 20ms from Neo4j
#   ‚Ä¢ Storage Operations: < 50ms across all tiers
MEMORY_OPTIMIZATION_LEVEL=auto
MEMORY_CACHE_SIZE=auto
ENABLE_MEMORY_MONITORING=auto

# === üöÄ PERFORMANCE OPTIMIZATION ===
# Advanced performance features for high-load production environments
# These settings can dramatically improve response times and throughput

# Production optimization system (combines all optimizations)
ENABLE_PRODUCTION_OPTIMIZATION=true            # Master switch for production optimizations

# Parallel AI processing (3-5x speed improvement)
ENABLE_PARALLEL_PROCESSING=true                # Process AI components concurrently
PARALLEL_PROCESSING_MAX_WORKERS=4              # Max concurrent AI processing tasks

# Memory performance optimizations
AI_MEMORY_OPTIMIZATION=true                    # Advanced memory retrieval optimizations
MEMORY_BATCH_SIZE=20                           # ChromaDB batch processing size
SEMANTIC_CLUSTERING_MAX_MEMORIES=20            # Max memories per clustering batch

# Cache optimization settings
CACHE_STRATEGY=hybrid                          # Cache strategy: lru/ttl/hybrid/adaptive
CACHE_MAX_SIZE=1000                           # Maximum number of cached items
CACHE_TTL_SECONDS=1200                        # Default cache time-to-live (20 minutes)

# Background processing optimizations
ENABLE_BACKGROUND_PROCESSING=true              # Async background AI processing
BACKGROUND_PROCESSING_QUEUE_SIZE=100           # Background task queue size

# Debug performance monitoring
ENABLE_PERFORMANCE_MONITORING=auto             # Track and log performance metrics
PERFORMANCE_LOG_LEVEL=info                     # Performance logging level (debug/info/warning)

# === AI Intelligence Phases ===
ENABLE_EMOTIONAL_INTELLIGENCE=true
ENABLE_EMOTIONAL_INTELLIGENCE_PERSISTENCE=true    # Store emotional assessments in database
ENABLE_PHASE3_MEMORY=true
AI_EMOTIONAL_RESONANCE=true
AI_ADAPTIVE_MODE=true
AI_PERSONALITY_ANALYSIS=true
ENABLE_DYNAMIC_PERSONALITY=true

# === Adaptive Prompt Engineering ===
ADAPTIVE_PROMPT_ENABLED=true                    # Enable/disable adaptive prompt system
ADAPTIVE_PROMPT_FORCE_SIZE=                     # Override auto-detection (small/medium/large)
ADAPTIVE_PROMPT_TOKEN_BUDGET=                   # Override default token budget
ADAPTIVE_PROMPT_COMPRESSION_LEVEL=balanced      # aggressive/balanced/minimal
ADAPTIVE_PROMPT_PERFORMANCE_MODE=quality        # fast/balanced/quality
ADAPTIVE_PROMPT_DEBUG=false                     # Enable detailed logging

# === Model-Specific Overrides ===
SMALL_MODEL_MAX_PROMPT=500                      # Max prompt size for small models (1B-3B params)
MEDIUM_MODEL_MAX_PROMPT=1000                    # Max prompt size for medium models (3B-8B params)
LARGE_MODEL_MAX_PROMPT=2000                     # Max prompt size for large models (8B+ params)

# === Template Selection ===
ADAPTIVE_TEMPLATE_PREFERENCE=optimized          # default/optimized/minimal
ADAPTIVE_CONTEXT_STRATEGY=smart_truncation      # truncate/compress/prioritize

# === Phase 4 Human-Like Intelligence ===
ENABLE_PHASE4_HUMAN_LIKE=true
PHASE4_PERSONALITY_TYPE=caring_friend
PHASE4_CONVERSATION_MODE=adaptive
PHASE4_EMOTIONAL_INTELLIGENCE_LEVEL=high
PHASE4_RELATIONSHIP_AWARENESS=true
PHASE4_CONVERSATION_FLOW_PRIORITY=true
PHASE4_EMPATHETIC_LANGUAGE=true
PHASE4_MEMORY_PERSONAL_DETAILS=true

# === Phase 4.2 Advanced Thread Management ===
ENABLE_PHASE4_THREAD_MANAGER=true            # Multi-thread conversation tracking
PHASE4_THREAD_MAX_ACTIVE=5                   # Maximum active conversation threads per user
PHASE4_THREAD_TIMEOUT_MINUTES=30             # Thread inactivity timeout

# === Phase 4.3 Proactive Engagement Engine ===
ENABLE_PHASE4_PROACTIVE_ENGAGEMENT=true      # Proactive conversation suggestions
PHASE4_ENGAGEMENT_MIN_SILENCE_MINUTES=10     # Minimum silence before proactive engagement
PHASE4_ENGAGEMENT_MAX_SUGGESTIONS_PER_DAY=3  # Limit proactive engagement frequency

# === Phase 3 Memory Integration ===
PHASE3_MAX_MEMORIES=50

# === Semantic Clustering ===
SEMANTIC_CLUSTERING_MAX_MEMORIES=20
SEMANTIC_CLUSTERING_TIMEOUT=30

# === Embeddings (Single Local Model) ===
# External embedding services removed ‚Äì using local MiniLM (384-dim)
LLM_LOCAL_EMBEDDING_MODEL=all-MiniLM-L6-v2

# =======================================================
# EMOTION AI CONFIGURATION
# =======================================================

# === Local Emotion Analysis ===
ENABLE_VADER_EMOTION=true
ENABLE_ROBERTA_EMOTION=true
ROBERTA_EMOTION_MODEL=cardiffnlp/twitter-roberta-base-emotion-multilingual-latest
EMOTION_CACHE_SIZE=500
EMOTION_BATCH_SIZE=8

# === GPU Acceleration ===
USE_GPU=false

# =======================================================
# VOICE CONFIGURATION
# =======================================================

# === Voice Features ===
VOICE_AUTO_JOIN=false
VOICE_RESPONSE_ENABLED=true
VOICE_LISTENING_ENABLED=true
VOICE_STREAMING_ENABLED=true
VOICE_JOIN_ANNOUNCEMENTS=true

# === Voice Limits & Timing ===
VOICE_MAX_AUDIO_LENGTH=30
VOICE_MAX_RESPONSE_LENGTH=300
VOICE_RESPONSE_DELAY=1.0

# === Voice Connection Management ===
VOICE_KEEPALIVE_INTERVAL=300
VOICE_HEARTBEAT_INTERVAL=30
VOICE_MAX_RECONNECT_ATTEMPTS=3

# =======================================================
# BACKUP & MAINTENANCE
# =======================================================

# === Automatic Backups ===
AUTO_BACKUP_ENABLED=true
AUTO_BACKUP_INTERVAL_HOURS=24
BACKUP_RETENTION_COUNT=5
BACKUP_PATH=./backups

# =======================================================
# DEVELOPMENT & MONITORING
# =======================================================

# === Debug & Logging ===
DEBUG_MODE=false                               # Enable debug mode for development
LOG_LEVEL=INFO                                 # File logging level (DEBUG/INFO/WARNING/ERROR/CRITICAL)
CONSOLE_LOG_LEVEL=INFO                         # Console logging level - can be different from file level

# Common logging patterns:
# Development with clean console:    LOG_LEVEL=DEBUG, CONSOLE_LOG_LEVEL=ERROR
# Production monitoring:             LOG_LEVEL=INFO, CONSOLE_LOG_LEVEL=WARNING  
# Full debugging:                    LOG_LEVEL=DEBUG, CONSOLE_LOG_LEVEL=DEBUG
# Minimal noise:                     LOG_LEVEL=WARNING, CONSOLE_LOG_LEVEL=ERROR

PYTHONUNBUFFERED=1                             # Ensure Python output isn't buffered

# === Health Checks ===
HEALTH_CHECK_PORT=9090
HEALTH_CHECK_HOST=0.0.0.0

# === Environment Detection (Auto-detected by system) ===
# Container Detection - automatically set by Docker/deployment
CONTAINER_MODE=false                        # true if running in container
DOCKER_ENV=false                           # true if Docker environment  
DEV_MODE=false                             # true to enable development features

# =======================================================
# VISUAL EMOTION ANALYSIS - SPRINT 6
# =======================================================

# === Visual Emotion Analysis ===
ENABLE_VISUAL_EMOTION_ANALYSIS=true         # Enable visual emotion features
VISUAL_EMOTION_PROCESSING_MODE=auto          # auto/local/cloud
VISUAL_EMOTION_CONFIDENCE_THRESHOLD=0.6      # Minimum confidence for emotion detection
VISUAL_EMOTION_MAX_IMAGE_SIZE=10             # MB limit for image processing

# === Vision Model Configuration ===
VISION_MODEL_PROVIDER=openai                 # openai/anthropic/local
VISION_MODEL_NAME=gpt-4-vision-preview       # Cloud: gpt-4v, claude-3-vision
LOCAL_VISION_MODEL=llava-1.5-7b              # Local: llava, blip2, clip

# === Privacy and Storage ===
VISUAL_EMOTION_RETENTION_DAYS=30             # How long to keep visual emotion memories
VISUAL_EMOTION_PRIVACY_MODE=enhanced         # basic/enhanced/strict
STORE_VISUAL_EMOTIONS_LOCALLY=true           # Desktop mode: store in local DB only

# === Discord Integration ===
DISCORD_VISUAL_EMOTION_ENABLED=true          # Enable Discord image analysis
DISCORD_VISUAL_RESPONSE_ENABLED=true         # Generate responses to images
DISCORD_VISUAL_REACTION_ENABLED=true         # Add emoji reactions based on emotions

# =======================================================
# CLOUD PROVIDER EXAMPLES (UNCOMMENT TO USE)
# =======================================================

# === OpenAI ===
# LLM_CHAT_API_URL=https://api.openai.com/v1
# LLM_CHAT_API_KEY=your_openai_api_key
# LLM_CHAT_MODEL=gpt-4

# === OpenRouter ===
# LLM_CHAT_API_URL=https://openrouter.ai/api/v1
# LLM_CHAT_API_KEY=your_openrouter_api_key
# LLM_CHAT_MODEL=openai/gpt-4o

# === Ollama (Local) ===
# LLM_CHAT_API_URL=http://localhost:11434/v1
# LLM_CHAT_MODEL=phi3:mini

# === HuggingFace ===
# HUGGINGFACE_API_KEY=your_huggingface_api_key

# =======================================================
# ADVANCED CONFIGURATION
# =======================================================

# === Core Environment & Mode Configuration ===
# Bot Architecture Mode:
# - "single_bot": Single Discord bot instance (recommended)
# - "multi_bot": Multi-bot scaling architecture for enterprise  
# - "desktop": Desktop application mode
WHISPERENGINE_MODE=single_bot

# Runtime Environment:
# - "development": Debug features, detailed logging, hot-reload
# - "production": Optimized performance, production error handling
# - "staging": Testing environment before production
# - "test": Testing environment for automated tests  
ENVIRONMENT=development

# Container Detection (auto-detected, usually don't change):
# - "true": Running in Docker container
# - "false": Running natively on host system
CONTAINER_MODE=false
DOCKER_ENV=false

# Development Features:
# - "true": Enable hot-reload, debug endpoints, extended logging
# - "false": Disable development features for performance
DEV_MODE=false

# Memory System Architecture:
# - "hierarchical": 4-tier memory (Redis‚ÜíPostgreSQL‚ÜíChromaDB‚ÜíNeo4j) [recommended]
# - "experimental_v2": Future experimental system (not implemented)
# - "test_mock": Mock memory for testing
MEMORY_SYSTEM_TYPE=hierarchical

# === Custom Paths ===
DOTENV_PATH=

# LLM Performance Settings
LLM_TEMPERATURE=0.7                         # Creativity 0.0-2.0
LLM_REQUEST_TIMEOUT=90                      # LM Studio can be slow
LLM_CONNECTION_TIMEOUT=10                   # Connection establishment timeout

# Token Limits - Optimized for large system prompts with emotional intelligence
LLM_MAX_TOKENS_CHAT=4096                   # Main chat response tokens
LLM_MAX_TOKENS_COMPLETION=1024              # Completion tokens
LLM_MAX_TOKENS_EMOTION=200                  # Emotion analysis tokens
LLM_MAX_TOKENS_FACT_EXTRACTION=500          # Fact extraction tokens
LLM_MAX_TOKENS_PERSONAL_INFO=400            # Personal info tokens
LLM_MAX_TOKENS_TRUST_DETECTION=300          # Trust detection tokens
LLM_MAX_TOKENS_USER_FACTS=400               # User facts tokens

# Vision Support (experimental)
LLM_SUPPORTS_VISION=false
LLM_VISION_MAX_IMAGES=5


# Message Security
MAX_SYSTEM_MESSAGE_LENGTH=                  # Auto-calculated
SECURITY_LOG_LEVEL=quiet

# =======================================================
# DATABASE CONFIGURATION
# =======================================================

# PostgreSQL Database Configuration (required)
# For Docker: use service name "postgres"
# For Native: use "localhost" 
POSTGRES_HOST=localhost                     # Use "postgres" for Docker mode
POSTGRES_PORT=5432
POSTGRES_DB=whisper_engine
POSTGRES_USER=bot_user
POSTGRES_PASSWORD=bot_password_change_me

# Connection pool settings
POSTGRES_MIN_CONNECTIONS=5
POSTGRES_MAX_CONNECTIONS=20
POSTGRES_PRIVACY_MIN_CONNECTIONS=3          # Privacy manager pool
POSTGRES_PRIVACY_MAX_CONNECTIONS=10         # Privacy manager pool

# Redis Configuration (required)
# For Docker: use service name "redis"
# For Native: use "localhost"
REDIS_HOST=localhost                        # Use "redis" for Docker mode
REDIS_PORT=6379
REDIS_DB=0
USE_REDIS_CACHE=true

# Redis cache settings
CONVERSATION_CACHE_TIMEOUT_MINUTES=15
CONVERSATION_CACHE_BOOTSTRAP_LIMIT=20
CONVERSATION_CACHE_MAX_LOCAL=50

# ChromaDB Configuration (required)
# ChromaDB always runs in HTTP client mode for containerized deployments
# For Docker: use service name "chromadb"
# For Native: use "localhost"
CHROMADB_HOST=localhost                     # Use "chromadb" for Docker mode
CHROMADB_PORT=8000
CHROMADB_COLLECTION_NAME=user_memories
CHROMADB_GLOBAL_COLLECTION_NAME=global_facts
ANONYMIZED_TELEMETRY=false

# Neo4j Graph Database (optional)
ENABLE_GRAPH_DATABASE=false
# For Docker: use service name "neo4j"
# For Native: use "localhost"
NEO4J_HOST=localhost                        # Use "neo4j" for Docker mode
NEO4J_PORT=7687
NEO4J_USERNAME=neo4j
NEO4J_PASSWORD=neo4j_password_change_me
NEO4J_DATABASE=neo4j

# Graph features
GRAPH_SYNC_MODE=async                       # Options: async, sync, disabled
FALLBACK_TO_EXISTING=true
EMOTION_GRAPH_SYNC_INTERVAL=10

# =======================================================
# MEMORY SYSTEM CONFIGURATION
# =======================================================

# Memory Features
ENABLE_AUTO_FACTS=true
ENABLE_GLOBAL_FACTS=true
ENABLE_EMOTIONS=true

# Memory Optimization Features (NEW)
ENABLE_MEMORY_SUMMARIZATION=true            # Intelligent conversation summarization
ENABLE_MEMORY_DEDUPLICATION=true            # Remove redundant memories
ENABLE_MEMORY_CLUSTERING=true               # Organize memories by topics
ENABLE_MEMORY_PRIORITIZATION=true           # Smart context ranking
MEMORY_OPTIMIZATION_INTERVAL=24             # Hours between optimization cycles

ENABLE_PRODUCTION_OPTIMIZATION=true         # Enable production optimizations

# Single embedding model (no fallback needed for local models)
LLM_LOCAL_EMBEDDING_MODEL=all-MiniLM-L6-v2

# === REDUNDANCY ELIMINATION ===
# Use local systems instead of redundant LLM API calls
USE_LOCAL_EMOTION_ANALYSIS=true              # Use Phase 2 + local emotion engine
USE_LOCAL_FACT_EXTRACTION=true               # Use spaCy + patterns
DISABLE_EXTERNAL_EMOTION_API=true            # Skip external emotion API calls
DISABLE_REDUNDANT_FACT_EXTRACTION=true       # Skip LLM fact extraction

# Local Emotion Engine Configuration
ENABLE_VADER_EMOTION=true                    # Ultra-fast VADER sentiment analysis
ENABLE_ROBERTA_EMOTION=true                  # High-quality RoBERTa emotion classification
ROBERTA_EMOTION_MODEL=cardiffnlp/twitter-roberta-base-sentiment-latest

# Embedding Performance
EMBEDDING_BATCH_SIZE=100
EMBEDDING_MAX_RETRIES=3
EMBEDDING_RETRY_DELAY=1.0
EMBEDDING_MAX_CONCURRENT=5

# Backup System
AUTO_BACKUP_ENABLED=true
AUTO_BACKUP_INTERVAL_HOURS=24
BACKUP_RETENTION_COUNT=5
BACKUP_PATH=./backups

# =======================================================
# VOICE FEATURES
# =======================================================

# Voice System
VOICE_SUPPORT_ENABLED=true

# ElevenLabs API
ELEVENLABS_API_KEY=                         # Your ElevenLabs API key
ELEVENLABS_DEFAULT_VOICE_ID=ked1vRAQW5Sk9vhZC3vI # Updated default voice
ELEVENLABS_VOICE_STABILITY=0.5              # 0.0-1.0
ELEVENLABS_VOICE_SIMILARITY_BOOST=0.8       # 0.0-1.0
ELEVENLABS_VOICE_STYLE=0.0                  # 0.0-1.0, most natural
ELEVENLABS_USE_SPEAKER_BOOST=true

# Voice Behavior
VOICE_AUTO_JOIN=false
VOICE_RESPONSE_ENABLED=true
VOICE_LISTENING_ENABLED=true
VOICE_STREAMING_ENABLED=true
VOICE_MAX_RESPONSE_LENGTH=300               # characters
VOICE_MAX_AUDIO_LENGTH=30                   # seconds
VOICE_RESPONSE_DELAY=1.0                    # seconds

# Voice Connection Management
VOICE_JOIN_ANNOUNCEMENTS=true
VOICE_KEEPALIVE_INTERVAL=300                # 5 minutes
VOICE_HEARTBEAT_INTERVAL=30                 # seconds
VOICE_MAX_RECONNECT_ATTEMPTS=3
VOICE_RECONNECT_DELAY=5.0                   # seconds

# =======================================================
# JOB SCHEDULER & AUTOMATION
# =======================================================

# Job Scheduler Configuration
JOB_SCHEDULER_ENABLED=true
JOB_SCHEDULER_CHECK_INTERVAL_SECONDS=30

# Follow-up Message Configuration  
FOLLOW_UP_ENABLED=true
FOLLOW_UP_DEFAULT_DELAY_HOURS=48
FOLLOW_UP_MAX_PER_USER_PER_WEEK=2
FOLLOW_UP_MIN_HOURS_BETWEEN=24

# Data Cleanup Configuration
CLEANUP_ENABLED=true
CLEANUP_OLD_CONVERSATIONS_DAYS=30
CLEANUP_TEMP_FILES_HOURS=24
CLEANUP_FAILED_JOBS_DAYS=7

# =======================================================
# SYSTEM CONFIGURATION
# =======================================================

# Environment and Debugging
ENVIRONMENT=development                        # Environment mode (development/production)
DEBUG_MODE=false                               # Enable debug mode features
LOG_LEVEL=INFO                                 # File logging level (DEBUG/INFO/WARNING/ERROR/CRITICAL)
CONSOLE_LOG_LEVEL=INFO                         # Console logging level - can be different from file level
LOG_DIR=logs                                   # Directory for log files
LOG_APP_NAME=discord_bot                       # Application name for log files

# Performance Settings
MAX_PROCESSING_TIME=60.0                    # seconds

# CDL Character Configuration - Customize your AI's personality using Character Definition Language
# Default: WhisperEngine Default Assistant
CDL_DEFAULT_CHARACTER=characters/default_assistant.json

# Choose from pre-built character templates:
# CDL_DEFAULT_CHARACTER=characters/examples/dream_of_the_endless.json           # ‚ú® Dream from The Sandman
# CDL_DEFAULT_CHARACTER=characters/examples/elena-rodriguez.json       # ÔøΩ Supportive companion
# CDL_DEFAULT_CHARACTER=characters/examples/marcus-chen.json           # ÔøΩ Professional assistant

# Or create your own custom character file in the characters directory:
# CDL_DEFAULT_CHARACTER=characters/my_custom_character.json

# Legacy prompt system (DEPRECATED - use CDL_DEFAULT_CHARACTER instead)
# BOT_SYSTEM_PROMPT_FILE=./prompts/default.md

# File and Image Processing
TEMP_IMAGES_DIR=temp_images

# NLP Configuration (advanced)
NLP_DEPLOYMENT_MODE=native_integrated
NLP_SERVICE_HOST=localhost
NLP_SERVICE_PORT=8080
NLP_TIMEOUT_SECONDS=30
NLP_SPACY_MODEL=en_core_web_lg

# Container Detection (auto-detected)
DOCKER_CONTAINER=                           # Auto-detected by environment
ENV_MODE=                                   # Explicit environment mode override
CONTAINER_MODE=                             # Container mode indicator
DEV_MODE=                                   # Development mode indicator

# External API Keys (for external services)
OPENAI_API_KEY=                             # OpenAI API key
OPENROUTER_API_KEY=                         # OpenRouter API key  
HUGGINGFACE_API_KEY=                        # HuggingFace API key

# ====================================
# üß† AI Features Configuration
# ====================================
# All AI features are ENABLED BY DEFAULT for full capabilities
# Use performance tuning parameters to optimize rather than disable features

# Phase 1: Basic LLM Responses + Context Awareness
ENABLE_BASIC_AI=true                        # Core AI functionality (always enabled)

# Phase 2: Emotional Intelligence System 
ENABLE_EMOTIONAL_INTELLIGENCE=true         # Multi-source emotion analysis (96-98% accuracy)
EMOTION_API_TIMEOUT=5                       # External emotion API timeout (seconds)
EMOTION_CACHE_SIZE=1000                     # Emotional state cache entries
EMOTION_CONFIDENCE_THRESHOLD=0.7            # Minimum confidence for emotion detection

# Phase 3: Multi-Dimensional Memory Networks
ENABLE_PHASE3_MEMORY=true                   # Advanced semantic memory (ChromaDB + Redis)
MEMORY_RETENTION_DAYS=30                    # Memory retention period
MEMORY_MAX_ENTRIES=10000                    # Maximum memory entries per user
ENABLE_MEMORY_COMPRESSION=true              # Compress old memories for efficiency

# Phase 4: Human-Like Conversation Adaptation
ENABLE_PHASE4_INTELLIGENCE=true             # Advanced conversation adaptation
PERSONALITY_ADAPTATION_ENABLED=true         # Dynamic personality adjustment
CONVERSATION_CONTEXT_DEPTH=10               # Conversation history depth for adaptation

# Performance Optimization (tune for your system)
ENABLE_PROMPT_OPTIMIZATION=true             # Use optimized prompts for bundled models
OPTIMIZED_PROMPT_MODE=auto                  # auto|always|never - automatic selection
MAX_CONCURRENT_AI_OPERATIONS=3              # Limit concurrent AI operations
AI_RESPONSE_TIMEOUT=30                      # AI response timeout (seconds)
ENABLE_AI_CACHING=true                      # Cache AI responses for performance

# Memory System Configuration
USE_REDIS_CACHE=true                        # Redis for conversation caching
ENABLE_GRAPH_DATABASE=false                 # Neo4j for relationship mapping (optional)
CHROMA_BATCH_SIZE=100                       # ChromaDB batch processing size
MEMORY_SEARCH_LIMIT=20                      # Maximum memory search results

# =======================================================
# PHANTOM FEATURES CONFIGURATION
# =======================================================
# Advanced AI capabilities that can be enabled/disabled
# For detailed documentation, see: docs/PHANTOM_FEATURES_ENVIRONMENT_GUIDE.md

# üß† Advanced Emotion Processing
ENABLE_LOCAL_EMOTION_ENGINE=false           # High-performance VADER + RoBERTa emotion analysis
ENABLE_VECTORIZED_EMOTION_PROCESSOR=false   # Batch emotion processing (resource intensive)
ENABLE_ADVANCED_EMOTION_DETECTOR=false      # Multi-modal emotion detection

# Emotion Processing Performance Settings
VECTORIZED_EMOTION_MAX_WORKERS=4            # Worker threads for batch processing
EMOTION_ANALYSIS_TIMEOUT=10                 # Timeout for emotion analysis (seconds)
EMOTION_BATCH_PROCESSING_SIZE=16            # Batch size for vectorized processing

# üí¨ Advanced Conversation Management
ENABLE_PROACTIVE_ENGAGEMENT_ENGINE=false    # AI-driven conversation initiation
ENABLE_ADVANCED_THREAD_MANAGER=false        # Multi-thread conversation tracking
ENABLE_CONCURRENT_CONVERSATION_MANAGER=false # Parallel conversation handling

# Conversation Performance Settings
PROACTIVE_ENGAGEMENT_CHECK_INTERVAL=300     # Check interval for engagement opportunities (seconds)
THREAD_MANAGER_MAX_ACTIVE_THREADS=10        # Maximum concurrent conversation threads
CONCURRENT_CONVERSATIONS_LIMIT=50           # Maximum parallel conversations
CONVERSATION_CONTEXT_RETENTION_HOURS=72     # Context retention time

# üîç Advanced Topic Analysis
ENABLE_ADVANCED_TOPIC_EXTRACTOR=false       # Sophisticated topic modeling

# Topic Analysis Settings
TOPIC_ANALYSIS_MIN_CONFIDENCE=0.7           # Minimum confidence for topic detection
TOPIC_CLUSTERING_MAX_TOPICS=10              # Maximum topics to extract
TOPIC_SIMILARITY_THRESHOLD=0.75             # Threshold for grouping similar topics

# üìä Phantom Feature Monitoring
ENABLE_PHANTOM_FEATURE_MONITORING=true      # Enable monitoring for phantom features
PHANTOM_FEATURES_PERFORMANCE_LOGGING=true   # Enable performance logging
PHANTOM_FEATURES_DETAILED_ANALYTICS=false   # Enable comprehensive analytics

# üîß Phantom Feature Resource Control
MAX_PHANTOM_FEATURE_MEMORY_MB=2048          # Maximum memory allocation (MB)
MAX_PHANTOM_FEATURE_CPU_PERCENT=60          # Maximum CPU usage percentage

# Performance Optimization
PHANTOM_FEATURES_ASYNC_PROCESSING=true      # Enable asynchronous processing
PHANTOM_FEATURES_CONNECTION_POOLING=true    # Enable database connection pooling
PHANTOM_FEATURES_RESULT_CACHING=true        # Enable result caching

# üõ°Ô∏è Security & Privacy
PHANTOM_FEATURES_ENCRYPTION_ENABLED=true    # Enable data encryption
PHANTOM_FEATURES_DATA_ANONYMIZATION=true    # Enable data anonymization
PHANTOM_FEATURES_AUDIT_LOGGING=false        # Enable audit logging (compliance)
PHANTOM_FEATURES_DATA_RETENTION_DAYS=90     # Data retention period
PHANTOM_FEATURES_CONSENT_REQUIRED=true      # Require user consent
