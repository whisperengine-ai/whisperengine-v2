# Multi-Bot Docker Compose Template
# This file is a template that gets populated by scripts/generate_multi_bot_config.py
# DO NOT EDIT: Generated sections will be overwritten
#
# Port Strategy:
# - Development Environment (multi-bot.sh): 5000-5999 + 9000-9999 range
# - End-User Environment (quickstart): 8000-8999 range
# - Docker Networks: whisperengine-multi_bot_network (dev) vs whisperengine-quickstart-network (end-user)

services:
  # ===== DATABASE MIGRATION INIT CONTAINER =====
  # Runs once before bot services start to ensure schema is up-to-date
  db-migrate:
    image: whisperengine-bot:${VERSION:-latest}
    container_name: whisperengine-multi-db-migrate
    entrypoint: []  # Override entrypoint to prevent main app startup
    command: ["python", "/app/scripts/run_migrations.py"]
    environment:
      - POSTGRES_HOST=postgres
      - POSTGRES_PORT=5432
      - POSTGRES_USER=whisperengine
      - POSTGRES_PASSWORD=whisperengine_password
      - POSTGRES_DB=whisperengine
    volumes:
      - ./scripts:/app/scripts  # Live mount for migration scripts
      - ./sql:/app/sql:ro       # Live mount for SQL files
    networks:
      - bot_network
    depends_on:
      postgres:
        condition: service_healthy
    restart: "no"  # Only run once per docker-compose up
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "2"
        tag: "db-migrate-{.ImageName}-{.Name}-{.ID}"

  # ===== INFRASTRUCTURE SERVICES =====
  postgres:
    image: postgres:16.4-alpine
    container_name: whisperengine-multi-postgres
    restart: unless-stopped
    environment:
      POSTGRES_DB: whisperengine
      POSTGRES_USER: whisperengine
      POSTGRES_PASSWORD: whisperengine_password
      POSTGRES_HOST_AUTH_METHOD: trust
    ports:
      - "5433:5432"  # PostgreSQL (Dev: 5433, Quickstart: 8432)
    volumes:
      - postgres_data:/var/lib/postgresql/data
    networks:
      - bot_network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U whisperengine -d whisperengine"]
      interval: 10s
      timeout: 10s
      retries: 5
      start_period: 40s
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "5"
        tag: "postgres-{.ImageName}-{.Name}-{.ID}"

  # Redis - Commented out since we're using vector-native memory with Qdrant only
  # redis:
  #   image: redis:7.4-alpine
  #   container_name: whisperengine-multi-redis
  #   restart: unless-stopped
  #   ports:
  #     - "6380:6379"
  #   volumes:
  #     - redis_data:/data
  #   networks:
  #     - bot_network
  #   healthcheck:
  #     test: ["CMD", "redis-cli", "ping"]
  #     interval: 30s
  #     timeout: 10s
  #     retries: 3

  qdrant:
    image: qdrant/qdrant:v1.15.4
    container_name: whisperengine-multi-qdrant
    restart: unless-stopped
    ports:
      - "6334:6333"  # Qdrant (Dev: 6334, Quickstart: 8333)
    volumes:
      - qdrant_data:/qdrant/storage
    networks:
      - bot_network
    # Health check disabled - Qdrant containers have minimal tooling
    # Bots will retry connections if Qdrant isn't ready yet
    logging:
      driver: "json-file"
      options:
        max-size: "25m"
        max-file: "3"
        tag: "qdrant-{.ImageName}-{.Name}-{.ID}"

  influxdb:
    image: influxdb:2.7-alpine
    container_name: whisperengine-multi-influxdb
    restart: unless-stopped
    environment:
      DOCKER_INFLUXDB_INIT_MODE: setup
      DOCKER_INFLUXDB_INIT_USERNAME: whisperengine
      DOCKER_INFLUXDB_INIT_PASSWORD: whisperengine_metrics
      DOCKER_INFLUXDB_INIT_ORG: whisperengine
      DOCKER_INFLUXDB_INIT_BUCKET: performance_metrics
      DOCKER_INFLUXDB_INIT_ADMIN_TOKEN: whisperengine-fidelity-first-metrics-token
    ports:
      - "8087:8086"  # InfluxDB (Dev: 8087, Quickstart: 8088)
    volumes:
      - influxdb_data:/var/lib/influxdb2
      - influxdb_config:/etc/influxdb2
    networks:
      - bot_network
    healthcheck:
      test: ["CMD", "influx", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    logging:
      driver: "json-file"
      options:
        max-size: "25m"
        max-file: "3"
        tag: "influxdb-{.ImageName}-{.Name}-{.ID}"

  # Grafana monitoring dashboard for development environment
  grafana:
    image: grafana/grafana:11.3.0
    container_name: whisperengine-multi-grafana
    restart: unless-stopped
    environment:
      GF_SECURITY_ADMIN_USER: ${GRAFANA_USER:-admin}
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_PASSWORD:-whisperengine_grafana}
      GF_FEATURE_TOGGLES_ENABLE: publicDashboards
      INFLUXDB_USER: whisperengine
      INFLUXDB_PASSWORD: whisperengine_metrics
    ports:
      - "3002:3000"  # Grafana (Dev: 3002, Quickstart: 8001)
    volumes:
      - grafana_data:/var/lib/grafana
      - ./grafana-config/datasource.yml:/etc/grafana/provisioning/datasources/datasource.yml:ro
      - ./grafana-config/dashboard.yml:/etc/grafana/provisioning/dashboards/dashboard.yml:ro
      - ./dashboards:/var/lib/grafana/dashboards:ro
    networks:
      - bot_network
    depends_on:
      influxdb:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    logging:
      driver: "json-file"
      options:
        max-size: "25m"
        max-file: "3"
        tag: "grafana-{.ImageName}-{.Name}-{.ID}"

  # CDL Web UI for character authoring and management
  cdl-web-ui:
    build:
      context: ./cdl-web-ui
      dockerfile: Dockerfile
    container_name: whisperengine-multi-cdl-web-ui
    restart: unless-stopped
    environment:
      # PostgreSQL connection (same as WhisperEngine bots)
      POSTGRES_HOST: postgres
      POSTGRES_PORT: 5432
      POSTGRES_DB: whisperengine
      POSTGRES_USER: whisperengine
      POSTGRES_PASSWORD: whisperengine_password
      # Legacy PG* variables for compatibility
      PGHOST: postgres
      PGPORT: 5432
      PGDATABASE: whisperengine
      PGUSER: whisperengine
      PGPASSWORD: whisperengine_password
      # Next.js configuration
      NODE_ENV: production
      NEXT_TELEMETRY_DISABLED: 1
    ports:
      - "3001:3000"  # CDL Web UI (Dev: 3001, Quickstart: 8080)
    depends_on:
      postgres:
        condition: service_healthy
    networks:
      - bot_network
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:3000/api/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
    logging:
      driver: "json-file"
      options:
        max-size: "25m"
        max-file: "3"
        tag: "cdl-web-ui-{.ImageName}-{.Name}-{.ID}"

  # ===== SHARED BOT IMAGE BUILD =====
  whisperengine-bot-builder:
    build:
      context: .
      dockerfile: Dockerfile.bundled-models
      target: production
    image: whisperengine-bot:${VERSION:-latest}
    profiles:
      - build-only
    # This service is only used for building the shared image
    # It doesn't run as a container - just builds the image

  # ===== BOT SERVICES (populated dynamically) =====
  jake-bot:
    image: whisperengine-bot:${VERSION:-latest}
    container_name: whisperengine-multi-jake-bot
    restart: unless-stopped
    entrypoint: []  # Override Docker entrypoint for direct Python execution
    command: ["python", "run.py"]
    env_file:
      - .env.jake
    environment:
      - DISCORD_BOT_NAME=jake
      - POSTGRES_HOST=postgres
      - QDRANT_HOST=qdrant
      - MODEL_CACHE_DIR=/app/models
      - DISABLE_MODEL_DOWNLOAD=true
      - HF_HUB_OFFLINE=false
      - TRANSFORMERS_OFFLINE=0
      - HF_HOME=/app/cache/huggingface
      - TRANSFORMERS_CACHE=/app/cache/huggingface
      - HF_DATASETS_CACHE=/app/cache/huggingface
      - HUGGINGFACE_HUB_CACHE=/app/cache/huggingface
      - FASTEMBED_CACHE_PATH=/app/cache/fastembed
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - DEBUG_MODE=false
      - PYTHONUNBUFFERED=1
      - HEALTH_CHECK_PORT=9097
      - HEALTH_CHECK_HOST=0.0.0.0
    ports:
      - "9097:9097"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        tag: "jake-{.ImageName}-{.Name}-{.ID}"
    deploy:
      resources:
        limits:
          memory: 4G
          cpus: "4.0"
        reservations:
          memory: 2G
          cpus: "2.0"
    healthcheck:
      test:
        - CMD-SHELL
        - curl -f http://localhost:9097/health || exit 1
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    volumes:
      - jake_backups:/app/backups
      - jake_privacy:/app/privacy
      - jake_temp:/app/temp
      - ./logs:/app/logs  # External mount for prompt/response logs
      - ./sql:/app/sql:ro
      # Shared Hugging Face cache (all bots share models)
      - huggingface_cache:/app/cache/huggingface
      # Live code mounting for development (no rebuild needed)
      - ./src:/app/src
      - ./scripts:/app/scripts
      - ./characters:/app/characters
      - ./config:/app/config
      - ./validate_config.py:/app/validate_config.py
      - ./run.py:/app/run.py
      - ./env_manager.py:/app/env_manager.py
      # Note: Using external host mount for logs instead of Docker volumes
    networks:
      - bot_network
    depends_on:
      db-migrate:
        condition: service_completed_successfully
      postgres:
        condition: service_healthy
      qdrant:
        condition: service_started
      influxdb:
        condition: service_healthy
      # - redis  # Commented out - using vector-native memory only

  marcus-bot:
    image: whisperengine-bot:${VERSION:-latest}
    container_name: whisperengine-multi-marcus-bot
    restart: unless-stopped
    entrypoint: []  # Override Docker entrypoint for direct Python execution
    command: ["python", "run.py"]
    env_file:
      - .env.marcus
    environment:
      - DISCORD_BOT_NAME=marcus
      - POSTGRES_HOST=postgres
      - QDRANT_HOST=qdrant
      - MODEL_CACHE_DIR=/app/models
      - DISABLE_MODEL_DOWNLOAD=true
      - HF_HUB_OFFLINE=false
      - TRANSFORMERS_OFFLINE=0
      - HF_HOME=/app/cache/huggingface
      - TRANSFORMERS_CACHE=/app/cache/huggingface
      - HF_DATASETS_CACHE=/app/cache/huggingface
      - HUGGINGFACE_HUB_CACHE=/app/cache/huggingface
      - FASTEMBED_CACHE_PATH=/app/cache/fastembed
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - DEBUG_MODE=false
      - PYTHONUNBUFFERED=1
      - HEALTH_CHECK_PORT=9092
      - HEALTH_CHECK_HOST=0.0.0.0
    ports:
      - "9092:9092"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        tag: "marcus-{.ImageName}-{.Name}-{.ID}"
    deploy:
      resources:
        limits:
          memory: 4G
          cpus: "4.0"
        reservations:
          memory: 2G
          cpus: "2.0"
    healthcheck:
      test:
        - CMD-SHELL
        - curl -f http://localhost:9092/health || exit 1
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    volumes:
      - marcus_backups:/app/backups
      - marcus_privacy:/app/privacy
      - marcus_temp:/app/temp
      - ./logs:/app/logs  # External mount for prompt/response logs
      - ./sql:/app/sql:ro
      # Shared Hugging Face cache (all bots share models)
      - huggingface_cache:/app/cache/huggingface
      # Live code mounting for development (no rebuild needed)
      - ./src:/app/src
      - ./scripts:/app/scripts
      - ./characters:/app/characters
      - ./config:/app/config
      - ./validate_config.py:/app/validate_config.py
      - ./run.py:/app/run.py
      - ./env_manager.py:/app/env_manager.py
      # Note: Using external host mount for logs instead of Docker volumes
    networks:
      - bot_network
    depends_on:
      db-migrate:
        condition: service_completed_successfully
      postgres:
        condition: service_healthy
      qdrant:
        condition: service_started
      influxdb:
        condition: service_healthy
      # - redis  # Commented out - using vector-native memory only

  dotty-bot:
    image: whisperengine-bot:${VERSION:-latest}
    container_name: whisperengine-multi-dotty-bot
    restart: unless-stopped
    entrypoint: []  # Override Docker entrypoint for direct Python execution
    command: ["python", "run.py"]
    env_file:
      - .env.dotty
    environment:
      - DISCORD_BOT_NAME=dotty
      - POSTGRES_HOST=postgres
      - QDRANT_HOST=qdrant
      - MODEL_CACHE_DIR=/app/models
      - DISABLE_MODEL_DOWNLOAD=true
      - HF_HUB_OFFLINE=false
      - TRANSFORMERS_OFFLINE=0
      - HF_HOME=/app/cache/huggingface
      - TRANSFORMERS_CACHE=/app/cache/huggingface
      - HF_DATASETS_CACHE=/app/cache/huggingface
      - HUGGINGFACE_HUB_CACHE=/app/cache/huggingface
      - FASTEMBED_CACHE_PATH=/app/cache/fastembed
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - DEBUG_MODE=false
      - PYTHONUNBUFFERED=1
      - HEALTH_CHECK_PORT=9098
      - HEALTH_CHECK_HOST=0.0.0.0
    ports:
      - "9098:9098"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        tag: "dotty-{.ImageName}-{.Name}-{.ID}"
    deploy:
      resources:
        limits:
          memory: 4G
          cpus: "4.0"
        reservations:
          memory: 2G
          cpus: "2.0"
    healthcheck:
      test:
        - CMD-SHELL
        - curl -f http://localhost:9098/health || exit 1
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    volumes:
      - dotty_backups:/app/backups
      - dotty_privacy:/app/privacy
      - dotty_temp:/app/temp
      - ./logs:/app/logs  # External mount for prompt/response logs
      - ./sql:/app/sql:ro
      # Shared Hugging Face cache (all bots share models)
      - huggingface_cache:/app/cache/huggingface
      # Live code mounting for development (no rebuild needed)
      - ./src:/app/src
      - ./scripts:/app/scripts
      - ./characters:/app/characters
      - ./config:/app/config
      - ./validate_config.py:/app/validate_config.py
      - ./run.py:/app/run.py
      - ./env_manager.py:/app/env_manager.py
      # Note: Using external host mount for logs instead of Docker volumes
    networks:
      - bot_network
    depends_on:
      db-migrate:
        condition: service_completed_successfully
      postgres:
        condition: service_healthy
      qdrant:
        condition: service_started
      influxdb:
        condition: service_healthy
      # - redis  # Commented out - using vector-native memory only

  dream-bot:
    image: whisperengine-bot:${VERSION:-latest}
    container_name: whisperengine-multi-dream-bot
    restart: unless-stopped
    entrypoint: []  # Override Docker entrypoint for direct Python execution
    command: ["python", "run.py"]
    env_file:
      - .env.dream
    environment:
      - DISCORD_BOT_NAME=dream
      - POSTGRES_HOST=postgres
      - QDRANT_HOST=qdrant
      - MODEL_CACHE_DIR=/app/models
      - DISABLE_MODEL_DOWNLOAD=true
      - HF_HUB_OFFLINE=false
      - TRANSFORMERS_OFFLINE=0
      - HF_HOME=/app/cache/huggingface
      - TRANSFORMERS_CACHE=/app/cache/huggingface
      - HF_DATASETS_CACHE=/app/cache/huggingface
      - HUGGINGFACE_HUB_CACHE=/app/cache/huggingface
      - FASTEMBED_CACHE_PATH=/app/cache/fastembed
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - DEBUG_MODE=false
      - PYTHONUNBUFFERED=1
      - HEALTH_CHECK_PORT=9094
      - HEALTH_CHECK_HOST=0.0.0.0
    ports:
      - "9094:9094"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        tag: "dream-{.ImageName}-{.Name}-{.ID}"
    deploy:
      resources:
        limits:
          memory: 4G
          cpus: "4.0"
        reservations:
          memory: 2G
          cpus: "2.0"
    healthcheck:
      test:
        - CMD-SHELL
        - curl -f http://localhost:9094/health || exit 1
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    volumes:
      - dream_backups:/app/backups
      - dream_privacy:/app/privacy
      - dream_temp:/app/temp
      - ./logs:/app/logs  # External mount for prompt/response logs
      - ./sql:/app/sql:ro
      # Shared Hugging Face cache (all bots share models)
      - huggingface_cache:/app/cache/huggingface
      # Live code mounting for development (no rebuild needed)
      - ./src:/app/src
      - ./scripts:/app/scripts
      - ./characters:/app/characters
      - ./config:/app/config
      - ./validate_config.py:/app/validate_config.py
      - ./run.py:/app/run.py
      - ./env_manager.py:/app/env_manager.py
      # Note: Using external host mount for logs instead of Docker volumes
    networks:
      - bot_network
    depends_on:
      db-migrate:
        condition: service_completed_successfully
      postgres:
        condition: service_healthy
      qdrant:
        condition: service_started
      influxdb:
        condition: service_healthy
      # - redis  # Commented out - using vector-native memory only

  aetheris-bot:
    image: whisperengine-bot:${VERSION:-latest}
    container_name: whisperengine-multi-aetheris-bot
    restart: unless-stopped
    entrypoint: []  # Override Docker entrypoint for direct Python execution
    command: ["python", "run.py"]
    env_file:
      - .env.aetheris
    environment:
      - DISCORD_BOT_NAME=aetheris
      - POSTGRES_HOST=postgres
      - QDRANT_HOST=qdrant
      - MODEL_CACHE_DIR=/app/models
      - DISABLE_MODEL_DOWNLOAD=true
      - HF_HUB_OFFLINE=false
      - TRANSFORMERS_OFFLINE=0
      - HF_HOME=/app/cache/huggingface
      - TRANSFORMERS_CACHE=/app/cache/huggingface
      - HF_DATASETS_CACHE=/app/cache/huggingface
      - HUGGINGFACE_HUB_CACHE=/app/cache/huggingface
      - FASTEMBED_CACHE_PATH=/app/cache/fastembed
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - DEBUG_MODE=false
      - PYTHONUNBUFFERED=1
      - HEALTH_CHECK_PORT=9099
      - HEALTH_CHECK_HOST=0.0.0.0
    ports:
      - "9099:9099"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        tag: "aetheris-{.ImageName}-{.Name}-{.ID}"
    deploy:
      resources:
        limits:
          memory: 4G
          cpus: "4.0"
        reservations:
          memory: 2G
          cpus: "2.0"
    healthcheck:
      test:
        - CMD-SHELL
        - curl -f http://localhost:9099/health || exit 1
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    volumes:
      - aetheris_backups:/app/backups
      - aetheris_privacy:/app/privacy
      - aetheris_temp:/app/temp
      - ./logs:/app/logs  # External mount for prompt/response logs
      - ./sql:/app/sql:ro
      # Shared Hugging Face cache (all bots share models)
      - huggingface_cache:/app/cache/huggingface
      # Live code mounting for development (no rebuild needed)
      - ./src:/app/src
      - ./scripts:/app/scripts
      - ./characters:/app/characters
      - ./config:/app/config
      - ./validate_config.py:/app/validate_config.py
      - ./run.py:/app/run.py
      - ./env_manager.py:/app/env_manager.py
      # Note: Using external host mount for logs instead of Docker volumes
    networks:
      - bot_network
    depends_on:
      db-migrate:
        condition: service_completed_successfully
      postgres:
        condition: service_healthy
      qdrant:
        condition: service_started
      influxdb:
        condition: service_healthy
      # - redis  # Commented out - using vector-native memory only

  elena-bot:
    image: whisperengine-bot:${VERSION:-latest}
    container_name: whisperengine-multi-elena-bot
    restart: unless-stopped
    entrypoint: []  # Override Docker entrypoint for direct Python execution
    command: ["python", "run.py"]
    env_file:
      - .env.elena
    environment:
      - DISCORD_BOT_NAME=elena
      - POSTGRES_HOST=postgres
      - QDRANT_HOST=qdrant
      - MODEL_CACHE_DIR=/app/models
      - DISABLE_MODEL_DOWNLOAD=true
      - HF_HUB_OFFLINE=false
      - TRANSFORMERS_OFFLINE=0
      - HF_HOME=/app/cache/huggingface
      - TRANSFORMERS_CACHE=/app/cache/huggingface
      - HF_DATASETS_CACHE=/app/cache/huggingface
      - HUGGINGFACE_HUB_CACHE=/app/cache/huggingface
      - FASTEMBED_CACHE_PATH=/app/cache/fastembed
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - DEBUG_MODE=false
      - PYTHONUNBUFFERED=1
      - HEALTH_CHECK_PORT=9091
      - HEALTH_CHECK_HOST=0.0.0.0
    ports:
      - "9091:9091"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        tag: "elena-{.ImageName}-{.Name}-{.ID}"
    deploy:
      resources:
        limits:
          memory: 4G
          cpus: "4.0"
        reservations:
          memory: 2G
          cpus: "2.0"
    healthcheck:
      test:
        - CMD-SHELL
        - curl -f http://localhost:9091/health || exit 1
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    volumes:
      - elena_backups:/app/backups
      - elena_privacy:/app/privacy
      - elena_temp:/app/temp
      - ./logs:/app/logs  # External mount for prompt/response logs
      - ./sql:/app/sql:ro
      # Shared Hugging Face cache (all bots share models)
      - huggingface_cache:/app/cache/huggingface
      # Live code mounting for development (no rebuild needed)
      - ./src:/app/src
      - ./scripts:/app/scripts
      - ./characters:/app/characters
      - ./config:/app/config
      - ./validate_config.py:/app/validate_config.py
      - ./run.py:/app/run.py
      - ./env_manager.py:/app/env_manager.py
      # Note: Using external host mount for logs instead of Docker volumes
    networks:
      - bot_network
    depends_on:
      db-migrate:
        condition: service_completed_successfully
      postgres:
        condition: service_healthy
      qdrant:
        condition: service_started
      influxdb:
        condition: service_healthy
      # - redis  # Commented out - using vector-native memory only

  aethys-bot:
    image: whisperengine-bot:${VERSION:-latest}
    container_name: whisperengine-multi-aethys-bot
    restart: unless-stopped
    entrypoint: []  # Override Docker entrypoint for direct Python execution
    command: ["python", "run.py"]
    env_file:
      - .env.aethys
    environment:
      - DISCORD_BOT_NAME=aethys
      - POSTGRES_HOST=postgres
      - QDRANT_HOST=qdrant
      - MODEL_CACHE_DIR=/app/models
      - DISABLE_MODEL_DOWNLOAD=true
      - HF_HUB_OFFLINE=false
      - TRANSFORMERS_OFFLINE=0
      - HF_HOME=/app/cache/huggingface
      - TRANSFORMERS_CACHE=/app/cache/huggingface
      - HF_DATASETS_CACHE=/app/cache/huggingface
      - HUGGINGFACE_HUB_CACHE=/app/cache/huggingface
      - FASTEMBED_CACHE_PATH=/app/cache/fastembed
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - DEBUG_MODE=false
      - PYTHONUNBUFFERED=1
      - HEALTH_CHECK_PORT=3007
      - HEALTH_CHECK_HOST=0.0.0.0
    ports:
      - "3007:3007"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        tag: "aethys-{.ImageName}-{.Name}-{.ID}"
    deploy:
      resources:
        limits:
          memory: 4G
          cpus: "4.0"
        reservations:
          memory: 2G
          cpus: "2.0"
    healthcheck:
      test:
        - CMD-SHELL
        - curl -f http://localhost:3007/health || exit 1
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    volumes:
      - aethys_backups:/app/backups
      - aethys_privacy:/app/privacy
      - aethys_temp:/app/temp
      - ./logs:/app/logs  # External mount for prompt/response logs
      - ./sql:/app/sql:ro
      # Shared Hugging Face cache (all bots share models)
      - huggingface_cache:/app/cache/huggingface
      # Live code mounting for development (no rebuild needed)
      - ./src:/app/src
      - ./scripts:/app/scripts
      - ./characters:/app/characters
      - ./config:/app/config
      - ./validate_config.py:/app/validate_config.py
      - ./run.py:/app/run.py
      - ./env_manager.py:/app/env_manager.py
      # Note: Using external host mount for logs instead of Docker volumes
    networks:
      - bot_network
    depends_on:
      db-migrate:
        condition: service_completed_successfully
      postgres:
        condition: service_healthy
      qdrant:
        condition: service_started
      influxdb:
        condition: service_healthy
      # - redis  # Commented out - using vector-native memory only

  ryan-bot:
    image: whisperengine-bot:${VERSION:-latest}
    container_name: whisperengine-multi-ryan-bot
    restart: unless-stopped
    entrypoint: []  # Override Docker entrypoint for direct Python execution
    command: ["python", "run.py"]
    env_file:
      - .env.ryan
    environment:
      - DISCORD_BOT_NAME=ryan
      - POSTGRES_HOST=postgres
      - QDRANT_HOST=qdrant
      - MODEL_CACHE_DIR=/app/models
      - DISABLE_MODEL_DOWNLOAD=true
      - HF_HUB_OFFLINE=false
      - TRANSFORMERS_OFFLINE=0
      - HF_HOME=/app/cache/huggingface
      - TRANSFORMERS_CACHE=/app/cache/huggingface
      - HF_DATASETS_CACHE=/app/cache/huggingface
      - HUGGINGFACE_HUB_CACHE=/app/cache/huggingface
      - FASTEMBED_CACHE_PATH=/app/cache/fastembed
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - DEBUG_MODE=false
      - PYTHONUNBUFFERED=1
      - HEALTH_CHECK_PORT=9093
      - HEALTH_CHECK_HOST=0.0.0.0
    ports:
      - "9093:9093"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        tag: "ryan-{.ImageName}-{.Name}-{.ID}"
    deploy:
      resources:
        limits:
          memory: 4G
          cpus: "4.0"
        reservations:
          memory: 2G
          cpus: "2.0"
    healthcheck:
      test:
        - CMD-SHELL
        - curl -f http://localhost:9093/health || exit 1
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    volumes:
      - ryan_backups:/app/backups
      - ryan_privacy:/app/privacy
      - ryan_temp:/app/temp
      - ./logs:/app/logs  # External mount for prompt/response logs
      - ./sql:/app/sql:ro
      # Shared Hugging Face cache (all bots share models)
      - huggingface_cache:/app/cache/huggingface
      # Live code mounting for development (no rebuild needed)
      - ./src:/app/src
      - ./scripts:/app/scripts
      - ./characters:/app/characters
      - ./config:/app/config
      - ./validate_config.py:/app/validate_config.py
      - ./run.py:/app/run.py
      - ./env_manager.py:/app/env_manager.py
      # Note: Using external host mount for logs instead of Docker volumes
    networks:
      - bot_network
    depends_on:
      db-migrate:
        condition: service_completed_successfully
      postgres:
        condition: service_healthy
      qdrant:
        condition: service_started
      influxdb:
        condition: service_healthy
      # - redis  # Commented out - using vector-native memory only

  gabriel-bot:
    image: whisperengine-bot:${VERSION:-latest}
    container_name: whisperengine-multi-gabriel-bot
    restart: unless-stopped
    entrypoint: []  # Override Docker entrypoint for direct Python execution
    command: ["python", "run.py"]
    env_file:
      - .env.gabriel
    environment:
      - DISCORD_BOT_NAME=gabriel
      - POSTGRES_HOST=postgres
      - QDRANT_HOST=qdrant
      - MODEL_CACHE_DIR=/app/models
      - DISABLE_MODEL_DOWNLOAD=true
      - HF_HUB_OFFLINE=false
      - TRANSFORMERS_OFFLINE=0
      - HF_HOME=/app/cache/huggingface
      - TRANSFORMERS_CACHE=/app/cache/huggingface
      - HF_DATASETS_CACHE=/app/cache/huggingface
      - HUGGINGFACE_HUB_CACHE=/app/cache/huggingface
      - FASTEMBED_CACHE_PATH=/app/cache/fastembed
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - DEBUG_MODE=false
      - PYTHONUNBUFFERED=1
      - HEALTH_CHECK_PORT=9095
      - HEALTH_CHECK_HOST=0.0.0.0
    ports:
      - "9095:9095"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        tag: "gabriel-{.ImageName}-{.Name}-{.ID}"
    deploy:
      resources:
        limits:
          memory: 4G
          cpus: "4.0"
        reservations:
          memory: 2G
          cpus: "2.0"
    healthcheck:
      test:
        - CMD-SHELL
        - curl -f http://localhost:9095/health || exit 1
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    volumes:
      - gabriel_backups:/app/backups
      - gabriel_privacy:/app/privacy
      - gabriel_temp:/app/temp
      - ./logs:/app/logs  # External mount for prompt/response logs
      - ./sql:/app/sql:ro
      # Shared Hugging Face cache (all bots share models)
      - huggingface_cache:/app/cache/huggingface
      # Live code mounting for development (no rebuild needed)
      - ./src:/app/src
      - ./scripts:/app/scripts
      - ./characters:/app/characters
      - ./config:/app/config
      - ./validate_config.py:/app/validate_config.py
      - ./run.py:/app/run.py
      - ./env_manager.py:/app/env_manager.py
      # Note: Using external host mount for logs instead of Docker volumes
    networks:
      - bot_network
    depends_on:
      db-migrate:
        condition: service_completed_successfully
      postgres:
        condition: service_healthy
      qdrant:
        condition: service_started
      influxdb:
        condition: service_healthy
      # - redis  # Commented out - using vector-native memory only

  sophia-bot:
    image: whisperengine-bot:${VERSION:-latest}
    container_name: whisperengine-multi-sophia-bot
    restart: unless-stopped
    entrypoint: []  # Override Docker entrypoint for direct Python execution
    command: ["python", "run.py"]
    env_file:
      - .env.sophia
    environment:
      - DISCORD_BOT_NAME=sophia
      - POSTGRES_HOST=postgres
      - QDRANT_HOST=qdrant
      - MODEL_CACHE_DIR=/app/models
      - DISABLE_MODEL_DOWNLOAD=true
      - HF_HUB_OFFLINE=false
      - TRANSFORMERS_OFFLINE=0
      - HF_HOME=/app/cache/huggingface
      - TRANSFORMERS_CACHE=/app/cache/huggingface
      - HF_DATASETS_CACHE=/app/cache/huggingface
      - HUGGINGFACE_HUB_CACHE=/app/cache/huggingface
      - FASTEMBED_CACHE_PATH=/app/cache/fastembed
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - DEBUG_MODE=false
      - PYTHONUNBUFFERED=1
      - HEALTH_CHECK_PORT=9096
      - HEALTH_CHECK_HOST=0.0.0.0
    ports:
      - "9096:9096"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        tag: "sophia-{.ImageName}-{.Name}-{.ID}"
    deploy:
      resources:
        limits:
          memory: 4G
          cpus: "4.0"
        reservations:
          memory: 2G
          cpus: "2.0"
    healthcheck:
      test:
        - CMD-SHELL
        - curl -f http://localhost:9096/health || exit 1
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    volumes:
      - sophia_backups:/app/backups
      - sophia_privacy:/app/privacy
      - sophia_temp:/app/temp
      - ./logs:/app/logs  # External mount for prompt/response logs
      - ./sql:/app/sql:ro
      # Shared Hugging Face cache (all bots share models)
      - huggingface_cache:/app/cache/huggingface
      # Live code mounting for development (no rebuild needed)
      - ./src:/app/src
      - ./scripts:/app/scripts
      - ./characters:/app/characters
      - ./config:/app/config
      - ./validate_config.py:/app/validate_config.py
      - ./run.py:/app/run.py
      - ./env_manager.py:/app/env_manager.py
      # Note: Using external host mount for logs instead of Docker volumes
    networks:
      - bot_network
    depends_on:
      db-migrate:
        condition: service_completed_successfully
      postgres:
        condition: service_healthy
      qdrant:
        condition: service_started
      influxdb:
        condition: service_healthy
      # - redis  # Commented out - using vector-native memory only

# ===== NETWORKS =====
networks:
  bot_network:
    name: whisperengine-multi_bot_network
    driver: bridge
    # Development network - external access enabled for debugging
    internal: false
    ipam:
      config:
        - subnet: 172.21.0.0/16

# ===== VOLUMES =====
volumes:
  postgres_data:
    name: whisperengine-multi_postgres_data
  # redis_data:  # Commented out - using vector-native memory only
  #   name: whisperengine-multi_redis_data
  qdrant_data:
    name: whisperengine-multi_qdrant_data
  influxdb_data:
    name: whisperengine-multi_influxdb_data
    external: true
  influxdb_config:
    name: whisperengine-multi_influxdb_config
    external: true
  grafana_data:
    name: whisperengine-multi_grafana_data
    external: true
  
  # Shared Hugging Face cache (all bots share to avoid duplicate downloads)
  huggingface_cache:
    name: whisperengine-multi_huggingface_cache
  
  # Bot-specific volumes (populated dynamically)
  jake_backups:
    name: whisperengine-multi_jake_backups
  jake_privacy:
    name: whisperengine-multi_jake_privacy
  jake_temp:
    name: whisperengine-multi_jake_temp
  jake_logs:
    name: whisperengine-multi_jake_logs
  marcus_backups:
    name: whisperengine-multi_marcus_backups
  marcus_privacy:
    name: whisperengine-multi_marcus_privacy
  marcus_temp:
    name: whisperengine-multi_marcus_temp
  marcus_logs:
    name: whisperengine-multi_marcus_logs
  dotty_backups:
    name: whisperengine-multi_dotty_backups
  dotty_privacy:
    name: whisperengine-multi_dotty_privacy
  dotty_temp:
    name: whisperengine-multi_dotty_temp
  dotty_logs:
    name: whisperengine-multi_dotty_logs
  dream_backups:
    name: whisperengine-multi_dream_backups
  dream_privacy:
    name: whisperengine-multi_dream_privacy
  dream_temp:
    name: whisperengine-multi_dream_temp
  dream_logs:
    name: whisperengine-multi_dream_logs
  aetheris_backups:
    name: whisperengine-multi_aetheris_backups
  aetheris_privacy:
    name: whisperengine-multi_aetheris_privacy
  aetheris_temp:
    name: whisperengine-multi_aetheris_temp
  aetheris_logs:
    name: whisperengine-multi_aetheris_logs
  elena_backups:
    name: whisperengine-multi_elena_backups
  elena_privacy:
    name: whisperengine-multi_elena_privacy
  elena_temp:
    name: whisperengine-multi_elena_temp
  elena_logs:
    name: whisperengine-multi_elena_logs
  aethys_backups:
    name: whisperengine-multi_aethys_backups
  aethys_privacy:
    name: whisperengine-multi_aethys_privacy
  aethys_temp:
    name: whisperengine-multi_aethys_temp
  aethys_logs:
    name: whisperengine-multi_aethys_logs
  ryan_backups:
    name: whisperengine-multi_ryan_backups
  ryan_privacy:
    name: whisperengine-multi_ryan_privacy
  ryan_temp:
    name: whisperengine-multi_ryan_temp
  ryan_logs:
    name: whisperengine-multi_ryan_logs
  gabriel_backups:
    name: whisperengine-multi_gabriel_backups
  gabriel_privacy:
    name: whisperengine-multi_gabriel_privacy
  gabriel_temp:
    name: whisperengine-multi_gabriel_temp
  gabriel_logs:
    name: whisperengine-multi_gabriel_logs
  sophia_backups:
    name: whisperengine-multi_sophia_backups
  sophia_privacy:
    name: whisperengine-multi_sophia_privacy
  sophia_temp:
    name: whisperengine-multi_sophia_temp
  sophia_logs:
    name: whisperengine-multi_sophia_logs