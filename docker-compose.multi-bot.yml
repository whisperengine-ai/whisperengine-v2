# Multi-Bot Docker Compose Template
# This file is a template that gets populated by scripts/generate_multi_bot_config.py
# DO NOT EDIT: Generated sections will be overwritten
#
# Port Strategy:
# - Development Environment (multi-bot.sh): 5000-5999 + 9000-9999 range
# - End-User Environment (quickstart): 8000-8999 range
# - Docker Networks: whisperengine-multi_bot_network (dev) vs whisperengine-quickstart-network (end-user)

services:
  # ===== DATABASE MIGRATION INIT CONTAINER =====
  # Runs once before bot services start to ensure schema is up-to-date
  db-migrate:
    image: whisperengine-bot:${VERSION:-latest}
    container_name: whisperengine-db-migrate
    entrypoint: []  # Override entrypoint to prevent main app startup
    command: ["python", "/app/scripts/run_migrations.py"]
    environment:
      - POSTGRES_HOST=postgres
      - POSTGRES_PORT=5432
      - POSTGRES_USER=whisperengine
      - POSTGRES_PASSWORD=whisperengine_password
      - POSTGRES_DB=whisperengine
      - MIGRATION_MODE=dev  # DEV mode: graceful handling when Docker image lacks latest migrations
    volumes:
      - ./scripts:/app/scripts  # Live mount for migration scripts
      - ./sql:/app/sql:ro       # Live mount for SQL files
      - ./alembic:/app/alembic  # Live mount for Alembic migrations (DEV mode)
      - ./alembic.ini:/app/alembic.ini:ro  # Live mount for Alembic config
    networks:
      - bot_network
    depends_on:
      postgres:
        condition: service_healthy
    restart: "no"  # Only run once per docker-compose up

  # ===== INFRASTRUCTURE SERVICES =====
  postgres:
    image: postgres:16.4-alpine
    container_name: postgres
    restart: unless-stopped
    environment:
      POSTGRES_DB: whisperengine
      POSTGRES_USER: whisperengine
      POSTGRES_PASSWORD: whisperengine_password
      POSTGRES_HOST_AUTH_METHOD: trust
    ports:
      - "5433:5432"  # PostgreSQL (Dev: 5433, Quickstart: 8432)
    volumes:
      - postgres_data:/var/lib/postgresql/data
    networks:
      - bot_network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U whisperengine -d whisperengine"]
      interval: 10s
      timeout: 10s
      retries: 5
      start_period: 40s

  # Redis - Commented out since we're using vector-native memory with Qdrant only
  # redis:
  #   image: redis:7.4-alpine
  #   container_name: whisperengine-multi-redis
  #   restart: unless-stopped
  #   ports:
  #     - "6380:6379"
  #   volumes:
  #     - redis_data:/data
  #   networks:
  #     - bot_network
  #   healthcheck:
  #     test: ["CMD", "redis-cli", "ping"]
  #     interval: 30s
  #     timeout: 10s
  #     retries: 3

  qdrant:
    image: qdrant/qdrant:v1.15.4
    container_name: qdrant
    restart: unless-stopped
    ports:
      - "6334:6333"  # Qdrant (Dev: 6334, Quickstart: 8333)
    volumes:
      - qdrant_data:/qdrant/storage
    networks:
      - bot_network
    # Health check disabled - Qdrant containers have minimal tooling
    # Bots will retry connections if Qdrant isn't ready yet

  influxdb:
    image: influxdb:2.7-alpine
    container_name: influxdb
    restart: unless-stopped
    environment:
      DOCKER_INFLUXDB_INIT_MODE: setup
      DOCKER_INFLUXDB_INIT_USERNAME: whisperengine
      DOCKER_INFLUXDB_INIT_PASSWORD: whisperengine_metrics
      DOCKER_INFLUXDB_INIT_ORG: whisperengine
      DOCKER_INFLUXDB_INIT_BUCKET: performance_metrics
      DOCKER_INFLUXDB_INIT_ADMIN_TOKEN: whisperengine-fidelity-first-metrics-token
    ports:
      - "8087:8086"  # InfluxDB (Dev: 8087, Quickstart: 8088)
    volumes:
      - influxdb_data:/var/lib/influxdb2
      - influxdb_config:/etc/influxdb2
    networks:
      - bot_network
    healthcheck:
      test: ["CMD", "influx", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # Grafana monitoring dashboard for development environment
  grafana:
    image: grafana/grafana:11.3.0
    container_name: grafana
    restart: unless-stopped
    environment:
      GF_SECURITY_ADMIN_USER: ${GRAFANA_USER:-admin}
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_PASSWORD:-whisperengine_grafana}
      GF_FEATURE_TOGGLES_ENABLE: publicDashboards
      INFLUXDB_USER: whisperengine
      INFLUXDB_PASSWORD: whisperengine_metrics
    ports:
      - "3002:3000"  # Grafana (Dev: 3002, Quickstart: 8001)
    volumes:
      - grafana_data:/var/lib/grafana
      - ./grafana-config/datasource.yml:/etc/grafana/provisioning/datasources/datasource.yml:ro
      - ./grafana-config/dashboard.yml:/etc/grafana/provisioning/dashboards/dashboard.yml:ro
      - ./dashboards:/var/lib/grafana/dashboards:ro
    networks:
      - bot_network
    depends_on:
      influxdb:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # CDL Web UI for character authoring and management
  cdl-web-ui:
    build:
      context: ./cdl-web-ui
      dockerfile: Dockerfile
    container_name: cdl-web-ui
    restart: unless-stopped
    environment:
      # PostgreSQL connection (same as WhisperEngine bots)
      POSTGRES_HOST: postgres
      POSTGRES_PORT: 5432
      POSTGRES_DB: whisperengine
      POSTGRES_USER: whisperengine
      POSTGRES_PASSWORD: whisperengine_password
      # Legacy PG* variables for compatibility
      PGHOST: postgres
      PGPORT: 5432
      PGDATABASE: whisperengine
      PGUSER: whisperengine
      PGPASSWORD: whisperengine_password
      # Next.js configuration
      NODE_ENV: production
      NEXT_TELEMETRY_DISABLED: 1
    ports:
      - "3001:3000"  # CDL Web UI (Dev: 3001, Quickstart: 8080)
    depends_on:
      postgres:
        condition: service_healthy
    networks:
      - bot_network
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:3000/api/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3

  # ===== SHARED BOT IMAGE BUILD =====
  whisperengine-bot-builder:
    build:
      context: .
      dockerfile: Dockerfile
    image: whisperengine-bot:${VERSION:-latest}
    profiles:
      - build-only
    # This service is only used for building the shared image
    # It doesn't run as a container - just builds the image

  # ===== BACKGROUND ENRICHMENT WORKER =====
  enrichment-worker:
    profiles:
      - enrichment  # Optional: Start with --profile enrichment (safety for dev environments)
    build:
      context: .
      dockerfile: docker/Dockerfile.enrichment-worker
    container_name: enrichment-worker
    restart: unless-stopped
    environment:
      # Qdrant connection
      - QDRANT_HOST=qdrant
      - QDRANT_PORT=6333
      
      # PostgreSQL connection
      - POSTGRES_HOST=postgres
      - POSTGRES_PORT=5432
      - POSTGRES_DB=whisperengine
      - POSTGRES_USER=whisperengine
      - POSTGRES_PASSWORD=whisperengine_password
      
      # Enrichment configuration
      - ENRICHMENT_INTERVAL_SECONDS=${ENRICHMENT_INTERVAL_SECONDS:-660}  # 11 minutes (prime number - tested optimal)
      - ENRICHMENT_BATCH_SIZE=${ENRICHMENT_BATCH_SIZE:-50}
      - MIN_MESSAGES_FOR_SUMMARY=${MIN_MESSAGES_FOR_SUMMARY:-5}
      - TIME_WINDOW_HOURS=${TIME_WINDOW_HOURS:-24}
      - LOOKBACK_DAYS=${LOOKBACK_DAYS:-3}  # Reduced from 30 to avoid massive backfill token burn
      
      # LLM configuration
      - LLM_MODEL=${ENRICHMENT_LLM_MODEL:-anthropic/claude-3.5-sonnet}
      - LLM_CHAT_MODEL=${LLM_CHAT_MODEL:-anthropic/claude-sonnet-4.5}
      - LLM_FACT_EXTRACTION_MODEL=${LLM_FACT_EXTRACTION_MODEL:-anthropic/claude-sonnet-4.5}
      - LLM_TEMPERATURE=${LLM_TEMPERATURE:-0.7}
      - LLM_FACT_EXTRACTION_TEMPERATURE=${LLM_FACT_EXTRACTION_TEMPERATURE:-0.2}
      - LLM_CHAT_API_URL=${LLM_CHAT_API_URL}
      - LLM_CHAT_API_KEY=${LLM_CHAT_API_KEY}
      
      # InfluxDB configuration (read-only access for temporal analysis)
      - INFLUXDB_URL=${INFLUXDB_URL:-http://influxdb:8086}
      - INFLUXDB_TOKEN=${INFLUXDB_TOKEN:-whisperengine-fidelity-first-metrics-token}
      - INFLUXDB_ORG=${INFLUXDB_ORG:-whisperengine}
      - INFLUXDB_BUCKET=${INFLUXDB_BUCKET:-performance_metrics}
    depends_on:
      postgres:
        condition: service_healthy
      qdrant:
        condition: service_started  # Qdrant has no healthcheck configured
    networks:
      - bot_network
    volumes:
      - ./logs/enrichment:/app/logs/enrichment
      # Live code mounting for development (no rebuild needed)
      - ./src:/app/src:ro
    healthcheck:
      test: ["CMD-SHELL", "ps aux | grep 'src.enrichment.worker' | grep -v grep || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 120s

  # ===== BOT SERVICES (populated dynamically) =====
  jake-bot:
    image: whisperengine-bot:${VERSION:-latest}
    container_name: jake-bot
    restart: unless-stopped
    entrypoint: []  # Override Docker entrypoint for direct Python execution
    command: ["python", "run.py"]
    env_file:
      - .env.jake
    environment:
      - DISCORD_BOT_NAME=jake
      - POSTGRES_HOST=postgres
      - QDRANT_HOST=qdrant
      - MODEL_CACHE_DIR=/app/models
      - DISABLE_MODEL_DOWNLOAD=true
      - HF_HUB_OFFLINE=false
      - TRANSFORMERS_OFFLINE=0
      - HF_HOME=/app/cache/huggingface
      - TRANSFORMERS_CACHE=/app/cache/huggingface
      - HF_DATASETS_CACHE=/app/cache/huggingface
      - HUGGINGFACE_HUB_CACHE=/app/cache/huggingface
      - FASTEMBED_CACHE_PATH=/app/cache/fastembed
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - DEBUG_MODE=false
      - PYTHONUNBUFFERED=1
      - HEALTH_CHECK_PORT=9097
      - HEALTH_CHECK_HOST=0.0.0.0
    ports:
      - "9097:9097"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        tag: "jake-{.ImageName}-{.Name}-{.ID}"
    deploy:
      resources:
        limits:
          memory: 4G
          cpus: "4.0"
        reservations:
          memory: 2G
          cpus: "2.0"
    healthcheck:
      test:
        - CMD-SHELL
        - curl -f http://localhost:9097/health || exit 1
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    volumes:
      - jake_backups:/app/backups
      - jake_privacy:/app/privacy
      - jake_temp:/app/temp
      - ./logs:/app/logs  # External mount for prompt/response logs
      - ./sql:/app/sql:ro
      # Shared Hugging Face cache (all bots share models)
      - huggingface_cache:/app/cache/huggingface
      # Live code mounting for development (no rebuild needed)
      - ./src:/app/src
      - ./scripts:/app/scripts
      - ./characters:/app/characters
      - ./config:/app/config
      - ./run.py:/app/run.py
      - ./env_manager.py:/app/env_manager.py
      # Note: Using external host mount for logs instead of Docker volumes
    networks:
      - bot_network
    depends_on:
      db-migrate:
        condition: service_completed_successfully
      postgres:
        condition: service_healthy
      qdrant:
        condition: service_started
      influxdb:
        condition: service_healthy
      # - redis  # Commented out - using vector-native memory only

  marcus-bot:
    image: whisperengine-bot:${VERSION:-latest}
    container_name: marcus-bot
    restart: unless-stopped
    entrypoint: []  # Override Docker entrypoint for direct Python execution
    command: ["python", "run.py"]
    env_file:
      - .env.marcus
    environment:
      - DISCORD_BOT_NAME=marcus
      - POSTGRES_HOST=postgres
      - QDRANT_HOST=qdrant
      - MODEL_CACHE_DIR=/app/models
      - DISABLE_MODEL_DOWNLOAD=true
      - HF_HUB_OFFLINE=false
      - TRANSFORMERS_OFFLINE=0
      - HF_HOME=/app/cache/huggingface
      - TRANSFORMERS_CACHE=/app/cache/huggingface
      - HF_DATASETS_CACHE=/app/cache/huggingface
      - HUGGINGFACE_HUB_CACHE=/app/cache/huggingface
      - FASTEMBED_CACHE_PATH=/app/cache/fastembed
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - DEBUG_MODE=false
      - PYTHONUNBUFFERED=1
      - HEALTH_CHECK_PORT=9092
      - HEALTH_CHECK_HOST=0.0.0.0
    ports:
      - "9092:9092"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        tag: "marcus-{.ImageName}-{.Name}-{.ID}"
    deploy:
      resources:
        limits:
          memory: 4G
          cpus: "4.0"
        reservations:
          memory: 2G
          cpus: "2.0"
    healthcheck:
      test:
        - CMD-SHELL
        - curl -f http://localhost:9092/health || exit 1
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    volumes:
      - marcus_backups:/app/backups
      - marcus_privacy:/app/privacy
      - marcus_temp:/app/temp
      - ./logs:/app/logs  # External mount for prompt/response logs
      - ./sql:/app/sql:ro
      # Shared Hugging Face cache (all bots share models)
      - huggingface_cache:/app/cache/huggingface
      # Live code mounting for development (no rebuild needed)
      - ./src:/app/src
      - ./scripts:/app/scripts
      - ./characters:/app/characters
      - ./config:/app/config
      - ./run.py:/app/run.py
      - ./env_manager.py:/app/env_manager.py
      # Note: Using external host mount for logs instead of Docker volumes
    networks:
      - bot_network
    depends_on:
      db-migrate:
        condition: service_completed_successfully
      postgres:
        condition: service_healthy
      qdrant:
        condition: service_started
      influxdb:
        condition: service_healthy
      # - redis  # Commented out - using vector-native memory only

  dotty-bot:
    image: whisperengine-bot:${VERSION:-latest}
    container_name: dotty-bot
    restart: unless-stopped
    entrypoint: []  # Override Docker entrypoint for direct Python execution
    command: ["python", "run.py"]
    env_file:
      - .env.dotty
    environment:
      - DISCORD_BOT_NAME=dotty
      - POSTGRES_HOST=postgres
      - QDRANT_HOST=qdrant
      - MODEL_CACHE_DIR=/app/models
      - DISABLE_MODEL_DOWNLOAD=true
      - HF_HUB_OFFLINE=false
      - TRANSFORMERS_OFFLINE=0
      - HF_HOME=/app/cache/huggingface
      - TRANSFORMERS_CACHE=/app/cache/huggingface
      - HF_DATASETS_CACHE=/app/cache/huggingface
      - HUGGINGFACE_HUB_CACHE=/app/cache/huggingface
      - FASTEMBED_CACHE_PATH=/app/cache/fastembed
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - DEBUG_MODE=false
      - PYTHONUNBUFFERED=1
      - HEALTH_CHECK_PORT=9098
      - HEALTH_CHECK_HOST=0.0.0.0
    ports:
      - "9098:9098"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        tag: "dotty-{.ImageName}-{.Name}-{.ID}"
    deploy:
      resources:
        limits:
          memory: 4G
          cpus: "4.0"
        reservations:
          memory: 2G
          cpus: "2.0"
    healthcheck:
      test:
        - CMD-SHELL
        - curl -f http://localhost:9098/health || exit 1
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    volumes:
      - dotty_backups:/app/backups
      - dotty_privacy:/app/privacy
      - dotty_temp:/app/temp
      - ./logs:/app/logs  # External mount for prompt/response logs
      - ./sql:/app/sql:ro
      # Shared Hugging Face cache (all bots share models)
      - huggingface_cache:/app/cache/huggingface
      # Live code mounting for development (no rebuild needed)
      - ./src:/app/src
      - ./scripts:/app/scripts
      - ./characters:/app/characters
      - ./config:/app/config
      - ./run.py:/app/run.py
      - ./env_manager.py:/app/env_manager.py
      # Note: Using external host mount for logs instead of Docker volumes
    networks:
      - bot_network
    depends_on:
      db-migrate:
        condition: service_completed_successfully
      postgres:
        condition: service_healthy
      qdrant:
        condition: service_started
      influxdb:
        condition: service_healthy
      # - redis  # Commented out - using vector-native memory only

  dream-bot:
    image: whisperengine-bot:${VERSION:-latest}
    container_name: dream-bot
    restart: unless-stopped
    entrypoint: []  # Override Docker entrypoint for direct Python execution
    command: ["python", "run.py"]
    env_file:
      - .env.dream
    environment:
      - DISCORD_BOT_NAME=dream
      - POSTGRES_HOST=postgres
      - QDRANT_HOST=qdrant
      - MODEL_CACHE_DIR=/app/models
      - DISABLE_MODEL_DOWNLOAD=true
      - HF_HUB_OFFLINE=false
      - TRANSFORMERS_OFFLINE=0
      - HF_HOME=/app/cache/huggingface
      - TRANSFORMERS_CACHE=/app/cache/huggingface
      - HF_DATASETS_CACHE=/app/cache/huggingface
      - HUGGINGFACE_HUB_CACHE=/app/cache/huggingface
      - FASTEMBED_CACHE_PATH=/app/cache/fastembed
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - DEBUG_MODE=false
      - PYTHONUNBUFFERED=1
      - HEALTH_CHECK_PORT=9094
      - HEALTH_CHECK_HOST=0.0.0.0
    ports:
      - "9094:9094"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        tag: "dream-{.ImageName}-{.Name}-{.ID}"
    deploy:
      resources:
        limits:
          memory: 4G
          cpus: "4.0"
        reservations:
          memory: 2G
          cpus: "2.0"
    healthcheck:
      test:
        - CMD-SHELL
        - curl -f http://localhost:9094/health || exit 1
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    volumes:
      - dream_backups:/app/backups
      - dream_privacy:/app/privacy
      - dream_temp:/app/temp
      - ./logs:/app/logs  # External mount for prompt/response logs
      - ./sql:/app/sql:ro
      # Shared Hugging Face cache (all bots share models)
      - huggingface_cache:/app/cache/huggingface
      # Live code mounting for development (no rebuild needed)
      - ./src:/app/src
      - ./scripts:/app/scripts
      - ./characters:/app/characters
      - ./config:/app/config
      - ./run.py:/app/run.py
      - ./env_manager.py:/app/env_manager.py
      # Note: Using external host mount for logs instead of Docker volumes
    networks:
      - bot_network
    depends_on:
      db-migrate:
        condition: service_completed_successfully
      postgres:
        condition: service_healthy
      qdrant:
        condition: service_started
      influxdb:
        condition: service_healthy
      # - redis  # Commented out - using vector-native memory only

  nottaylor-bot:
    image: whisperengine-bot:${VERSION:-latest}
    container_name: nottaylor-bot
    restart: unless-stopped
    entrypoint: []  # Override Docker entrypoint for direct Python execution
    command: ["python", "run.py"]
    env_file:
      - .env.nottaylor
    environment:
      - DISCORD_BOT_NAME=nottaylor
      - POSTGRES_HOST=postgres
      - QDRANT_HOST=qdrant
      - MODEL_CACHE_DIR=/app/models
      - DISABLE_MODEL_DOWNLOAD=true
      - HF_HUB_OFFLINE=false
      - TRANSFORMERS_OFFLINE=0
      - HF_HOME=/app/cache/huggingface
      - TRANSFORMERS_CACHE=/app/cache/huggingface
      - HF_DATASETS_CACHE=/app/cache/huggingface
      - HUGGINGFACE_HUB_CACHE=/app/cache/huggingface
      - FASTEMBED_CACHE_PATH=/app/cache/fastembed
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - DEBUG_MODE=false
      - PYTHONUNBUFFERED=1
      - HEALTH_CHECK_PORT=9100
      - HEALTH_CHECK_HOST=0.0.0.0
    ports:
      - "9100:9100"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        tag: "nottaylor-{.ImageName}-{.Name}-{.ID}"
    deploy:
      resources:
        limits:
          memory: 4G
          cpus: "4.0"
        reservations:
          memory: 2G
          cpus: "2.0"
    healthcheck:
      test:
        - CMD-SHELL
        - curl -f http://localhost:9100/health || exit 1
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    volumes:
      - nottaylor_backups:/app/backups
      - nottaylor_privacy:/app/privacy
      - nottaylor_temp:/app/temp
      - ./logs:/app/logs  # External mount for prompt/response logs
      - ./sql:/app/sql:ro
      # Shared Hugging Face cache (all bots share models)
      - huggingface_cache:/app/cache/huggingface
      # Live code mounting for development (no rebuild needed)
      - ./src:/app/src
      - ./scripts:/app/scripts
      - ./characters:/app/characters
      - ./config:/app/config
      - ./run.py:/app/run.py
      - ./env_manager.py:/app/env_manager.py
      # Note: Using external host mount for logs instead of Docker volumes
    networks:
      - bot_network
    depends_on:
      db-migrate:
        condition: service_completed_successfully
      postgres:
        condition: service_healthy
      qdrant:
        condition: service_started
      influxdb:
        condition: service_healthy
      # - redis  # Commented out - using vector-native memory only

  assistant-bot:
    image: whisperengine-bot:${VERSION:-latest}
    container_name: assistant-bot
    restart: unless-stopped
    entrypoint: []  # Override Docker entrypoint for direct Python execution
    command: ["python", "run.py"]
    env_file:
      - .env.assistant
    environment:
      - DISCORD_BOT_NAME=assistant
      - POSTGRES_HOST=postgres
      - QDRANT_HOST=qdrant
      - MODEL_CACHE_DIR=/app/models
      - DISABLE_MODEL_DOWNLOAD=true
      - HF_HUB_OFFLINE=false
      - TRANSFORMERS_OFFLINE=0
      - HF_HOME=/app/cache/huggingface
      - TRANSFORMERS_CACHE=/app/cache/huggingface
      - HF_DATASETS_CACHE=/app/cache/huggingface
      - HUGGINGFACE_HUB_CACHE=/app/cache/huggingface
      - FASTEMBED_CACHE_PATH=/app/cache/fastembed
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - DEBUG_MODE=false
      - PYTHONUNBUFFERED=1
      - HEALTH_CHECK_PORT=9101
      - HEALTH_CHECK_HOST=0.0.0.0
    ports:
      - "9101:9101"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        tag: "assistant-{.ImageName}-{.Name}-{.ID}"
    deploy:
      resources:
        limits:
          memory: 4G
          cpus: "4.0"
        reservations:
          memory: 2G
          cpus: "2.0"
    healthcheck:
      test:
        - CMD-SHELL
        - curl -f http://localhost:9101/health || exit 1
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    volumes:
      - assistant_backups:/app/backups
      - assistant_privacy:/app/privacy
      - assistant_temp:/app/temp
      - ./logs:/app/logs  # External mount for prompt/response logs
      - ./sql:/app/sql:ro
      # Shared Hugging Face cache (all bots share models)
      - huggingface_cache:/app/cache/huggingface
      # Live code mounting for development (no rebuild needed)
      - ./src:/app/src
      - ./scripts:/app/scripts
      - ./characters:/app/characters
      - ./config:/app/config
      - ./run.py:/app/run.py
      - ./env_manager.py:/app/env_manager.py
      # Note: Using external host mount for logs instead of Docker volumes
    networks:
      - bot_network
    depends_on:
      db-migrate:
        condition: service_completed_successfully
      postgres:
        condition: service_healthy
      qdrant:
        condition: service_started
      influxdb:
        condition: service_healthy
      # - redis  # Commented out - using vector-native memory only

  aetheris-bot:
    image: whisperengine-bot:${VERSION:-latest}
    container_name: aetheris-bot
    restart: unless-stopped
    entrypoint: []  # Override Docker entrypoint for direct Python execution
    command: ["python", "run.py"]
    env_file:
      - .env.aetheris
    environment:
      - DISCORD_BOT_NAME=aetheris
      - POSTGRES_HOST=postgres
      - QDRANT_HOST=qdrant
      - MODEL_CACHE_DIR=/app/models
      - DISABLE_MODEL_DOWNLOAD=true
      - HF_HUB_OFFLINE=false
      - TRANSFORMERS_OFFLINE=0
      - HF_HOME=/app/cache/huggingface
      - TRANSFORMERS_CACHE=/app/cache/huggingface
      - HF_DATASETS_CACHE=/app/cache/huggingface
      - HUGGINGFACE_HUB_CACHE=/app/cache/huggingface
      - FASTEMBED_CACHE_PATH=/app/cache/fastembed
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - DEBUG_MODE=false
      - PYTHONUNBUFFERED=1
      - HEALTH_CHECK_PORT=9099
      - HEALTH_CHECK_HOST=0.0.0.0
    ports:
      - "9099:9099"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        tag: "aetheris-{.ImageName}-{.Name}-{.ID}"
    deploy:
      resources:
        limits:
          memory: 4G
          cpus: "4.0"
        reservations:
          memory: 2G
          cpus: "2.0"
    healthcheck:
      test:
        - CMD-SHELL
        - curl -f http://localhost:9099/health || exit 1
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    volumes:
      - aetheris_backups:/app/backups
      - aetheris_privacy:/app/privacy
      - aetheris_temp:/app/temp
      - ./logs:/app/logs  # External mount for prompt/response logs
      - ./sql:/app/sql:ro
      # Shared Hugging Face cache (all bots share models)
      - huggingface_cache:/app/cache/huggingface
      # Live code mounting for development (no rebuild needed)
      - ./src:/app/src
      - ./scripts:/app/scripts
      - ./characters:/app/characters
      - ./config:/app/config
      - ./run.py:/app/run.py
      - ./env_manager.py:/app/env_manager.py
      # Note: Using external host mount for logs instead of Docker volumes
    networks:
      - bot_network
    depends_on:
      db-migrate:
        condition: service_completed_successfully
      postgres:
        condition: service_healthy
      qdrant:
        condition: service_started
      influxdb:
        condition: service_healthy
      # - redis  # Commented out - using vector-native memory only

  elena-bot:
    image: whisperengine-bot:${VERSION:-latest}
    container_name: elena-bot
    restart: unless-stopped
    entrypoint: []  # Override Docker entrypoint for direct Python execution
    command: ["python", "run.py"]
    env_file:
      - .env.elena
    environment:
      - DISCORD_BOT_NAME=elena
      - POSTGRES_HOST=postgres
      - QDRANT_HOST=qdrant
      - MODEL_CACHE_DIR=/app/models
      - DISABLE_MODEL_DOWNLOAD=true
      - HF_HUB_OFFLINE=false
      - TRANSFORMERS_OFFLINE=0
      - HF_HOME=/app/cache/huggingface
      - TRANSFORMERS_CACHE=/app/cache/huggingface
      - HF_DATASETS_CACHE=/app/cache/huggingface
      - HUGGINGFACE_HUB_CACHE=/app/cache/huggingface
      - FASTEMBED_CACHE_PATH=/app/cache/fastembed
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - DEBUG_MODE=false
      - PYTHONUNBUFFERED=1
      - HEALTH_CHECK_PORT=9091
      - HEALTH_CHECK_HOST=0.0.0.0
    ports:
      - "9091:9091"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        tag: "elena-{.ImageName}-{.Name}-{.ID}"
    deploy:
      resources:
        limits:
          memory: 4G
          cpus: "4.0"
        reservations:
          memory: 2G
          cpus: "2.0"
    healthcheck:
      test:
        - CMD-SHELL
        - curl -f http://localhost:9091/health || exit 1
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    volumes:
      - elena_backups:/app/backups
      - elena_privacy:/app/privacy
      - elena_temp:/app/temp
      - ./logs:/app/logs  # External mount for prompt/response logs
      - ./sql:/app/sql:ro
      # Shared Hugging Face cache (all bots share models)
      - huggingface_cache:/app/cache/huggingface
      # Live code mounting for development (no rebuild needed)
      - ./src:/app/src
      - ./scripts:/app/scripts
      - ./characters:/app/characters
      - ./config:/app/config
      - ./run.py:/app/run.py
      - ./env_manager.py:/app/env_manager.py
      # Note: Using external host mount for logs instead of Docker volumes
    networks:
      - bot_network
    depends_on:
      db-migrate:
        condition: service_completed_successfully
      postgres:
        condition: service_healthy
      qdrant:
        condition: service_started
      influxdb:
        condition: service_healthy
      # - redis  # Commented out - using vector-native memory only

  aethys-bot:
    image: whisperengine-bot:${VERSION:-latest}
    container_name: aethys-bot
    restart: unless-stopped
    entrypoint: []  # Override Docker entrypoint for direct Python execution
    command: ["python", "run.py"]
    env_file:
      - .env.aethys
    environment:
      - DISCORD_BOT_NAME=aethys
      - POSTGRES_HOST=postgres
      - QDRANT_HOST=qdrant
      - MODEL_CACHE_DIR=/app/models
      - DISABLE_MODEL_DOWNLOAD=true
      - HF_HUB_OFFLINE=false
      - TRANSFORMERS_OFFLINE=0
      - HF_HOME=/app/cache/huggingface
      - TRANSFORMERS_CACHE=/app/cache/huggingface
      - HF_DATASETS_CACHE=/app/cache/huggingface
      - HUGGINGFACE_HUB_CACHE=/app/cache/huggingface
      - FASTEMBED_CACHE_PATH=/app/cache/fastembed
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - DEBUG_MODE=false
      - PYTHONUNBUFFERED=1
      - HEALTH_CHECK_PORT=3007
      - HEALTH_CHECK_HOST=0.0.0.0
    ports:
      - "3007:3007"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        tag: "aethys-{.ImageName}-{.Name}-{.ID}"
    deploy:
      resources:
        limits:
          memory: 4G
          cpus: "4.0"
        reservations:
          memory: 2G
          cpus: "2.0"
    healthcheck:
      test:
        - CMD-SHELL
        - curl -f http://localhost:3007/health || exit 1
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    volumes:
      - aethys_backups:/app/backups
      - aethys_privacy:/app/privacy
      - aethys_temp:/app/temp
      - ./logs:/app/logs  # External mount for prompt/response logs
      - ./sql:/app/sql:ro
      # Shared Hugging Face cache (all bots share models)
      - huggingface_cache:/app/cache/huggingface
      # Live code mounting for development (no rebuild needed)
      - ./src:/app/src
      - ./scripts:/app/scripts
      - ./characters:/app/characters
      - ./config:/app/config
      - ./run.py:/app/run.py
      - ./env_manager.py:/app/env_manager.py
      # Note: Using external host mount for logs instead of Docker volumes
    networks:
      - bot_network
    depends_on:
      db-migrate:
        condition: service_completed_successfully
      postgres:
        condition: service_healthy
      qdrant:
        condition: service_started
      influxdb:
        condition: service_healthy
      # - redis  # Commented out - using vector-native memory only

  ryan-bot:
    image: whisperengine-bot:${VERSION:-latest}
    container_name: ryan-bot
    restart: unless-stopped
    entrypoint: []  # Override Docker entrypoint for direct Python execution
    command: ["python", "run.py"]
    env_file:
      - .env.ryan
    environment:
      - DISCORD_BOT_NAME=ryan
      - POSTGRES_HOST=postgres
      - QDRANT_HOST=qdrant
      - MODEL_CACHE_DIR=/app/models
      - DISABLE_MODEL_DOWNLOAD=true
      - HF_HUB_OFFLINE=false
      - TRANSFORMERS_OFFLINE=0
      - HF_HOME=/app/cache/huggingface
      - TRANSFORMERS_CACHE=/app/cache/huggingface
      - HF_DATASETS_CACHE=/app/cache/huggingface
      - HUGGINGFACE_HUB_CACHE=/app/cache/huggingface
      - FASTEMBED_CACHE_PATH=/app/cache/fastembed
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - DEBUG_MODE=false
      - PYTHONUNBUFFERED=1
      - HEALTH_CHECK_PORT=9093
      - HEALTH_CHECK_HOST=0.0.0.0
    ports:
      - "9093:9093"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        tag: "ryan-{.ImageName}-{.Name}-{.ID}"
    deploy:
      resources:
        limits:
          memory: 4G
          cpus: "4.0"
        reservations:
          memory: 2G
          cpus: "2.0"
    healthcheck:
      test:
        - CMD-SHELL
        - curl -f http://localhost:9093/health || exit 1
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    volumes:
      - ryan_backups:/app/backups
      - ryan_privacy:/app/privacy
      - ryan_temp:/app/temp
      - ./logs:/app/logs  # External mount for prompt/response logs
      - ./sql:/app/sql:ro
      # Shared Hugging Face cache (all bots share models)
      - huggingface_cache:/app/cache/huggingface
      # Live code mounting for development (no rebuild needed)
      - ./src:/app/src
      - ./scripts:/app/scripts
      - ./characters:/app/characters
      - ./config:/app/config
      - ./run.py:/app/run.py
      - ./env_manager.py:/app/env_manager.py
      # Note: Using external host mount for logs instead of Docker volumes
    networks:
      - bot_network
    depends_on:
      db-migrate:
        condition: service_completed_successfully
      postgres:
        condition: service_healthy
      qdrant:
        condition: service_started
      influxdb:
        condition: service_healthy
      # - redis  # Commented out - using vector-native memory only

  gabriel-bot:
    image: whisperengine-bot:${VERSION:-latest}
    container_name: gabriel-bot
    restart: unless-stopped
    entrypoint: []  # Override Docker entrypoint for direct Python execution
    command: ["python", "run.py"]
    env_file:
      - .env.gabriel
    environment:
      - DISCORD_BOT_NAME=gabriel
      - POSTGRES_HOST=postgres
      - QDRANT_HOST=qdrant
      - MODEL_CACHE_DIR=/app/models
      - DISABLE_MODEL_DOWNLOAD=true
      - HF_HUB_OFFLINE=false
      - TRANSFORMERS_OFFLINE=0
      - HF_HOME=/app/cache/huggingface
      - TRANSFORMERS_CACHE=/app/cache/huggingface
      - HF_DATASETS_CACHE=/app/cache/huggingface
      - HUGGINGFACE_HUB_CACHE=/app/cache/huggingface
      - FASTEMBED_CACHE_PATH=/app/cache/fastembed
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - DEBUG_MODE=false
      - PYTHONUNBUFFERED=1
      - HEALTH_CHECK_PORT=9095
      - HEALTH_CHECK_HOST=0.0.0.0
    ports:
      - "9095:9095"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        tag: "gabriel-{.ImageName}-{.Name}-{.ID}"
    deploy:
      resources:
        limits:
          memory: 4G
          cpus: "4.0"
        reservations:
          memory: 2G
          cpus: "2.0"
    healthcheck:
      test:
        - CMD-SHELL
        - curl -f http://localhost:9095/health || exit 1
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    volumes:
      - gabriel_backups:/app/backups
      - gabriel_privacy:/app/privacy
      - gabriel_temp:/app/temp
      - ./logs:/app/logs  # External mount for prompt/response logs
      - ./sql:/app/sql:ro
      # Shared Hugging Face cache (all bots share models)
      - huggingface_cache:/app/cache/huggingface
      # Live code mounting for development (no rebuild needed)
      - ./src:/app/src
      - ./scripts:/app/scripts
      - ./characters:/app/characters
      - ./config:/app/config
      - ./run.py:/app/run.py
      - ./env_manager.py:/app/env_manager.py
      # Note: Using external host mount for logs instead of Docker volumes
    networks:
      - bot_network
    depends_on:
      db-migrate:
        condition: service_completed_successfully
      postgres:
        condition: service_healthy
      qdrant:
        condition: service_started
      influxdb:
        condition: service_healthy
      # - redis  # Commented out - using vector-native memory only

  sophia-bot:
    image: whisperengine-bot:${VERSION:-latest}
    container_name: sophia-bot
    restart: unless-stopped
    entrypoint: []  # Override Docker entrypoint for direct Python execution
    command: ["python", "run.py"]
    env_file:
      - .env.sophia
    environment:
      - DISCORD_BOT_NAME=sophia
      - POSTGRES_HOST=postgres
      - QDRANT_HOST=qdrant
      - MODEL_CACHE_DIR=/app/models
      - DISABLE_MODEL_DOWNLOAD=true
      - HF_HUB_OFFLINE=false
      - TRANSFORMERS_OFFLINE=0
      - HF_HOME=/app/cache/huggingface
      - TRANSFORMERS_CACHE=/app/cache/huggingface
      - HF_DATASETS_CACHE=/app/cache/huggingface
      - HUGGINGFACE_HUB_CACHE=/app/cache/huggingface
      - FASTEMBED_CACHE_PATH=/app/cache/fastembed
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - DEBUG_MODE=false
      - PYTHONUNBUFFERED=1
      - HEALTH_CHECK_PORT=9096
      - HEALTH_CHECK_HOST=0.0.0.0
    ports:
      - "9096:9096"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        tag: "sophia-{.ImageName}-{.Name}-{.ID}"
    deploy:
      resources:
        limits:
          memory: 4G
          cpus: "4.0"
        reservations:
          memory: 2G
          cpus: "2.0"
    healthcheck:
      test:
        - CMD-SHELL
        - curl -f http://localhost:9096/health || exit 1
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    volumes:
      - sophia_backups:/app/backups
      - sophia_privacy:/app/privacy
      - sophia_temp:/app/temp
      - ./logs:/app/logs  # External mount for prompt/response logs
      - ./sql:/app/sql:ro
      # Shared Hugging Face cache (all bots share models)
      - huggingface_cache:/app/cache/huggingface
      # Live code mounting for development (no rebuild needed)
      - ./src:/app/src
      - ./scripts:/app/scripts
      - ./characters:/app/characters
      - ./config:/app/config
      - ./run.py:/app/run.py
      - ./env_manager.py:/app/env_manager.py
      # Note: Using external host mount for logs instead of Docker volumes
    networks:
      - bot_network
    depends_on:
      db-migrate:
        condition: service_completed_successfully
      postgres:
        condition: service_healthy
      qdrant:
        condition: service_started
      influxdb:
        condition: service_healthy
      # - redis  # Commented out - using vector-native memory only

  aria-bot:
    image: whisperengine-bot:${VERSION:-latest}
    container_name: aria-bot
    restart: unless-stopped
    entrypoint: []  # Override Docker entrypoint for direct Python execution
    command: ["python", "run.py"]
    env_file:
      - .env.aria
    environment:
      - DISCORD_BOT_NAME=aria
      - POSTGRES_HOST=postgres
      - QDRANT_HOST=qdrant
      - MODEL_CACHE_DIR=/app/models
      - DISABLE_MODEL_DOWNLOAD=true
      - HF_HUB_OFFLINE=false
      - TRANSFORMERS_OFFLINE=0
      - HF_HOME=/app/cache/huggingface
      - TRANSFORMERS_CACHE=/app/cache/huggingface
      - HF_DATASETS_CACHE=/app/cache/huggingface
      - HUGGINGFACE_HUB_CACHE=/app/cache/huggingface
      - FASTEMBED_CACHE_PATH=/app/cache/fastembed
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - DEBUG_MODE=false
      - PYTHONUNBUFFERED=1
      - HEALTH_CHECK_PORT=9383
      - HEALTH_CHECK_HOST=0.0.0.0
    ports:
      - "9383:9383"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        tag: "aria-{.ImageName}-{.Name}-{.ID}"
    deploy:
      resources:
        limits:
          memory: 4G
          cpus: "4.0"
        reservations:
          memory: 2G
          cpus: "2.0"
    healthcheck:
      test:
        - CMD-SHELL
        - curl -f http://localhost:9383/health || exit 1
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    volumes:
      - aria_backups:/app/backups
      - aria_privacy:/app/privacy
      - aria_temp:/app/temp
      - ./logs:/app/logs  # External mount for prompt/response logs
      - ./sql:/app/sql:ro
      # Shared Hugging Face cache (all bots share models)
      - huggingface_cache:/app/cache/huggingface
      # Live code mounting for development (no rebuild needed)
      - ./src:/app/src
      - ./scripts:/app/scripts
      - ./characters:/app/characters
      - ./config:/app/config
      - ./run.py:/app/run.py
      - ./env_manager.py:/app/env_manager.py
      # Note: Using external host mount for logs instead of Docker volumes
    networks:
      - bot_network
    depends_on:
      db-migrate:
        condition: service_completed_successfully
      postgres:
        condition: service_healthy
      qdrant:
        condition: service_started
      influxdb:
        condition: service_healthy
      # - redis  # Commented out - using vector-native memory only

# ===== NETWORKS =====
networks:
  bot_network:
    name: whisperengine-multi_bot_network
    driver: bridge

# ===== VOLUMES =====
volumes:
  postgres_data:
    name: whisperengine-multi_postgres_data
  # redis_data:  # Commented out - using vector-native memory only
  #   name: whisperengine-multi_redis_data
  qdrant_data:
    name: whisperengine-multi_qdrant_data
  influxdb_data:
    name: whisperengine-multi_influxdb_data
    external: true
  influxdb_config:
    name: whisperengine-multi_influxdb_config
    external: true
  grafana_data:
    name: whisperengine-multi_grafana_data
    external: true
  
  # Shared Hugging Face cache (all bots share to avoid duplicate downloads)
  huggingface_cache:
    name: whisperengine-multi_huggingface_cache
  
  # Bot-specific volumes (populated dynamically)
  jake_backups:
    name: whisperengine-multi_jake_backups
  jake_privacy:
    name: whisperengine-multi_jake_privacy
  jake_temp:
    name: whisperengine-multi_jake_temp
  jake_logs:
    name: whisperengine-multi_jake_logs
  marcus_backups:
    name: whisperengine-multi_marcus_backups
  marcus_privacy:
    name: whisperengine-multi_marcus_privacy
  marcus_temp:
    name: whisperengine-multi_marcus_temp
  marcus_logs:
    name: whisperengine-multi_marcus_logs
  dotty_backups:
    name: whisperengine-multi_dotty_backups
  dotty_privacy:
    name: whisperengine-multi_dotty_privacy
  dotty_temp:
    name: whisperengine-multi_dotty_temp
  dotty_logs:
    name: whisperengine-multi_dotty_logs
  dream_backups:
    name: whisperengine-multi_dream_backups
  dream_privacy:
    name: whisperengine-multi_dream_privacy
  dream_temp:
    name: whisperengine-multi_dream_temp
  dream_logs:
    name: whisperengine-multi_dream_logs
  nottaylor_backups:
    name: whisperengine-multi_nottaylor_backups
  nottaylor_privacy:
    name: whisperengine-multi_nottaylor_privacy
  nottaylor_temp:
    name: whisperengine-multi_nottaylor_temp
  nottaylor_logs:
    name: whisperengine-multi_nottaylor_logs
  assistant_backups:
    name: whisperengine-multi_assistant_backups
  assistant_privacy:
    name: whisperengine-multi_assistant_privacy
  assistant_temp:
    name: whisperengine-multi_assistant_temp
  assistant_logs:
    name: whisperengine-multi_assistant_logs
  aetheris_backups:
    name: whisperengine-multi_aetheris_backups
  aetheris_privacy:
    name: whisperengine-multi_aetheris_privacy
  aetheris_temp:
    name: whisperengine-multi_aetheris_temp
  aetheris_logs:
    name: whisperengine-multi_aetheris_logs
  elena_backups:
    name: whisperengine-multi_elena_backups
  elena_privacy:
    name: whisperengine-multi_elena_privacy
  elena_temp:
    name: whisperengine-multi_elena_temp
  elena_logs:
    name: whisperengine-multi_elena_logs
  aethys_backups:
    name: whisperengine-multi_aethys_backups
  aethys_privacy:
    name: whisperengine-multi_aethys_privacy
  aethys_temp:
    name: whisperengine-multi_aethys_temp
  aethys_logs:
    name: whisperengine-multi_aethys_logs
  ryan_backups:
    name: whisperengine-multi_ryan_backups
  ryan_privacy:
    name: whisperengine-multi_ryan_privacy
  ryan_temp:
    name: whisperengine-multi_ryan_temp
  ryan_logs:
    name: whisperengine-multi_ryan_logs
  gabriel_backups:
    name: whisperengine-multi_gabriel_backups
  gabriel_privacy:
    name: whisperengine-multi_gabriel_privacy
  gabriel_temp:
    name: whisperengine-multi_gabriel_temp
  gabriel_logs:
    name: whisperengine-multi_gabriel_logs
  sophia_backups:
    name: whisperengine-multi_sophia_backups
  sophia_privacy:
    name: whisperengine-multi_sophia_privacy
  sophia_temp:
    name: whisperengine-multi_sophia_temp
  sophia_logs:
    name: whisperengine-multi_sophia_logs
  aria_backups:
    name: whisperengine-multi_aria_backups
  aria_privacy:
    name: whisperengine-multi_aria_privacy
  aria_temp:
    name: whisperengine-multi_aria_temp
  aria_logs:
    name: whisperengine-multi_aria_logs