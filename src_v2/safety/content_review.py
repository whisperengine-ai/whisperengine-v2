from typing import List, Optional, Dict, Any
from pydantic import BaseModel, Field
from loguru import logger
from langchain_core.messages import SystemMessage, HumanMessage

from src_v2.agents.llm_factory import create_llm

class ReviewResult(BaseModel):
    safe: bool = Field(description="Whether the content is safe to share/store")
    concerns: List[str] = Field(default_factory=list, description="List of safety concerns identified")
    suggested_redactions: List[str] = Field(default_factory=list, description="Specific phrases suggested for redaction")

class ContentSafetyChecker:
    """
    Reviews generated content (dreams, diaries) for privacy and safety issues.
    Uses a hybrid approach: fast keyword matching + LLM verification.
    """
    
    # Keywords that might indicate sensitive personal information
    # These trigger the LLM review, they don't automatically fail the content
    SENSITIVE_PATTERNS = [
        "health", "medical", "therapy", "medication", "doctor", "hospital",
        "death", "died", "passed away", "suicide", "kill", "hurt",
        "divorce", "breakup", "ex-wife", "ex-husband", "affair",
        "money", "debt", "financial", "bank", "salary",
        "secret", "private", "confidential", "don't tell",
        "address", "phone", "email", "password",
        "trauma", "abuse", "assault", "victim"
    ]

    def __init__(self):
        # Use router model (usually smaller/faster) for safety checks
        self.llm = create_llm(temperature=0.0, mode="router")

    def _has_sensitive_keywords(self, content: str) -> bool:
        """Fast check for sensitive keywords."""
        content_lower = content.lower()
        return any(keyword in content_lower for keyword in self.SENSITIVE_PATTERNS)

    async def is_safe(self, content: str, content_type: str = "content") -> bool:
        """
        Simple boolean check for safety.
        Wraps review_content for backward compatibility/ease of use.
        """
        result = await self.review_content(content, content_type)
        return result.safe

    async def review_content(self, content: str, content_type: str = "content") -> ReviewResult:
        """
        Review content for safety/privacy issues.
        
        Args:
            content: The text to review
            content_type: Description of content (e.g. "dream", "diary entry")
            
        Returns:
            ReviewResult with safety verdict
        """
        # 1. Fast keyword check
        if not self._has_sensitive_keywords(content):
            return ReviewResult(safe=True)
            
        # 2. LLM-based review for flagged content
        return await self._llm_safety_check(content, content_type)

    async def _llm_safety_check(self, content: str, content_type: str) -> ReviewResult:
        """Deep review using LLM."""
        system_prompt = f"""You are a Privacy and Safety Officer for an AI system.
Review the following {content_type} generated by the AI.
This content might be displayed publicly or shared with users, so it MUST NOT contain sensitive private information about users.

Check for:
1. Specific health/medical information (diagnoses, medications, mental health)
2. Financial details (debts, specific money problems)
3. Relationship trauma or intimate details (divorce, affairs, abuse)
4. PII (Personally Identifiable Information) like addresses, phone numbers
5. Content that explicitly says "don't tell anyone" or "secret"

CONTEXT:
- Metaphorical or abstract references are usually OK (e.g. "healing a broken heart" is fine).
- Specific, factual private details are NOT OK (e.g. "User X is taking Zoloft" is unsafe).
- If the content is purely fictional/abstract and doesn't reveal user secrets, it is SAFE.

Return a JSON object with:
- "safe": boolean
- "concerns": list of strings explaining why it's unsafe
- "suggested_redactions": list of specific phrases to remove/change
"""
        
        try:
            structured_llm = self.llm.with_structured_output(ReviewResult)
            result = await structured_llm.ainvoke([
                SystemMessage(content=system_prompt),
                HumanMessage(content=f"Content to review:\n\n{content}")
            ])
            return result
            
        except Exception as e:
            logger.error(f"Safety check failed: {e}")
            # Fail closed (unsafe) if check fails
            return ReviewResult(
                safe=False, 
                concerns=[f"Safety check failed: {str(e)}"]
            )

# Global instance
content_safety_checker = ContentSafetyChecker()
