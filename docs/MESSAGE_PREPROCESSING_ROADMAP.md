# Message Preprocessing & Chunking Enhancement Roadmap

## ðŸŽ¯ Overview

This document outlines the comprehensive enhancement plan for message preprocessing and chunking in WhisperEngine's vector memory system. The goal is to optimize vector quality, improve search precision, and enhance conversational intelligence for AI companions.

## âœ… Quick Fix Implementation (COMPLETED)

### What Was Implemented

1. **Basic Message Chunking**
   - Character length threshold (300+ characters)
   - Sentence count threshold (2+ sentences)
   - Question/exclamation count threshold (multiple punctuation)
   - Simple sentence-based splitting with semantic awareness

2. **Chunking Logic**
   ```python
   def _should_chunk_content(self, content: str) -> bool:
       return (
           len(content) > 300 or          # Long messages
           content.count('.') > 2 or      # Multiple sentences
           content.count('!') > 1 or      # Multiple exclamations
           content.count('?') > 1         # Multiple questions
       )
   ```

3. **Storage Strategy**
   - Primary memory ID returned for first chunk
   - Enhanced metadata tracking chunk relationships
   - Fallback to original storage on chunking failure
   - Preserved all existing vector features (named vectors, emotional analysis, etc.)

### Benefits Achieved
- âœ… Improved vector quality for long messages
- âœ… Better semantic coherence in embeddings
- âœ… Enhanced search precision for multi-topic messages
- âœ… Maintained backward compatibility
- âœ… Zero breaking changes to existing API

## ðŸš€ Future Enhancement Phases

### Phase 1: Advanced Semantic Chunking (1-2 weeks)

#### 1.1 Semantic Coherence Detection
```python
class SemanticChunker:
    """Advanced semantic-aware message chunking"""
    
    def __init__(self):
        self.sentence_embedder = SentenceTransformer('all-MiniLM-L6-v2')
        self.coherence_threshold = 0.7
    
    async def chunk_by_semantic_coherence(self, content: str) -> List[ChunkResult]:
        """Break content into semantically coherent chunks"""
        sentences = self._split_into_sentences(content)
        sentence_embeddings = self.sentence_embedder.encode(sentences)
        
        chunks = []
        current_chunk = [sentences[0]]
        current_embeddings = [sentence_embeddings[0]]
        
        for i in range(1, len(sentences)):
            # Calculate coherence with current chunk
            coherence_score = self._calculate_coherence(
                current_embeddings, 
                sentence_embeddings[i]
            )
            
            if coherence_score > self.coherence_threshold:
                current_chunk.append(sentences[i])
                current_embeddings.append(sentence_embeddings[i])
            else:
                # Start new chunk
                chunks.append(ChunkResult(
                    content=' '.join(current_chunk),
                    coherence_score=np.mean([...]),
                    sentence_count=len(current_chunk)
                ))
                current_chunk = [sentences[i]]
                current_embeddings = [sentence_embeddings[i]]
        
        return chunks
```

#### 1.2 Content Type Classification
```python
class ContentClassifier:
    """Classify message content for specialized handling"""
    
    CONTENT_TYPES = {
        'emotional_expression': ['excited', 'nervous', 'happy', 'sad', 'worried'],
        'factual_information': ['my name', 'i work', 'i am', 'i have', 'i live'],
        'question': ['what', 'how', 'when', 'where', 'why', '?'],
        'technical_discussion': ['algorithm', 'code', 'system', 'API', 'database'],
        'future_planning': ['will', 'going to', 'plan to', 'next week', 'tomorrow'],
        'past_experience': ['yesterday', 'last week', 'remember when', 'used to'],
        'opinion_preference': ['i think', 'i believe', 'prefer', 'like', 'dislike']
    }
    
    def classify_content_type(self, content: str) -> str:
        """Classify the primary type of message content"""
        content_lower = content.lower()
        type_scores = {}
        
        for content_type, keywords in self.CONTENT_TYPES.items():
            score = sum(1 for keyword in keywords if keyword in content_lower)
            if score > 0:
                type_scores[content_type] = score
        
        return max(type_scores, key=type_scores.get) if type_scores else 'general_conversation'
```

#### 1.3 Named Entity Recognition & Normalization
```python
class EntityNormalizer:
    """Normalize entities for consistent vector matching"""
    
    def __init__(self):
        self.company_aliases = {
            'google': 'Google Inc',
            'fb': 'Facebook',
            'meta': 'Meta Platforms',
            'apple': 'Apple Inc'
        }
        
        self.temporal_patterns = {
            r'\bmonday\b': 'next Monday',
            r'\btomorrow\b': 'next day',
            r'\byesterday\b': 'previous day'
        }
    
    def normalize_entities(self, content: str) -> str:
        """Normalize named entities for better matching"""
        normalized = content
        
        # Company normalization
        for alias, canonical in self.company_aliases.items():
            normalized = re.sub(rf'\b{alias}\b', canonical, normalized, flags=re.IGNORECASE)
        
        # Temporal normalization with context
        for pattern, replacement in self.temporal_patterns.items():
            normalized = re.sub(pattern, replacement, normalized, flags=re.IGNORECASE)
        
        return normalized
```

### Phase 2: Advanced Content Preprocessing (2-3 weeks)

#### 2.1 Noise Reduction & Content Cleaning
```python
class ContentPreprocessor:
    """Advanced content cleaning and optimization"""
    
    def __init__(self):
        self.filler_patterns = [
            r'\b(um|uh|like|you know|i mean)\b',
            r'\b(basically|actually|literally)\b',
            r'\b(sort of|kind of)\b'
        ]
        
        self.normalization_rules = {
            'repeated_punctuation': r'[!]{2,}|[?]{2,}|[.]{3,}',
            'excessive_whitespace': r'\s+',
            'emoji_normalization': r'[ðŸ˜€-ðŸ™]{2,}'  # Multiple emoji
        }
    
    def clean_content(self, content: str) -> CleanedContent:
        """Clean and optimize content for vector embedding"""
        original_length = len(content)
        cleaned = content
        
        # Remove filler words
        for pattern in self.filler_patterns:
            cleaned = re.sub(pattern, '', cleaned, flags=re.IGNORECASE)
        
        # Normalize punctuation and whitespace
        for rule_name, pattern in self.normalization_rules.items():
            if rule_name == 'repeated_punctuation':
                cleaned = re.sub(pattern, lambda m: m.group(0)[0], cleaned)
            elif rule_name == 'excessive_whitespace':
                cleaned = re.sub(pattern, ' ', cleaned)
        
        # Remove leading/trailing whitespace
        cleaned = cleaned.strip()
        
        return CleanedContent(
            original=content,
            cleaned=cleaned,
            length_reduction=original_length - len(cleaned),
            modifications_applied=['filler_removal', 'punctuation_normalization']
        )
```

#### 2.2 Pronoun Resolution with Context
```python
class PronounResolver:
    """Resolve pronouns using conversation context"""
    
    def __init__(self, vector_memory_store):
        self.memory_store = vector_memory_store
        self.pronoun_patterns = {
            'it': r'\bit\b',
            'they': r'\bthey\b',
            'he': r'\bhe\b',
            'she': r'\bshe\b',
            'this': r'\bthis\b',
            'that': r'\bthat\b'
        }
    
    async def resolve_pronouns(self, content: str, user_id: str) -> str:
        """Resolve pronouns using recent conversation context"""
        # Get recent context for pronoun resolution
        recent_memories = await self.memory_store.search_memories(
            query=content,
            user_id=user_id,
            top_k=5,
            memory_types=[MemoryType.CONVERSATION]
        )
        
        # Extract potential referents from recent context
        referents = self._extract_referents(recent_memories)
        
        # Resolve pronouns based on context
        resolved = content
        for pronoun, pattern in self.pronoun_patterns.items():
            if re.search(pattern, content, re.IGNORECASE):
                referent = self._find_best_referent(pronoun, referents, content)
                if referent:
                    resolved = re.sub(pattern, referent, resolved, flags=re.IGNORECASE)
        
        return resolved
```

### Phase 3: Intelligent Storage Optimization (3-4 weeks)

#### 3.1 Adaptive Chunking Strategy
```python
class AdaptiveChunker:
    """Adaptive chunking based on content characteristics"""
    
    def __init__(self):
        self.strategies = {
            'technical_discussion': TechnicalChunkingStrategy(),
            'emotional_expression': EmotionalChunkingStrategy(),
            'factual_information': FactualChunkingStrategy(),
            'conversation_flow': ConversationalChunkingStrategy()
        }
    
    async def chunk_adaptively(self, content: str, content_type: str) -> List[AdaptiveChunk]:
        """Choose chunking strategy based on content type"""
        strategy = self.strategies.get(content_type, self.strategies['conversation_flow'])
        
        return await strategy.create_chunks(content)

class TechnicalChunkingStrategy:
    """Specialized chunking for technical discussions"""
    
    async def create_chunks(self, content: str) -> List[AdaptiveChunk]:
        """Chunk technical content by concepts and code blocks"""
        # Preserve code blocks as single chunks
        # Group related technical concepts
        # Maintain API/method references together
        pass

class EmotionalChunkingStrategy:
    """Specialized chunking for emotional expressions"""
    
    async def create_chunks(self, content: str) -> List[AdaptiveChunk]:
        """Chunk emotional content by emotional state transitions"""
        # Group sentences with similar emotional valence
        # Separate emotional transitions
        # Preserve emotional intensity peaks
        pass
```

#### 3.2 Vector Quality Optimization
```python
class VectorQualityOptimizer:
    """Optimize vector embeddings for better search performance"""
    
    def __init__(self):
        self.optimal_chunk_sizes = {
            'technical_discussion': (150, 300),
            'emotional_expression': (100, 200), 
            'factual_information': (50, 150),
            'conversation_flow': (100, 250)
        }
    
    def optimize_chunk_for_embedding(self, chunk: AdaptiveChunk) -> OptimizedChunk:
        """Optimize chunk for embedding model performance"""
        optimal_range = self.optimal_chunk_sizes.get(
            chunk.content_type, 
            (100, 250)
        )
        
        if len(chunk.content) < optimal_range[0]:
            # Expand with context if too short
            return self._expand_chunk_with_context(chunk)
        elif len(chunk.content) > optimal_range[1]:
            # Further split if too long
            return self._split_oversized_chunk(chunk)
        
        return OptimizedChunk(
            content=chunk.content,
            optimization_applied='already_optimal',
            quality_score=self._calculate_quality_score(chunk)
        )
```

### Phase 4: Multi-Language & Advanced Features (4-6 weeks)

#### 4.1 Multi-Language Support
```python
class MultiLanguagePreprocessor:
    """Handle multiple languages in preprocessing"""
    
    def __init__(self):
        self.language_detectors = {
            'en': EnglishPreprocessor(),
            'es': SpanishPreprocessor(),
            'fr': FrenchPreprocessor(),
            'auto': AutoLanguageDetector()
        }
    
    async def preprocess_multilingual(self, content: str) -> MultiLingualResult:
        """Detect language and apply appropriate preprocessing"""
        detected_language = await self._detect_language(content)
        preprocessor = self.language_detectors.get(detected_language, self.language_detectors['en'])
        
        return await preprocessor.preprocess(content)
```

#### 4.2 Contextual Chunk Relationships
```python
class ChunkRelationshipManager:
    """Manage relationships between chunks for better retrieval"""
    
    def __init__(self):
        self.relationship_types = [
            'sequential',      # Chunks from same message in order
            'topical',         # Chunks about same topic
            'emotional',       # Chunks with similar emotional context
            'temporal',        # Chunks from similar time periods
            'conversational'   # Chunks from same conversation thread
        ]
    
    async def establish_chunk_relationships(self, chunks: List[StoredChunk]) -> RelationshipGraph:
        """Create relationship graph between stored chunks"""
        graph = RelationshipGraph()
        
        for chunk in chunks:
            # Find related chunks using various strategies
            related_chunks = await self._find_related_chunks(chunk)
            
            for related_chunk, relationship_type in related_chunks:
                graph.add_relationship(
                    chunk.id, 
                    related_chunk.id, 
                    relationship_type,
                    strength=self._calculate_relationship_strength(chunk, related_chunk)
                )
        
        return graph
```

## ðŸ“Š Performance Metrics & Monitoring

### Key Metrics to Track

1. **Vector Quality Metrics**
   - Average chunk size distribution
   - Semantic coherence scores
   - Search precision improvements
   - Embedding utilization efficiency

2. **Search Performance Metrics**
   - Query response time improvements
   - Relevance score distributions
   - False positive reduction
   - Context retrieval accuracy

3. **User Experience Metrics**
   - Conversation continuity improvements
   - Memory recall accuracy
   - Personality consistency scores
   - Emotional intelligence effectiveness

### Monitoring Implementation
```python
class ChunkingMetricsCollector:
    """Collect and analyze chunking performance metrics"""
    
    def __init__(self):
        self.metrics = {
            'chunks_created': 0,
            'average_chunk_size': 0,
            'coherence_scores': [],
            'search_improvements': [],
            'processing_time': []
        }
    
    async def collect_chunking_metrics(self, original_content: str, chunks: List[Chunk]) -> ChunkingMetrics:
        """Collect metrics for chunking operation"""
        return ChunkingMetrics(
            original_length=len(original_content),
            chunks_created=len(chunks),
            average_chunk_size=np.mean([len(c.content) for c in chunks]),
            coherence_scores=[c.coherence_score for c in chunks],
            processing_time=time.time() - self.start_time
        )
```

## ðŸŽ¯ Implementation Priority

### Immediate (Next Sprint)
1. âœ… Basic chunking implementation (COMPLETED)
2. âœ… Testing and validation (COMPLETED)
3. Monitor chunking effectiveness in production

### Short Term (1-2 sprints)
1. Semantic coherence detection
2. Content type classification  
3. Basic entity normalization

### Medium Term (3-6 sprints)
1. Advanced content preprocessing
2. Pronoun resolution
3. Adaptive chunking strategies

### Long Term (6+ sprints)
1. Multi-language support
2. Chunk relationship management
3. Advanced vector optimization

## ðŸ”§ Configuration Options

### Environment Variables for Chunking Control
```bash
# Basic chunking controls
VECTOR_CHUNKING_ENABLED=true
VECTOR_CHUNK_MAX_LENGTH=300
VECTOR_CHUNK_MIN_LENGTH=50
VECTOR_CHUNK_SENTENCE_THRESHOLD=2

# Advanced features (future)
VECTOR_SEMANTIC_CHUNKING=false
VECTOR_ENTITY_NORMALIZATION=false
VECTOR_PRONOUN_RESOLUTION=false
VECTOR_MULTILANG_SUPPORT=false

# Performance tuning
VECTOR_CHUNKING_PARALLEL_PROCESSING=true
VECTOR_CHUNKING_CACHE_SIZE=1000
VECTOR_CHUNKING_METRICS_ENABLED=true
```

## ðŸ“ API Documentation

### New Methods Added

```python
class VectorMemoryStore:
    def _should_chunk_content(self, content: str) -> bool:
        """Determine if content should be chunked for better vector quality"""
    
    def _create_content_chunks(self, content: str) -> List[str]:
        """Split content into semantic chunks for better vector quality"""
    
    async def _store_memory_with_chunking(self, memory: VectorMemory) -> str:
        """Store long content as multiple optimized chunks"""
    
    async def _store_memory_original(self, memory: VectorMemory) -> str:
        """Original memory storage logic (renamed for chunking integration)"""
```

### Metadata Enhancements

```python
# New metadata fields for chunked memories
chunk_metadata = {
    "original_message_id": "original_memory_id",
    "chunk_index": 0,                    # Position in chunk sequence
    "total_chunks": 3,                   # Total chunks for this message
    "is_chunked": True,                  # Flag for chunked content
    "original_length": 450,              # Original message length
    "chunk_length": 150,                 # This chunk's length
    "chunk_type": "emotional_expression" # Content type classification (future)
}
```

## ðŸŽ‰ Success Criteria

### Phase 1 Success Metrics
- [ ] 90%+ reduction in vector quality issues for long messages
- [ ] 25%+ improvement in search precision for multi-topic queries
- [ ] Zero performance degradation for short messages
- [ ] 100% backward compatibility maintained

### Long-term Success Metrics
- [ ] 50%+ improvement in conversational intelligence scores
- [ ] 40%+ reduction in false positive memory retrievals
- [ ] 30%+ improvement in personality consistency
- [ ] Support for 95% of real-world conversation patterns

---

**Next Steps**: Monitor the quick fix implementation in production and begin Phase 1 development based on observed performance improvements and user feedback.