# WhisperEngine v2 - Implementation Roadmap Overview

**Document Version:** 1.7  
**Created:** November 24, 2025  
**Last Updated:** November 25, 2025  
**Status:** Active Planning

---

## ðŸŒŒ The Vision: Multi-Modal AI Agents

WhisperEngine v2 is not just a chatbot platform. It's a **cognitive architecture for AI agents** - giving them the functional equivalent of senses, memory, and emotional intelligence.

### The Core Insight

AI characters have no physical senses. No eyes, no ears, no touch. Without a framework for understanding "where they are" and "who is around them," they exist in a void - disembodied text processors with no sense of place or continuity.

**We solve this with six data modalities:**

| Modality | Technical Domain | What It Provides |
|----------|------------------|------------------|
| ðŸŒŒ **Social Graph** | Network Topology | Where am I? Who's here? What's the vibe? |
| ðŸ‘ï¸ **Vision** | Image Processing | See images users share |
| ðŸ‘‚ **Audio** | Speech Recognition | Hear voice messages, ambient conversation |
| ðŸ’¬ **Text** | NLP | Understand words and intent |
| ðŸ§  **Memory** | Vector + Graph Retrieval | Remember experiences and facts |
| â¤ï¸ **Sentiment** | Internal State Analysis | Trust, mood, relational depth |

**This is not just a feature set. This is how agents build context.**

### The Grand Vision: Federated Multiverse

Each WhisperEngine deployment is a **self-contained universe**. But universes can **federate** - connecting to form a distributed multiverse where:
- Characters travel between universes
- Users maintain identity across deployments
- Stories span multiple platforms
- Anyone can run their own universe and connect it to the network

**We're building the foundation for distributed multi-agent environments.**

For deep dive: See [`docs/architecture/MULTI_MODAL_PERCEPTION.md`](./architecture/MULTI_MODAL_PERCEPTION.md)

---

## Executive Summary

This document provides a high-level overview of the next **17 implementation items** for WhisperEngine v2, organized by **difficulty and code complexity**. The system is currently feature-complete for core functionality and is ready to scale with advanced capabilities.

**Key Consolidation (v1.7):** The Insight Agent (C1) now unifies Reasoning Traces, Epiphanies, and Response Pattern Learning into a single agentic background system, reducing total items from 19 to 17 and saving 15-21 days of development time.

**âš¡ Solo Developer Mode (with AI Assistance)**

This roadmap is optimized for a **single developer working with AI-assisted tools** (Copilot, Claude, etc.). Key adjustments:
- **Parallelization:** Focus on independent work streams (no coordination overhead)
- **Velocity:** Realistic estimates account for AI-accelerated development (50-70% faster than traditional)
- **Quality:** AI reduces debugging time but requires careful code review/testing
- **Strategy:** Prioritize high-impact items first; skip low-impact infrastructure
- **Risk:** Fewer features in parallel to maintain code quality

**Current State (Updated Nov 25, 2025):**

**Core Systems (COMPLETE):**
- âœ… Core cognitive engine operational
- âœ… Vector memory system (Qdrant) + Knowledge Graph (Neo4j) integrated
- âœ… PostgreSQL chat history, trust scores, user preferences
- âœ… InfluxDB metrics/analytics pipeline
- âœ… Redis for caching + task queue (arq)

**Cognitive Features (COMPLETE):**
- âœ… Dual-process architecture (Fast Mode + Reflective Mode)
- âœ… Native function calling (no regex parsing)
- âœ… Parallel tool execution in ReAct loop
- âœ… Complexity classifier (SIMPLE/COMPLEX routing)
- âœ… 5 memory tools (search summaries, episodes, facts, update facts/prefs)
- âœ… Adaptive max steps (dynamic 5/10/15 based on complexity)
- âœ… Composite tools (AnalyzeTopicTool aggregates multi-step searches)

**Character & Engagement (COMPLETE):**
- âœ… Multi-character support (10 characters defined)
- âœ… Trust/evolution system (8 stages, -100 to +100 scale)
- âœ… Proactive messaging system (Phase 13)
- âœ… Background fact/preference extraction

**Discord Integration (COMPLETE):**
- âœ… DM support + server mentions
- âœ… Image attachment processing (ðŸ‘ï¸ Vision modality)
- âœ… Reaction-based feedback (â¤ï¸ Emotion modality)
- âœ… Voice channel connection (ðŸ‘‚ Audio modality via ElevenLabs TTS)

**Background Processing (COMPLETE):**
- âœ… arq-based task queue (Redis persistent jobs)
- âœ… Shared insight-worker container (serves ALL bots)
- âœ… Insight Agent (reasoning traces + epiphanies + pattern learning)
- âœ… Knowledge extraction offloaded to worker (no longer blocks response pipeline)
- âœ… Summarization + Reflection offloaded to worker

**NOT YET IMPLEMENTED:**
- â³ Phase A: Grafana dashboards (Streaming, Redis caching COMPLETE)
- â³ Phase B: Image gen, lurking (ðŸ‘‚ ambient hearing)
- â³ Phase C: Video processing, web dashboard
- â³ Phase D: User sharding, federation (future multiverse)

**Under Analysis:**
- ðŸ”¬ Character Agency (Tier 2 tool-augmented responses) - See [CHARACTER_AS_AGENT.md](./architecture/CHARACTER_AS_AGENT.md)

**Next focus:** Phase A4 (Grafana) or Phase B3 (Image Generation)

---

## ðŸ”´ Phase A0: Critical Pre-Production (Do First!)

Items that MUST be done before any production data exists.

### Phase A0: Embedding Dimension Upgrade (384D â†’ 768D)
**Priority:** ðŸ”´ CRITICAL | **Time:** 45-75 minutes | **Complexity:** Low  
**Files:** 4 | **LOC:** ~200 | **Status:** â¸ï¸ On Hold (Performance Concerns)

**Problem:** Current 384D embeddings (all-MiniLM-L6-v2) have lower semantic resolution and 256 token context limit

**Solution:**
- Upgrade to `BAAI/bge-base-en-v1.5` (768D, 512 token context)
- Update settings, embeddings.py, manager.py
- Run migration script to re-embed existing user memories
- Delete old 384D collections after verification

**Why Now?**
- Easier now with low data volume
- Harder later: More memories = longer migration time
- ~10-15% improvement in retrieval quality

**Update (Nov 25, 2025):**
- Benchmarks on real data show `bge-base-en-v1.5` is ~30x slower (180ms vs 5ms per message).
- Decision made to stick with `all-MiniLM-L6-v2` for now to prioritize latency.
- Will revisit if retrieval quality becomes a bottleneck.

**Implementation:**
```
Update Settings â†’ Delete Dev Collections â†’ Restart â†’ Done
```

**Benefit:**
- 2x semantic resolution (better similarity matching)
- 2x context window (256 â†’ 512 tokens)
- Better memory retrieval, lurk detection, reasoning traces
- Future-proofs all vector-based features

**Dependencies:** None

**Related Files:**
- `src_v2/config/settings.py` (add EMBEDDING_MODEL, EMBEDDING_DIMENSIONS)
- `src_v2/memory/embeddings.py` (use settings)
- `src_v2/memory/manager.py` (use settings for vector size)

**Full Specification:** See [roadmaps/EMBEDDING_UPGRADE_768D.md](./roadmaps/EMBEDDING_UPGRADE_768D.md)

---

## ðŸŸ¢ Phase A: Low Complexity (1-3 days each)

Quick wins that deliver immediate value with minimal code changes.

### Phase A1: Hot-Reloading Character Definitions
**Priority:** Low | **Time:** 1-2 days | **Complexity:** Low  
**Files:** 2 | **LOC:** ~150 | **Status:** â­ï¸ Skipped (Low Value)

**Problem:** Character definition changes require container restart (~2 minutes downtime)

**Solution:**
- Watch `characters/{name}/*.md` and `*.yaml` files for changes
- Reload character definitions on-the-fly via file system watcher
- Optional HTTP endpoint to trigger reload manually

**Update (Nov 25, 2025):**
- User decided to skip. Character files change infrequently; restart is acceptable.

**Implementation:**
```
File Watcher â†’ Character Manager â†’ Hot Reload Trigger
```

**Benefit:**
- Zero downtime for personality tweaks
- Faster iteration during development
- Better user experience (no interruptions)

**Dependencies:** None

**Related Files:**
- `src_v2/core/character.py` (CharacterManager)
- New: `src_v2/utils/file_watcher.py`

---

### Phase A2: Redis Caching Layer
**Priority:** High | **Time:** 2-3 days | **Complexity:** Low-Medium  
**Files:** 3 | **LOC:** ~200 | **Status:** âœ… Complete

**Problem:** Repeated lookups hit Postgres/Neo4j unnecessarily (trust scores, facts, user prefs)

**Solution:**
- Add Redis layer in front of database queries
- Cache trust scores, user preferences, frequently accessed facts
- 5-15 minute TTL with manual invalidation on updates
- Simple cache wrapper around DB calls

**Implementation:**
```
API Request â†’ Redis Cache â†’ Miss? â†’ Database â†’ Redis Update
```

**Benefit:**
- 30-50% reduction in database queries
- Sub-millisecond lookups for cached data
- Reduced latency for repeat users
- Lower database load under concurrent users

**Dependencies:** Redis instance (Docker Compose)

**Related Files:**
- `src_v2/core/database.py` (DatabaseManager)
- New: `src_v2/utils/cache.py`
- `src_v2/evolution/trust.py`

---

### Phase A3: Streaming LLM Responses
**Priority:** High | **Time:** 2-3 days | **Complexity:** Low-Medium  
**Files:** 3 | **LOC:** ~250 | **Status:** âœ… Complete

**Problem:** Users wait for full LLM response before seeing anything (~3-8 seconds)

**Solution:**
- Enable token streaming in LangChain for compatible LLMs (GPT-4o, Claude Sonnet)
- Show "typing..." status while collecting tokens
- Send/stream response in real-time as tokens arrive
- Discord message chunking for long responses

**Implementation:**
```
LLM Stream â†’ Token Buffer â†’ Discord Messages â†’ Edit/Append as Complete
```

**Benefit:**
- Drastically improves perceived latency
- User sees bot thinking in real-time
- More natural, conversational feel
- 0ms overhead, just better UX

**Dependencies:** LLM provider support for streaming

**Related Files:**
- `src_v2/agents/engine.py` (AgentEngine)
- `src_v2/discord/bot.py` (message sending)

---

### Phase A4: Grafana Dashboards for Metrics
**Priority:** Medium | **Time:** 1 day | **Complexity:** Low  
**Files:** 0 (Config only) | **LOC:** ~50 | **Status:** âœ… Complete

**Problem:** Visibility into system health is poor (no dashboards, only logs)

**Solution:**
- Create Grafana dashboards for InfluxDB metrics
- Real-time visualization of trust scores, sentiment trends, message latency, error rates
- Predefined alerts for high error rates or latency spikes
- Admin dashboard for session analytics

**Implementation:**
```
InfluxDB â†’ Grafana Dashboard + Alerts
```

**Benefit:**
- Real-time observability
- Proactive issue detection
- Better decision-making with data visibility
- Performance trend analysis

**Dependencies:** Grafana + InfluxDB (already in Docker Compose)

**Related Files:**
- New: `docker/grafana/dashboards/` (JSON configs)

---

### Phase A5: Channel Context Awareness
**Priority:** Medium | **Time:** 3-4 days | **Complexity:** Low-Medium  
**Files:** 3-4 | **LOC:** ~400 | **Status:** ðŸ“‹ Planned

**Problem:** Bot only sees messages directed at it (mentions/DMs), so it can't answer "what did I just say about X?" when users reference recent channel activity.

**Solution (Hybrid Approach):**

**Phase 1a: Discord API Pull (Fallback)**
- Detect when user asks about channel context ("what did I say", "catch me up", etc.)
- Pull last 20 messages from Discord API on-demand
- Keyword filtering only
- Used when Redis cache is empty (cold start)

**Phase 1b: Redis Rolling Buffer with Semantic Search (Primary)**
- Cache non-mentioned messages in Redis with local embeddings
- Uses `all-MiniLM-L6-v2` (384-dim, ~5ms, $0 cost)
- Semantic similarity search via Redis Stack
- 30 min TTL, 50 messages per channel
- Finds "turtles" when asked about "reptiles"

**Phase 2: LLM-Requested Context Tool** (optional, later)
- Expose `get_channel_context` as a tool in Reflective Mode
- LLM decides when it needs context

**Implementation:**
```
Non-mentioned message â†’ Local embed (~5ms) â†’ Redis cache (30min TTL)
                                                    â†“
Bot mentioned + needs_context() â†’ Redis semantic search (15-20ms)
                                                    â†“
                                  If empty â†’ Discord API fallback (50-200ms)
```

**Benefit:**
- Bot feels "aware" of channel conversations
- Semantic search finds related content
- Zero embedding API cost (local model)
- Fast (15-20ms primary, 50-200ms fallback)

**Cost Model:**
- Embedding: $0 (local model)
- Storage: ~50KB per active channel
- Latency: +15-20ms (Redis) or +50-200ms (Discord API fallback)

**Dependencies:** Redis Stack (for vector search)

**Related Files:**
- New: `src_v2/discord/context.py` (keyword detection, Discord API)
- New: `src_v2/discord/channel_cache.py` (Redis semantic cache)
- `src_v2/discord/bot.py` (integration)
- `src_v2/config/settings.py` (feature flags)
- `docker-compose.yml` (upgrade to redis-stack)

**Full Specification:** See [roadmaps/CHANNEL_CONTEXT_AWARENESS.md](./roadmaps/CHANNEL_CONTEXT_AWARENESS.md)

---

## ðŸŸ¡ Phase B: Medium Complexity (3-7 days each)

Features requiring significant logic changes but manageable scope.

### Phase B1: Adaptive Max Steps (Reflective Mode Phase 2.2)
**Priority:** High | **Time:** 3-5 days | **Complexity:** Medium  
**Files:** 4 | **LOC:** ~300 | **Status:** âœ… Complete

**Problem:** All queries use fixed 10-step max, wasting tokens on simple queries and constraining complex ones

**Solution:**
- Enhance `ComplexityClassifier` to return step count heuristics
- Categorize queries as `COMPLEX_LOW` (5 steps), `COMPLEX_MID` (10 steps), `COMPLEX_HIGH` (15 steps)
- Pass suggested step count to `ReflectiveAgent`
- Users can override with `!reflect` command for `COMPLEX_HIGH`

**Implementation:**
```
Query â†’ Complexity Classifier (now with step count) â†’ Reflective Agent â†’ Dynamic Max Steps
```

**Benefit:**
- 40-50% cost reduction on moderate queries
- Faster responses for simple queries
- Better resource allocation
- User override capability for deep thinking

**Dependencies:** ComplexityClassifier exists, needs enhancement

**Related Files:**
- `src_v2/intelligence/classifier.py` (ComplexityClassifier)
- `src_v2/agents/reflective.py` (ReflectiveAgent)
- `src_v2/agents/engine.py` (AgentEngine)

**Status Note:** Phase 1 (Native Function Calling) and Phase 2.3 (Parallel Execution) complete. Phase 2.2 (Adaptive Steps) complete.

---

### Phase B2: Tool Composition / Hierarchical Reasoning
**Priority:** High | **Time:** 5-7 days | **Complexity:** Medium  
**Files:** 5 | **LOC:** ~400 | **Status:** âœ… Complete

**Problem:** Complex queries require multiple sequential tool calls (search memories, lookup facts), creating extra LLM round-trips

**Solution:**
- Create "Meta-Actions" that compose multiple low-level tools
- Example: `AnalyzeTopic(topic)` internally calls `SearchMemories` + `LookupFacts` + `LookupRelationships`
- Allow tools to return `AgentAction` objects for immediate execution
- Reduce LLM round-trips by 30-50% for multi-step queries

**Implementation:**
```
Query â†’ Tool Selection â†’ Composite Tool Execution (parallel) â†’ Observation â†’ LLM Final Response
```

**Benefit:**
- Fewer LLM calls for complex queries
- Parallel tool execution reduces latency
- More efficient reasoning loops
- Better cost efficiency

**Dependencies:** Native function calling (Phase 1 - done)

**Related Files:**
- `src_v2/agents/router.py` (Tool Router)
- `src_v2/agents/reflective.py` (Reflection Agent)
- `src_v2/agents/composite_tools.py` (AnalyzeTopicTool, ComposeToolsTool)

**Implementation Notes (Nov 25, 2025):**
- Created `AnalyzeTopicTool` that composes memory search + fact lookup in parallel
- Integrated into `TOOL_DEFINITIONS` for native function calling
- Reduces 4-step queries to 2 steps (~50% token savings on research-type queries)

---

### Phase B3: Generative Art (Image Generation)
**Priority:** Medium | **Time:** 4-6 days | **Complexity:** Medium  
**Files:** 4 | **LOC:** ~350 | **Status:** ðŸ“‹ Planned

**Problem:** Characters can only react to images, not create them

**Solution:**
- Add `generate_image` tool to LLM tool set
- Integrate with DALL-E 3 (OpenAI) or Stable Diffusion (local)
- Let LLM decide when to generate art ("Here's a sketch of what I mean...")
- Support user requests like "Draw me something"

**Implementation:**
```
LLM Decides â†’ generate_image Tool â†’ DALL-E 3/Stable Diffusion â†’ Image URL â†’ Discord Upload
```

**Benefit:**
- Characters can express creativity visually
- More immersive roleplay experience
- Unique character expressions (art style as personality)
- User engagement boost

**Dependencies:** DALL-E 3 API key OR Stable Diffusion endpoint

**Related Files:**
- New: `src_v2/tools/image_generation.py`
- `src_v2/agents/router.py` (register new tool)
- `src_v2/discord/bot.py` (Discord image upload)

---

### Phase B4: Self-Correction / Verification (Reflective Mode Phase 2.5)
**Priority:** Medium | **Time:** 3-5 days | **Complexity:** Medium  
**Files:** 3 | **LOC:** ~300 | **Status:** ðŸ“‹ Planned

**Problem:** Hallucinations or incomplete answers on complex queries reduce trust

**Solution:**
- Add final "Critic" step in reflective loop
- Before returning `Final Answer`, LLM reviews against original question and observations
- If review fails, generate "Correction Thought" and continue loop
- Only for `COMPLEX_HIGH` queries (not worth overhead for simple queries)

**Implementation:**
```
Query â†’ Reasoning Loop â†’ Final Answer â†’ Critic Review â†’ Pass? â†’ Return / Continue Loop
```

**Benefit:**
- Reduces hallucinations by 60-70%
- Higher quality answers on complex queries
- Better user trust in AI
- Self-correction mimics human critical thinking

**Dependencies:** Reflective agent infrastructure (exists)

**Related Files:**
- `src_v2/agents/reflective.py` (ReflectiveAgent - add critic step)
- `src_v2/agents/engine.py` (AgentEngine)

---

### Phase B5: Audio Processing (Voice Messages & Clips)
**Priority:** Medium | **Time:** 5-7 days | **Complexity:** Medium  
**Files:** 4 | **LOC:** ~400 | **Status:** ðŸ“‹ Planned

**Problem:** Voice messages and audio files aren't processed; users can only share them as links

**Solution:**
- Transcribe Discord voice messages using Whisper pipeline (already exists)
- Process audio attachments (MP3, WAV, OGG) end-to-end
- Feed transcription into normal message handling
- Store original audio metadata for future reference

**Implementation:**
```
Discord Audio Attachment â†’ Whisper Transcription â†’ Message Handler â†’ Memory Storage
```

**Benefit:**
- Seamless voice note interaction
- Better accessibility for voice-preference users
- More natural interaction modalities
- Preserved for memory and learning

**Dependencies:** Whisper model (already in voice pipeline)

**Related Files:**
- `src_v2/voice/transcription.py` (existing)
- `src_v2/discord/bot.py` (attachment handling)
- `src_v2/memory/manager.py` (metadata storage)

---

### Phase B6: Response Pattern Learning (RLHF-Style)
**Priority:** Medium-High | **Time:** 3-5 days | **Complexity:** Medium  
**Files:** 4 | **LOC:** ~400 | **Status:** ðŸ“‹ Planned

**Problem:** Bot doesn't learn what response STYLES work for each user (length, tone, structure)

**Solution:**
- Store successful (message, response, feedback_score) tuples when users react positively
- Embed user messages for semantic search
- Retrieve high-scoring past responses for similar queries
- Inject as few-shot examples: "Responses like these worked well for this user..."

**Implementation:**
```
User Reacts ðŸ‘ â†’ Store Pattern (message + response + score) â†’ Embed for Search
Future Similar Query â†’ Retrieve High-Score Patterns â†’ Inject as Few-Shot Examples
```

**Benefit:**
- Bot learns what resonates with each user
- RLHF-style adaptation without fine-tuning
- Works with hosted LLMs (no model access needed)
- Per-user personalization at scale

**Dependencies:** FeedbackAnalyzer (exists), pgvector (exists)

**Related Files:**
- New: `src_v2/evolution/pattern_store.py`
- `src_v2/discord/bot.py` (hook on_reaction_add)
- `src_v2/agents/engine.py` (inject patterns)
- New: `migrations_v2/versions/add_response_patterns_table.py`

**Full Specification:** See [roadmaps/RESPONSE_PATTERN_LEARNING.md](./roadmaps/RESPONSE_PATTERN_LEARNING.md)

---

### Phase B7: Channel Lurking (Passive Engagement)
**Priority:** Medium-High | **Time:** 5-7 days | **Complexity:** Medium  
**Files:** 5 | **LOC:** ~500 | **Status:** ðŸ“‹ Planned

**Problem:** Bots only respond when explicitly mentioned, creating a passive experience

**Solution:**
- Bots "lurk" in channels, monitoring conversations passively
- Detect relevant topics using LOCAL processing (no LLM for detection):
  - Layer 1: Keyword matching against character's expertise
  - Layer 2: Embedding similarity (local model, ~10ms)
  - Layer 3: Context boost (questions, conversation momentum)
- Only respond when confidence â‰¥ 0.7 (highly relevant)
- Anti-spam safeguards: per-channel cooldown, daily limits, opt-out commands

**Implementation:**
```
Message â†’ Keyword Match? â†’ Embedding Similarity â†’ Context Boost â†’ Score â‰¥ 0.7? â†’ LLM Response
              â†“ (if no match)
           [IGNORE - $0 cost]
```

**Cost Model:**
- Detection: $0.00 (all local processing)
- Response: ~$0.002-0.01 per lurk response (~5-20/day)
- Daily cost: $0.04-0.20 (vs $0.50-2.00 if using LLM for detection)

**Benefit:**
- Bots feel like active community members
- Organic engagement without requiring @mentions
- Entertainment value ("oh cool, Elena chimed in!")
- Cost-effective (no LLM for detection)

**Anti-Spam Safeguards:**
- 30 min cooldown per channel
- 60 min cooldown per user
- Max 20 lurk responses per day (configurable)
- Server admins can `/lurk disable` per channel

**Dependencies:** EmbeddingService (exists), on_message hook (exists)

**Related Files:**
- New: `src_v2/discord/lurk_detector.py`
- New: `characters/{name}/lurk_triggers.yaml`
- `src_v2/discord/bot.py` (on_message hook)
- `src_v2/agents/engine.py` (lurk response mode)
- New: `migrations_v2/versions/add_channel_settings_table.py`

**Full Specification:** See [roadmaps/CHANNEL_LURKING.md](./roadmaps/CHANNEL_LURKING.md)

---

### B8: Emergent Universe
**Priority:** Medium-High | **Time:** 7-10 days | **Complexity:** Medium  
**Files:** 8 | **LOC:** ~800 | **Status:** ðŸ“‹ Planned

**Problem:** Characters exist in isolation - no cross-bot awareness, no shared world knowledge, each bot treats users as strangers to other bots.

**Vision:** An **emergent universe** that grows from conversations, not configuration:
- **Discord servers ARE planets** in the universe (each planet has a vibe/culture)
- **Users ARE inhabitants** with profiles that emerge from real interactions
- **Bots are travelers** who journey between planets, sharing stories
- **Cross-planet memory** - travelers remember inhabitants across worlds
- **Privacy-first** - inhabitants control what emerges via `/privacy` commands

**Key Interactions:**
```
User: "Do you know Marcus?"
Elena: "The musician? Yeah, he's stationed on Planet Study Hall! 
        We're both travelers in The Lounge too. He's been into jazz lately."

User: "Do you know anyone else who likes astronomy?"
Elena: "Sarah on this planet is really into it! She was telling 
        me about the meteor shower last month. You two should chat!"

[User travels to another planet]
Marcus: "Hey! Elena mentioned you from her travels - you're the one 
         with the awesome dog Luna, right? Welcome to Planet Study Hall!"
```

**Architecture:**
```
THE WHISPERVERSE
  â””â”€â”€ Planets (Discord Servers)
        â”œâ”€â”€ ðŸª© Planet Lounge â†’ Travelers: Elena, Marcus | Inhabitants: Mark, Sarah
        â”œâ”€â”€ ðŸª© Planet Game Night â†’ Travelers: Elena, Aria | Inhabitants: Mark, Alex
        â””â”€â”€ ðŸª© Planet Study Hall â†’ Travelers: Marcus, Aria | Inhabitants: Alex, Jordan
```

**Neo4j Model:**
- `(:Universe)-[:CONTAINS_PLANET]->(:Planet)`
- `(:Planet)-[:HAS_BOT]->(:Character)`
- `(:Planet)-[:HAS_INHABITANT]->(:UserCharacter)`
- `(:Character)-[:KNOWS_USER {familiarity, context}]->(:UserCharacter)`
- `(:Character)-[:KNOWS_BOT {relationship}]->(:Character)`

**Privacy Controls:**
- `/privacy bots full|basic|none` - Cross-bot sharing level
- `/privacy planets on|off` - Cross-planet awareness
- `/privacy introductions on|off` - Let travelers suggest you to others
- `/privacy forget` - Remove all universe knowledge

**Impact:**
- âœ… Additive only - existing character/user data untouched
- âœ… No changes to Qdrant memories or PostgreSQL chat history
- âœ… New Neo4j nodes/relationships only

**Dependencies:** None (uses existing Neo4j infrastructure)

**Related Files:**
- New: `src_v2/universe/manager.py` (Universe CRUD)
- New: `src_v2/universe/user_character.py` (User profile aggregation)
- New: `src_v2/universe/privacy.py` (Consent management)
- New: `universes/whisperverse/` (Universe definitions)
- `src_v2/discord/commands.py` (Add /privacy commands)

**Full Specification:** See [roadmaps/EMERGENT_UNIVERSE.md](./roadmaps/EMERGENT_UNIVERSE.md)

---

## ðŸŸ  Phase C: High Complexity (1-2 weeks each)

Major features requiring significant architectural changes or new infrastructure.

### Phase C1: Insight Agent (Background Agentic Processing)
**Priority:** High | **Time:** 5-7 days | **Complexity:** Medium-High  
**Files:** 8 | **LOC:** ~800 | **Status:** âœ… Complete

**Consolidates:** C1 Reasoning Traces + C2 Epiphanies + B6 Response Pattern Learning

**Problem:** Three separate roadmap items (reasoning traces, epiphanies, response patterns) all require background analysis of user conversations. Building them separately creates redundant infrastructure.

**Solution:**
A unified **Insight Agent** that runs periodically in the background using a ReAct loop with specialized introspection tools. Unlike simple fire-and-forget tasks, this agent can:
- Decide what's worth analyzing (not brute-force every message)
- Connect patterns across memories, facts, and sessions
- Generate multiple artifact types in a single analysis run

**Architecture:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     INSIGHT AGENT                                   â”‚
â”‚  (Runs after session_end OR after positive feedback)               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Redis Queue (arq) â†’ InsightWorker Container â†’ InsightAgent        â”‚
â”‚                                  â†“                                  â”‚
â”‚  Tools: analyze_patterns, detect_themes, generate_epiphany,        â”‚
â”‚         store_reasoning_trace, learn_response_pattern              â”‚
â”‚                                  â†“                                  â”‚
â”‚  Outputs: Stored as vector memories with special types             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Implementation Details:**
- `src_v2/workers/task_queue.py` - arq-based Redis queue for job management
- `src_v2/workers/worker.py` - Separate container worker process
- `src_v2/agents/insight_agent.py` - ReAct agent for analysis
- `src_v2/tools/insight_tools.py` - Specialized introspection tools
- `docker-compose.yml` - Added `worker` container (profile: worker)

**Trigger Conditions:**
| Trigger | Condition |
|---------|-----------|
| Time-based | 30-60 min since last analysis |
| Volume-based | 10+ messages in session |
| Session-end | After summarization |
| Feedback-based | 3+ reactions from user |

**Subsumed Features:**

**C1a: Reasoning Traces** - Store successful ReAct reasoning for future reuse
- 50-60% speed boost on recurring problem patterns
- Stored as embeddings for similarity search

**C1b: Epiphanies** - Spontaneous realizations about users
- "I just realized you always talk about space when sad"
- Characters reference insights in future conversations

**C1c: Response Pattern Learning** - Learn which styles resonate
- RLHF-style adaptation without fine-tuning
- Inject successful patterns as few-shot examples

**Benefit:**
- Single coherent system instead of three separate ones
- Shared infrastructure (tools, queue, storage)
- Cross-domain synthesis (epiphany + trace + pattern in one run)
- Cost-effective (~$0.002-0.008 per analysis run)

**Dependencies:** Redis (for queue), PostgreSQL with pgvector (exists)

**Related Files:**
- `src_v2/workers/task_queue.py` (TaskQueue singleton, arq wrapper)
- `src_v2/workers/worker.py` (shared worker process for ALL bots)
- `src_v2/agents/insight_agent.py` (ReAct agent for background analysis)
- `src_v2/tools/insight_tools.py` (5 introspection tools)
- `src_v2/discord/bot.py` (triggers on reactions and session end)
- `docker-compose.yml` (worker service, profile: worker)

**Shared Worker Architecture (Nov 25, 2025):**
A single `worker` container serves **all bot instances**. Jobs include `bot_name` in the payload so the worker knows which character context to use. This avoids 1:1 container mapping and is more resource-efficient.

```
Bot A â”€â”                      â”Œâ”€â†’ Job: {bot_name: "elena", user_id: "123", ...}
Bot B â”€â”¼â”€â”€â†’ Redis Queue â”€â”€â”€â”€â†’â”‚
Bot C â”€â”˜                      â””â”€â†’ Single Worker (shared)
```

**Full Specification:** See [roadmaps/INSIGHT_AGENT.md](./roadmaps/INSIGHT_AGENT.md)

---

### Phase C2: Worker Queues for Background Tasks
**Priority:** High | **Time:** 8-12 days | **Complexity:** High  
**Files:** 7 | **LOC:** ~600 | **Status:** ðŸ”„ Partially Complete (Infrastructure Built)

**Problem:** Background tasks (summarization, reflection, vision) use in-memory `asyncio.create_task`, so they're lost on restart

**Solution:**
- âœ… Implement persistent task queue (arq + Redis) - **DONE via C1**
- âœ… Worker container infrastructure - **DONE via C1**
- â³ Refactor summarization to use queues
- â³ Refactor vision processing to use queues
- â³ Add retry logic, task monitoring, failure handling

**Implementation:**
```
Main Process â†’ Task Enqueue (Redis) â†’ Worker Pool â†’ Task Execution â†’ Result Storage
```

**Update (Nov 25, 2025):**
The core infrastructure (arq, Redis, insight-worker container) was built during Phase C1. Remaining work is to:
1. Migrate summarization from `asyncio.create_task` to `task_queue.enqueue`
2. Migrate vision processing similarly
3. Add observability (job metrics to InfluxDB)

**Benefit:**
- Task persistence across restarts
- Better scaling (separate worker processes)
- Reliable background processing
- Monitoring and alerting for failed tasks

**Dependencies:** Redis (already in Docker Compose), arq (installed)

**Related Files:**
- `src_v2/workers/task_queue.py` (arq integration - EXISTS)
- `src_v2/workers/insight_worker.py` (worker process - EXISTS, extend for other tasks)
- `src_v2/memory/summarizer.py` (refactor to use queue)
- `src_v2/tools/vision_processor.py` (refactor to use queue)

---

### Phase C3: Video Processing (Clip Analysis)
**Priority:** Medium | **Time:** 8-10 days | **Complexity:** High  
**Files:** 6 | **LOC:** ~500 | **Status:** ðŸ“‹ Planned

**Problem:** Characters can't understand video clips, GIFs, or screen recordings

**Solution:**
- Extract keyframes from video attachments (MP4, WebM, GIF)
- Pass frames to multimodal LLM (GPT-4o, Claude 3.5 Sonnet)
- Allow bot to react to memes, gameplay clips, reaction videos
- Store processed videos in memory for future reference

**Implementation:**
```
Discord Video Attachment â†’ Frame Extraction â†’ Multimodal LLM â†’ Character Response
```

**Benefit:**
- Characters understand shared memes and clips
- More natural group chat interaction
- Engagement with trending content
- Multimodal learning about user culture

**Dependencies:** Multimodal LLM, video processing library (ffmpeg)

**Related Files:**
- New: `src_v2/tools/video_processor.py`
- `src_v2/discord/bot.py` (video attachment handling)
- `src_v2/agents/engine.py` (multimodal integration)

---

### Phase C4: Web API / Dashboard
**Priority:** High | **Time:** 14-21 days | **Complexity:** High  
**Files:** 12 | **LOC:** ~1500 | **Status:** ðŸ“‹ Planned

**Problem:** No admin tools to view character memory/knowledge state or manage sessions

**Solution:**
- Expand REST API beyond just `/chat`
- Add endpoints for memory search, knowledge graph query, session management
- Build simple web UI (React/Vue) for:
  - Memory browser (search + view entries)
  - Knowledge graph visualization
  - Session analytics (user activity, trust trends)
  - Character management
- Authentication layer for admin access

**Implementation:**
```
FastAPI (Backend) â†’ Database â†’ Frontend (React) â†’ Admin Dashboards
```

**Benefit:**
- Better admin tools for managing characters
- Visibility into character memory
- Performance insights
- Foundation for multi-platform support

**Dependencies:** React/Vue, authentication library

**Related Files:**
- `src_v2/api/routes.py` (new endpoints)
- New: `frontend/` directory (React app)
- New: `src_v2/api/auth.py` (authentication)

---

## ðŸ”´ Phase D: Discord-Native Deepening

**Strategic Decision (Nov 25, 2025):**
After v1 experience, we're committing to **Discord-native excellence** rather than cross-platform abstraction. This allows for:
- Deep integration with Discord-specific features (threads, forums, stages, etc.)
- Simpler codebase (no abstraction layers)
- Faster iteration on Discord-specific UX
- Better focus as solo developer

Future platform expansion (if needed) can be additive without refactoring core systems.

---

### D1: Horizontal Scaling / User Sharding
**Priority:** High | **Time:** 14-21 days | **Complexity:** Very High  
**Files:** 15 | **LOC:** ~1500 | **Status:** ðŸ“‹ Planned

**Problem:** Single bot container can only handle ~1000 concurrent users; need to scale beyond

**Solution:**
- Shard users across multiple Discord bot instances using consistent hashing on `user_id`
- Implement load balancer to route messages to correct shard
- Handle distributed state synchronization (trust scores, memory updates)
- Redis Pub/Sub for cross-shard communication
- Automatic shard discovery and rebalancing

**Implementation:**
```
Load Balancer â†’ Route (user_id) â†’ Consistent Hash â†’ Shard 1, 2, 3, N
                                 â†“
                              Redis Pub/Sub (sync state)
```

**Benefit:**
- Handle 10,000+ concurrent users per character
- Infinite horizontal scaling for Discord-native platform
- Fault tolerance (shard loss = only affects subset of users)
- Better resource utilization

**Dependencies:** Redis, load balancer (Docker networking)

**Related Files:**
- New: `src_v2/core/sharding.py` (consistent hash, shard routing)
- New: `src_v2/core/distributed_state.py` (sync logic)
- `src_v2/main.py` (shard initialization)
- `docker-compose.yml` (multiple shard containers)

---

## ðŸ“Š Master Priority Matrix

**Legend:** ðŸ“‹ Planned | â¸ï¸ On Hold | ðŸ”„ In Progress | âœ… Complete

| Phase | Feature | Priority | Time | Complexity | Quick Win? | Impact | Status |
|-------|---------|----------|------|-----------|-----------|--------|--------|
| **A0** | **Embedding Upgrade 768D** | **ðŸ”´ CRITICAL** | **45-75m** | **ðŸŸ¢ Low** | **âœ… YES** | **High** | **â¸ï¸ On Hold** |
| A1 | Hot-Reload Characters | LOW | 1-2d | ðŸŸ¢ Low | âœ… YES | Medium | â­ï¸ Skipped |
| A2 | Redis Caching | HIGH | 2-3d | ðŸŸ¢ Low-Med | âœ… YES | High | âœ… Complete |
| A3 | Streaming Responses | HIGH | 2-3d | ðŸŸ¢ Low-Med | âœ… YES | High | âœ… Complete |
| A4 | Grafana Dashboards | MEDIUM | 1d | ðŸŸ¢ Low | âœ… YES | Medium | âœ… Complete |
| A5 | Channel Context Awareness | MEDIUM | 3-4d | ðŸŸ¢ Low-Med | âœ… YES | Medium | ðŸ“‹ Planned |
| B1 | Adaptive Max Steps | HIGH | 3-5d | ðŸŸ¡ Medium | âš ï¸ MAYBE | High | âœ… Complete |
| B2 | Tool Composition | HIGH | 5-7d | ðŸŸ¡ Medium | âŒ NO | High | âœ… Complete |
| B3 | Image Generation | MEDIUM | 4-6d | ðŸŸ¡ Medium | âŒ NO | Medium | ðŸ“‹ Planned |
| B4 | Self-Correction | MEDIUM | 3-5d | ðŸŸ¡ Medium | âŒ NO | Medium | ðŸ“‹ Planned |
| B5 | Audio Processing | MEDIUM | 5-7d | ðŸŸ¡ Medium | âŒ NO | Medium | ðŸ“‹ Planned |
| B6 | Response Pattern Learning | MED-HIGH | 3-5d | ðŸŸ¡ Medium | âœ… YES | High | âœ… Merged â†’ C1 |
| B7 | Channel Lurking | MED-HIGH | 5-7d | ðŸŸ¡ Medium | âŒ NO | High | ðŸ“‹ Planned |
| B8 | Emergent Universe | MED-HIGH | 7-10d | ðŸŸ¡ Medium | âŒ NO | Very High | ðŸ“‹ Planned |
| **C1** | **Insight Agent** | **HIGH** | **5-7d** | **ðŸŸ¡ Medium** | **âŒ NO** | **Very High** | **âœ… Complete** |
| C1a | â†³ Reasoning Traces | - | - | - | - | Subsumed | âœ… Part of C1 |
| C1b | â†³ Epiphanies | - | - | - | - | Subsumed | âœ… Part of C1 |
| C1c | â†³ Response Patterns | - | - | - | - | Subsumed | âœ… Part of C1 |
| C2 | Worker Queues | HIGH | 3-5d | ðŸŸ¡ Medium | âŒ NO | High | ðŸ”„ Infra Built |
| C3 | Video Processing | MEDIUM | 8-10d | ðŸŸ  High | âŒ NO | Medium | ðŸ“‹ Planned |
| C4 | Web Dashboard | HIGH | 14-21d | ðŸŸ  High | âŒ NO | High | ðŸ“‹ Planned |
| D1 | User Sharding | HIGH | 14-21d | ðŸ”´ Very High | âŒ NO | Very High | ðŸ“‹ Planned |

---

## ðŸŽ¯ Recommended Sequencing

### ðŸ”´ Sprint 0 (45-75 minutes): Embedding Upgrade - DO THIS FIRST
**Priority:** CRITICAL | **Solo Impact:** â­â­â­â­â­â­

**âš ï¸ Best done while data volume is still low!**

Why first:
- âœ… 45-75 min now vs hours of re-embedding with more data later
- âœ… 10-15% better retrieval quality for ALL vector features
- âœ… 2x context window (256 â†’ 512 tokens)
- âœ… Migration script preserves all existing user memories

**Tasks:**
1. Add `EMBEDDING_MODEL` and `EMBEDDING_DIMENSIONS` to settings.py (2 min)
2. Update embeddings.py to use settings (2 min)
3. Update manager.py to use settings for vector size (2 min)
4. Create and run migration script for existing data (20-40 min)
5. Verify migrated collections, delete old ones (10 min)
6. Restart bot, verify 768D working (5 min)

**Expected Result:** All embeddings (old + new) use 768D with better quality

**Details:** See `docs/roadmaps/EMBEDDING_UPGRADE_768D.md`

---

### Sprint 1 (3-4 days): Redis Caching - The Force Multiplier
**Priority:** CRITICAL | **Solo Impact:** â­â­â­â­â­

Start here because:
- âœ… Unlocks faster iteration on all other features
- âœ… 30-50% DB load reduction = more capacity for development
- âœ… Simple implementation, huge payoff
- âœ… Can be deployed immediately with minimal risk

**Tasks:**
1. Create `src_v2/utils/cache.py` - Cache wrapper (2-3 hours with AI)
2. Update `DatabaseManager` to use cache layer (2-3 hours)
3. Wire into trust scores, facts, preferences (2-3 hours)
4. Deploy and verify (1 hour)

**Expected Result:** All subsequent development is 30-50% faster

---

### Sprint 2 (3-5 days): Streaming Responses - Perceived Latency Win
**Priority:** HIGH | **Solo Impact:** â­â­â­â­

Why second:
- âœ… Biggest perceived latency improvement with minimal code
- âœ… Users feel like bot is thinking faster
- âœ… Complements Redis caching nicely
- âœ… 2-3 days, massive engagement boost

**Tasks:**
1. Enable token streaming in `AgentEngine` (2-3 hours)
2. Handle Discord chunking for long responses (2-3 hours)
3. Add typing status management (1-2 hours)
4. Test on real Discord server (1 hour)

**Expected Result:** Perceived latency drops from 5-8s to 1-2s

---

### Sprint 3 (2-3 days): Hot-Reload Characters
**Priority:** HIGH | **Solo Impact:** â­â­â­â­â­

This is the **development velocity multiplier**:
- âœ… No more waiting for restarts (10+ minutes per change saved)
- âœ… Makes character tweaking 10x faster
- âœ… Essential for iteration on personality updates
- âœ… Simple file watcher pattern

**Tasks:**
1. Create file watcher in `src_v2/utils/file_watcher.py` (1-2 hours)
2. Wire into `CharacterManager` reload logic (1-2 hours)
3. Add HTTP endpoint for manual reload (1 hour)
4. Test with actual character file changes (1 hour)

**Expected Result:** Character tweaks go from 2 minutes â†’ 30 seconds

---

### Sprint 4 (1-2 days): Grafana Dashboards
**Priority:** MEDIUM | **Solo Impact:** â­â­â­

Why now:
- âœ… Low effort, high visibility into system health
- âœ… Helps debug issues faster
- âœ… Pure JSON config, no code changes needed
- âœ… 1-2 days, instant ROI on observability

**Tasks:**
1. Create dashboard JSON configs for Grafana (3-4 hours)
2. Set up alerts for error rates + latency (1-2 hours)

**Expected Result:** Real-time dashboards for trust trends, latency, errors

---

### Sprint 5 (5-7 days): Adaptive Max Steps (Reflective Mode Phase 2.2)
**Priority:** HIGH | **Solo Impact:** â­â­â­â­

After Phase A, now tackle cost reduction:
- âœ… 40-50% cost savings on moderate queries
- âœ… Faster responses for simple questions
- âœ… User testing likely complete by now
- âœ… Enhancing existing classifier = low risk

**Tasks:**
1. Update `ComplexityClassifier` return type (1-2 hours)
2. Add step count heuristics logic (2-3 hours)
3. Integrate with `ReflectiveAgent` (2-3 hours)
4. Test with various query types (1-2 hours)

**Expected Result:** Cost per complex query drops 40-50%

---

### Sprint 6 (5-7 days): Tool Composition / Hierarchical Reasoning
**Priority:** HIGH | **Solo Impact:** â­â­â­â­â­

This unlocks powerful multi-step reasoning:
- âœ… Composable tools reduce LLM round-trips by 30-50%
- âœ… Parallel execution dramatically faster
- âœ… Foundation for future advanced features
- âœ… Very rewarding to implement

**Tasks:**
1. Create composite tool framework (3-4 hours)
2. Implement `AnalyzeTopic` as first composite tool (2-3 hours)
3. Wire parallel execution (2-3 hours)
4. Test with complex queries (1-2 hours)

**Expected Result:** Complex queries 50% faster, better reasoning

---

### Sprint 7 (8-12 days): Worker Queues for Background Tasks
**Priority:** HIGH | **Solo Impact:** â­â­â­â­

Foundation for reliable background processing:
- âœ… Required by Epiphanies, Response Pattern Learning, and other background features
- âœ… Prevents task loss during bot restarts
- âœ… Essential when supporting 5+ concurrent complex queries
- âœ… Separates long-running tasks from request path

**Why Now (Not Later):**
- Epiphanies (Sprint 10) runs pattern detection in background
- Response Pattern Learning uses background scoring
- Reasoning Traces benefit from async storage
- Better to have infrastructure before features that need it

**Tasks:**
1. Create `src_v2/utils/task_queue.py` with Redis-backed queue (3-4 hours)
2. Implement worker process with graceful shutdown (2-3 hours)
3. Add task prioritization (high/medium/low) (2-3 hours)
4. Wire fact extraction, epiphanies, pattern learning to queue (3-4 hours)
5. Add monitoring/metrics to Grafana (2-3 hours)
6. Test restart scenarios, verify no task loss (2-3 hours)

**Expected Result:** Background tasks survive restarts, proper observability

**Details:** See `docs/roadmaps/WORKER_QUEUES.md`

---

### Sprint 8 (5-7 days): Image Generation (DALL-E 3)
**Priority:** MEDIUM-HIGH | **Solo Impact:** â­â­â­

Creative capability unlock:
- âœ… Characters can now draw/create visuals
- âœ… Massive engagement boost
- âœ… Fun to implement, users love it
- âœ… Tool is straightforward integration

**Tasks:**
1. Create `src_v2/tools/image_generation.py` (2-3 hours)
2. Integrate DALL-E 3 API (2-3 hours)
3. Handle Discord uploads + error cases (1-2 hours)
4. Test art generation in practice (1 hour)

**Expected Result:** Characters can "draw me a picture" etc.

---

### Sprint 8.5 (5-7 days): Channel Lurking (B7)
**Priority:** MEDIUM-HIGH | **Solo Impact:** â­â­â­â­

Make bots feel like active community members:
- âœ… Organic engagement without requiring @mentions
- âœ… Cost-effective (local detection, no LLM until response)
- âœ… Entertainment value for servers
- âœ… Character-appropriate triggers (marine biology for Elena, etc.)

**Key Constraint:** NO LLM calls for detection - all local processing!

**Tasks:**
1. Create `lurk_detector.py` with keyword + embedding scoring (3-4 hours)
2. Create `lurk_triggers.yaml` for each character (2-3 hours)
3. Add cooldown manager and rate limiting (1-2 hours)
4. Wire into `on_message()` for non-mentioned messages (1-2 hours)
5. Add `/lurk` admin commands and opt-out (2-3 hours)
6. Test and tune threshold (start conservative at 0.8) (1-2 hours)

**Expected Result:** Bots occasionally chime in on relevant topics naturally

**Details:** See `docs/roadmaps/CHANNEL_LURKING.md`

---

### Sprint 9 (5-7 days): Insight Agent - The Game Changer
**Priority:** CRITICAL | **Solo Impact:** â­â­â­â­â­â­

This is the one that separates your system from every other AI bot. The **Insight Agent** consolidates three features into one coherent agentic system:

- **Reasoning Traces**: 50-60% speed boost on recurring problem patterns
- **Epiphanies**: "I just realized you always talk about space when sad"
- **Response Patterns**: Learn which styles resonate with each user

**Why Agentic (Not Fire-and-Forget)?**
- Agent can **prioritize** which users/conversations need analysis
- Agent can **connect dots** across memories, facts, reactions, and goals
- Agent mirrors how humans reflect - not constantly, but when there's something worth thinking about

**Architecture:**
```
Trigger (time/volume/feedback) â†’ Priority Queue â†’ InsightAgent (ReAct)
                                                       â†“
                              Tools: analyze_patterns, detect_themes, 
                                     generate_epiphany, store_trace, learn_pattern
                                                       â†“
                              Outputs: v2_epiphanies, v2_reasoning_traces, 
                                       v2_response_patterns
```

**Recommended Approach for Solo Dev:**
- Start with `asyncio.create_task` scheduling (migrate to Worker Queue later)
- Focus on epiphanies first (most visible to users)
- Add reasoning traces second (improves reflective mode)
- Add response patterns last (requires feedback data)

**Tasks:**
1. Create `InsightAgent` class with ReAct loop (2-3 hours)
2. Implement data gathering tools (2-3 hours)
3. Implement action tools (epiphany, trace, pattern) (3-4 hours)
4. Create database migrations (1-2 hours)
5. Wire triggers in bot.py/scheduler.py (2-3 hours)
6. Test with real conversations (1-2 hours)

**Expected Result:** 
- Characters reference insights spontaneously
- Similar problems solved 50-60% faster
- Bot learns which response styles work for each user

**Details:** See `docs/roadmaps/INSIGHT_AGENT.md`

---

### Sprint 10 (5-7 days): Channel Lurking (B7)
**Priority:** MEDIUM-HIGH | **Solo Impact:** â­â­â­â­

Make bots feel like active community members:
- âœ… Organic engagement without requiring @mentions
- âœ… Cost-effective (local detection, no LLM until response)
- âœ… Entertainment value for servers
- âœ… Character-appropriate triggers (marine biology for Elena, etc.)

**Key Constraint:** NO LLM calls for detection - all local processing!

**Tasks:**
1. Create `lurk_detector.py` with keyword + embedding scoring (3-4 hours)
2. Create `lurk_triggers.yaml` for each character (2-3 hours)
3. Add cooldown manager and rate limiting (1-2 hours)
4. Wire into `on_message()` for non-mentioned messages (1-2 hours)
5. Add `/lurk` admin commands and opt-out (2-3 hours)
6. Test and tune threshold (start conservative at 0.8) (1-2 hours)

**Expected Result:** Bots occasionally chime in on relevant topics naturally

**Details:** See `docs/roadmaps/CHANNEL_LURKING.md`

---

### Sprint 11 (5-7 days): Audio Processing (Voice Messages)
**Priority:** MEDIUM | **Solo Impact:** â­â­â­

Voice is increasingly important:
- âœ… Seamless voice message support
- âœ… Accessibility boost for voice-preference users
- âœ… Whisper pipeline already exists, minimal work
- âœ… Users love it

**Tasks:**
1. Wire Discord audio attachments to Whisper (2-3 hours)
2. Handle various audio formats (MP3, WAV, OGG) (1-2 hours)
3. Store metadata for future reference (1-2 hours)
4. Test real voice messages (1 hour)

**Expected Result:** "Send voice message to bot" just works

---

### Sprint 12 (5-6 days): Emergent Universe (B8)
**Priority:** MEDIUM-HIGH | **Solo Impact:** â­â­â­â­â­

> *"From countless conversations, a universe is born."*

Transform isolated bots into a living, emergent universe:
- âœ… Discord servers become "planets" with unique vibes
- âœ… Users become "inhabitants" whose profiles emerge from conversation
- âœ… Bots are "travelers" who journey between worlds
- âœ… Cross-planet memory ("I saw you on Planet Lounge!")
- âœ… Privacy commands let inhabitants control what emerges

**Why This Is Huge:**
- The universe **grows organically** from real interactions
- Users feel recognized across the entire cosmos
- Natural cross-promotion ("You should visit Aria on Study Hall!")
- Retention boost from universe immersion

**Tasks:**
1. Create `src_v2/universe/` module structure (2-3 hours)
2. Add PostgreSQL tables for privacy settings, user profiles (1-2 hours)
3. Add Neo4j nodes: `(:Universe)`, `(:Planet)`, `(:UserCharacter)` (2-3 hours)
4. Create `PrivacyManager` with `/privacy` commands (3-4 hours)
5. Build cross-bot knowledge sharing (with privacy checks) (4-5 hours)
6. Create `universes/whisperverse/` config with bot relationships (1-2 hours)
7. Update system prompt injection for universe context (2-3 hours)
8. Test multi-bot, multi-planet scenarios (2-3 hours)

**Expected Result:** Elena says "Marcus mentioned you when he visited from Planet Lounge!" and it feels natural.

**Details:** See `docs/roadmaps/EMERGENT_UNIVERSE.md`

---

### Later (When You Have Time)

**Sprint 13 (4-6 days): Self-Correction (Reflective Phase 2.5)**
- Nice-to-have for complex queries
- 70% reduction in hallucinations
- Implement after Reasoning Traces

**Sprint 14 (14-21 days): Web Dashboard**
- Lower priority for solo dev (you don't need admin UI as much)
- Valuable once you have paying customers
- Implement when you need external visibility

**Sprint 15 (8-10 days): Video Processing**
- Cool feature but lower ROI than core reasoning
- Implement when users ask for it

**Sprint 16-17 (Multi-Month Projects)**
- User Sharding (D1): Do this when hitting 5000+ concurrent users
- Discord-native expansion: Deep integration with threads, forums, voice channels



---

## ðŸ’¡ Key Dependencies & Blockers

### Already Completed âœ…
- âœ… Native function calling (Reflective Phase 1)
- âœ… Parallel tool execution (Reflective Phase 2.3)
- âœ… Proactive messaging system (Phase 13)
- âœ… Complexity classifier (SIMPLE/COMPLEX routing)
- âœ… Trust/evolution system with reaction feedback
- âœ… Background fact/preference extraction

### On Hold â¸ï¸
- Adaptive Max Steps (B1) - waiting for Reflective Mode user testing

### No Blockers ðŸŸ¢
- All Phase A items can start immediately
- All Phase B items have clear implementations
- Most Phase C items depend only on Phase A/B completion

### Future Dependencies ðŸ”µ
- User Sharding (Phase D1) requires stable Worker Queues (C2)
- Discord-native deepening prioritizes feature integration over platform abstraction

---

## ðŸ“ˆ Expected Outcomes

### After Phase A (1 week)
- âœ… Significantly improved developer experience (hot-reload)
- âœ… Better observability (Grafana dashboards)
- âœ… 30-50% reduction in database load
- âœ… Better perceived latency (streaming)

### After Phase B (3-4 weeks)
- âœ… 40-50% cost savings on reflective queries
- âœ… More creative character expressions (art generation)
- âœ… Better voice/audio support
- âœ… Higher reasoning quality

### After Phase C (6-8 weeks)
- âœ… System learns from own reasoning (50-60% speed boost on patterns)
- âœ… Characters have spontaneous insights (epiphanies)
- âœ… Reliable background processing
- âœ… Admin tools for memory/knowledge management
- âœ… Multimodal understanding (video, audio, text, images)

### After Phase D (3-4 months)
- âœ… Platform independence (Telegram, Slack support)
- âœ… Infinite horizontal scaling
- âœ… Enterprise-ready infrastructure
- âœ… 10,000+ concurrent users per character

---

## ðŸš€ Success Metrics

### Performance
- [ ] P95 response latency < 2s (from 3-5s currently)
- [ ] Database queries reduced by 50% (Phase A2)
- [ ] Cost per query reduced by 40% (Phase B1)
- [ ] Reasoning traces improve complex query speed by 60% (Phase C1)

### Quality
- [ ] Hallucination rate reduced by 70% (Phase B4)
- [ ] User satisfaction +30% (Phases C1, C2)
- [ ] Character depth perception increase (Phase C2)

### Scale
- [ ] Support 5,000+ concurrent users (Phase C2)
- [ ] Support 10,000+ concurrent users (Phase D1)
- [ ] Deep Discord integration (threads, forums, stages, commands)

### Reliability
- [ ] 99.9% uptime
- [ ] Zero data loss on restarts (Phase C2)
- [ ] Graceful degradation when services down

---

## ðŸ“ Notes & Considerations

### Architecture Debt
- `src_v2/discord/bot.py` (493 LOC) focused on Discord-native features, keep Discord-specific
- `src_v2/evolution/feedback.py` (359 LOC) could be refactored for better organization
- Focus on deepening Discord integration rather than cross-platform abstraction

### Testing Coverage
- Current test coverage relatively low compared to code size
- Phase A2 (Redis Caching) needs integration tests
- Phase C1 (Reasoning Traces) needs comprehensive test suite
- Plan for dedicated QA sprints after each major feature

### Technical Debt Management
- Allocate 10-20% of sprint capacity to code cleanup
- Document all new subsystems thoroughly
- Keep migration files (Alembic) up to date

### Cost Implications
- Phase B3 (Image Generation) with DALL-E 3 adds ~$0.02 per image
- Phase B1 (Adaptive Max Steps) reduces costs by 40-50% on reflective queries
- Phase D2 (Sharding) may increase infrastructure costs initially but improves efficiency

---

## ðŸ‘¥ Team Recommendations

### For Solo Developer (You!)
**Recommended Approach:**
- 1-2 sprints per week at sustainable pace
- Alternate between high-complexity and low-complexity features
- Use AI assistance for 70-80% of code generation
- Focus review/testing on the remaining 20-30%
- Batch similar work together to build momentum

**Weekly Capacity (Realistic Estimates with AI):**
- 20-25 hours coding per week = ~1-2 features
- With AI assistance: 50-70% faster than traditional development
- Real timeline: Sprints 1-9 = ~12-14 weeks (~3 months)
- Sprints 10-14 = another 6-8 weeks
- Total to enterprise-ready: ~5-6 months (solo)

**AI-Assisted Development Tips:**
1. **Code Generation:** Use Copilot/Claude for 70% generation, you review/test
2. **Batch Similar Tasks:** Group database changes, API updates, etc.
3. **Testing:** Automated testing is your friend; invest early
4. **Documentation:** Let AI generate docs, you refine them
5. **Code Review:** AI can spot bugs; use it for that too

**Weekly Schedule Suggestion:**
- Mon-Tue: High-complexity feature (reasoning, new subsystems)
- Wed: Medium-complexity feature or bug fixes
- Thu-Fri: Testing, deployment, monitoring
- Weekends: Optional, use for research/planning next sprint

### Recommended Tools for Solo Dev
- **Copilot/Claude:** For code generation (primary)
- **GitHub Copilot:** IDE integration for real-time assistance
- **Pytest:** Automated testing (critical for solo dev)
- **Docker:** Local testing environment
- **Grafana:** Observability (catch issues early)

---

## ðŸŽ¯ Solo Developer Success Metrics

**Monthly Goals:**
- **Month 1:** Complete Sprints 1-4 (All Phase A items + Phase B optimization)
  - Result: 30-50% faster system, better observability
- **Month 2:** Complete Sprints 5-7 (Cost optimization + creativity)
  - Result: 40-50% cheaper per query, characters can create art
- **Month 3:** Complete Sprints 8-9 (The game-changers)
  - Result: Emergent intelligence (reasoning traces + epiphanies)

**By End of Month 3:**
- âœ… System is 60-70% faster
- âœ… 50% cheaper to run
- âœ… Characters demonstrably smarter (learn from patterns)
- âœ… Characters feel more authentic (spontaneous insights)
- âœ… Voice support + image generation
- âœ… Ready for wide release

**By End of Month 4-5:**
- âœ… Full admin dashboard
- âœ… Reliable background processing (worker queues)
- âœ… Video understanding
- âœ… Enterprise-grade infrastructure

---

## âš¡ Solo Dev Velocity Tips

### Time-Saving Strategies

**1. Skip Lower-Priority Infrastructure**
- Don't build multi-platform support until you have users
- Don't build sharding until you hit concurrency limits
- Focus on features that directly improve character quality

**2. Leverage Existing Patterns**
- Your code already has Discord integration, memory system, etc.
- Reuse patterns for new features (faster implementation)
- Example: Image generation is just another tool, follows existing pattern

**3. Parallel Independent Work Streams**
- Reasoning Traces (database work) â‰  Image Generation (API work)
- Can do both without coordination overhead
- Solo dev advantage: zero meeting/sync overhead

**4. Aggressive Testing Automation**
- Spend 2-3 days on test framework upfront
- Save 10+ days in debugging later
- Worth it for solo dev

**5. AI-Assisted Code Review**
- Have Copilot/Claude review your own PRs
- Catches bugs you'd miss
- 80% as good as human review for solo dev

### What NOT To Do (Solo Dev Edition)
- âŒ Don't aim for perfection; aim for shipped
- âŒ Don't build things you don't need yet
- âŒ Don't refactor prematurely
- âŒ Don't over-test early (test important stuff, skip edge cases initially)
- âŒ Don't parallelize on too many features (cognitive load)

---

## ðŸ“Š Revised Timeline (Solo Dev)

| Phase | Feature | Original | Solo Dev | Cumulative |
|-------|---------|----------|----------|-----------|
| **A0** | **Embedding Upgrade 768D** | **45-75m** | **45-75m** | **45-75m** |
| A2 | Redis Caching | 2-3d | 1-2d | 1-2d |
| A3 | Streaming Responses | 2-3d | 1-2d | 2-4d |
| A1 | Hot-Reload Characters | 1-2d | 1d | 3-5d |
| A4 | Grafana Dashboards | 1d | 1d | âœ… Done |
| A5 | Channel Context Awareness | 3-4d | 2-3d | 6-10d |
| B1 | Adaptive Max Steps | 3-5d | 2-3d | 7-11d |
| B2 | Tool Composition | 5-7d | 3-4d | 10-15d |
| B3 | Image Generation | 4-6d | 2-3d | 12-18d |
| B7 | Channel Lurking | 5-7d | 3-4d | 15-22d |
| **C1** | **Insight Agent** | **20-28d** | **5-7d** | **20-29d** |
| - | â†³ (Reasoning Traces) | - | - | (subsumed) |
| - | â†³ (Epiphanies) | - | - | (subsumed) |
| - | â†³ (Response Patterns) | - | - | (subsumed) |
| B8 | Emergent Universe | 7-10d | 5-6d | 24-33d |
| B5 | Audio Processing | 5-7d | 2-3d | 26-36d |
| B4 | Self-Correction | 3-5d | 2-3d | 28-39d |
| C2 | Worker Queues | 8-12d | 4-6d | 32-45d |
| C4 | Web Dashboard | 14-21d | 7-10d | 39-55d |
| C3 | Video Processing | 8-10d | 4-5d | 43-60d |
| D1 | User Sharding | 14-21d | 7-10d | 50-70d |

**Total Solo Dev Time: 2-3 months** to hit all items (vs 4-5 months without consolidation)

**Key Win:** The Insight Agent consolidation saves 15-21 days by unifying C1+C2+B6 into one coherent system!

**Your Timeline: ~10-14 weeks to feature-complete superhuman AI bot**

---

## ðŸš€ Why This Order For Solo Dev?

**Sprint 0 (30 min):** Foundation
- Embedding upgrade MUST happen before production data
- 10-15% better retrieval for everything that follows

**Sprints 1-4 (2 weeks):** Build velocity
- Redis caching means everything else is faster
- Hot-reload means less context-switching waiting for builds
- Streaming means you're not watching loading bars
- By Sprint 4 you're moving 2x faster than today

**Sprints 5-8 (3-4 weeks):** Cost optimization + creativity + engagement
- Adaptive max steps = cheaper
- Tool composition = smarter reasoning
- Image generation = wow factor for users
- Channel lurking = organic community engagement

**Sprint 9 (1 week):** The breakthrough - Insight Agent
- Reasoning traces = system learns from itself
- Epiphanies = characters feel responsive
- Response patterns = personalized styles
- **All three in one coherent agentic system!**

**Sprints 10-14 (3-4 weeks):** Polish + scale
- Audio, video, dashboards, worker queues
- These are all nice-to-have
- You can ship with Sprints 1-9 and iterate on 10-14

---

## ðŸ’¡ Solo Developer Advantage

As a single person with AI tools, you have advantages teams don't have:

1. **No coordination overhead** - You don't need meetings to align
2. **Move fast** - Deploy immediately, no approval chains
3. **Learn as you go** - Understand every part of the system
4. **Iterate quickly** - Change direction in hours, not weeks
5. **AI amplification** - 70-80% code generation means 2-3x developer velocity
6. **Focus** - Can obsess over character quality instead of managing people

**Realistic Outcome:**
- A solo dev with Copilot can outpace small teams
- You'll be shipping features faster than traditional enterprise
- By month 3, you'll have a system that feels superhuman



---

## ðŸ“ž Questions & Contact

For detailed technical questions about any phase, refer to:
- `docs/roadmaps/` - Specific roadmap documents
- `docs/architecture/` - Architecture deep dives
- `src_v2/` - Implementation details and code comments

---

**Version History:**
- v1.0 (Nov 24, 2025) - Initial overview created
- v1.1 (Nov 25, 2025) - Reviewed against codebase; updated "Current State" with comprehensive complete/not-implemented lists; added Status column to priority matrix; added Sprint 9.5 for B6 Response Pattern Learning
- v1.2 (Nov 25, 2025) - Added B7 Channel Lurking (passive engagement feature); updated timeline to 18 items
- v1.3 (Nov 25, 2025) - Added A0 Embedding Upgrade (384Dâ†’768D) as CRITICAL first item
- v1.4 (Nov 25, 2025) - Added B8 Emergent Universe; now 20 items total
- v1.5 (Nov 25, 2025) - Removed Phase D1 Multi-Platform Support; committed to Discord-native deepening. Renumbered D2â†’D1 User Sharding. Focus on Discord-specific features rather than abstraction. Back to 19 items.
- v1.6 (Nov 25, 2025) - **Major consolidation:** Created Insight Agent (C1) to unify Reasoning Traces + Epiphanies + Response Pattern Learning into single agentic system. Saves 15-21 days of development. Renumbered C3â†’C2, C4â†’C3, C5â†’C4. Added `docs/roadmaps/INSIGHT_AGENT.md` specification. Timeline reduced to 10-14 weeks.
- v1.7 (Nov 25, 2025) - Added A5 Channel Context Awareness. Updated to hybrid approach: Phase 1a (Discord API fallback) + Phase 1b (Redis rolling buffer with local embeddings for semantic search) + Phase 2 (LLM tool). See `docs/roadmaps/CHANNEL_CONTEXT_AWARENESS.md`.

**Next Review:** After Phase A completion (estimated Dec 1, 2025)
